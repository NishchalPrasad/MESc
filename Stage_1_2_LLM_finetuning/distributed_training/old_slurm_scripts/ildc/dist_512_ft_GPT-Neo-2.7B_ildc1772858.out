+ module load python
+ unset _mlshdbg
+ '[' 0 = 1 ']'
+ unset _mlre _mlIFS
+ '[' -n x ']'
+ _mlIFS=' 	
'
+ IFS=' '
+ for _mlv in ${MODULES_RUN_QUARANTINE:-}
+ '[' LD_LIBRARY_PATH = LD_LIBRARY_PATH -a LD_LIBRARY_PATH = LD_LIBRARY_PATH ']'
++ eval 'echo ${LD_LIBRARY_PATH+x}'
+++ echo x
+ '[' -n x ']'
++ eval 'echo ${LD_LIBRARY_PATH}'
+++ echo /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' '
+ _mlrv=MODULES_RUNENV_LD_LIBRARY_PATH
++ eval 'echo ${MODULES_RUNENV_LD_LIBRARY_PATH:-}'
+++ echo
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' '
+ '[' -n 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' ' ']'
++ eval 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\''' 'LD_LIBRARY_PATH='\'''\''' /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash '"$@"'
+++ LD_LIBRARY_PATH_modquar=/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+++ LD_LIBRARY_PATH=
+++ /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash load python
+ eval '_LMFILES__modshare=/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/python/3.10.4:1:/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1;' export '_LMFILES__modshare;
LOADEDMODULES_modshare=python/3.10.4:1:cpuarch/amd:1;' export 'LOADEDMODULES_modshare;
PYTHONUNBUFFERED=1;' export 'PYTHONUNBUFFERED;
MODULES_LMCONFLICT_modshare=python/3.10.4\&anaconda-py3\&anaconda-py2\&python\&tensorflow-gpu\&pytorch-gpu:1;' export 'MODULES_LMCONFLICT_modshare;
_LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/python/3.10.4;' export '_LMFILES_;
LOADEDMODULES=cpuarch/amd:python/3.10.4;' export 'LOADEDMODULES;
MODULES_LMCONFLICT=python/3.10.4\&anaconda-py3\&anaconda-py2\&python\&tensorflow-gpu\&pytorch-gpu;' export 'MODULES_LMCONFLICT;
.' '/gpfslocalsup/pub/anaconda-py3/2021.05/etc/profile.d/conda.sh;
conda' activate 'python-3.10.4;
test' '0;'
++ _LMFILES__modshare=/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/python/3.10.4:1:/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1
++ export _LMFILES__modshare
++ LOADEDMODULES_modshare=python/3.10.4:1:cpuarch/amd:1
++ export LOADEDMODULES_modshare
++ PYTHONUNBUFFERED=1
++ export PYTHONUNBUFFERED
++ MODULES_LMCONFLICT_modshare='python/3.10.4&anaconda-py3&anaconda-py2&python&tensorflow-gpu&pytorch-gpu:1'
++ export MODULES_LMCONFLICT_modshare
++ _LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/python/3.10.4
++ export _LMFILES_
++ LOADEDMODULES=cpuarch/amd:python/3.10.4
++ export LOADEDMODULES
++ MODULES_LMCONFLICT='python/3.10.4&anaconda-py3&anaconda-py2&python&tensorflow-gpu&pytorch-gpu'
++ export MODULES_LMCONFLICT
++ . /gpfslocalsup/pub/anaconda-py3/2021.05/etc/profile.d/conda.sh
+++ export CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2021.05/bin/conda
+++ CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2021.05/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2021.05/bin/python
+++ CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2021.05/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /gpfslocalsup/pub/anaconda-py3/2021.05/bin/conda
++++ dirname /gpfslocalsup/pub/anaconda-py3/2021.05/bin
+++ PATH=/gpfslocalsup/pub/anaconda-py3/2021.05/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin:/gpfslocalsys/idrzap/current/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate python-3.10.4
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate python-3.10.4
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate python-3.10.4
+++ /gpfslocalsup/pub/anaconda-py3/2021.05/bin/conda shell.posix activate python-3.10.4
++ ask_conda='PS1='\''(python-3.10.4) '\''
export PATH='\''/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/bin:/gpfslocalsup/pub/anaconda-py3/2021.05/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin:/gpfslocalsys/idrzap/current/bin'\''
export CONDA_PREFIX='\''/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''python-3.10.4'\''
export CONDA_PROMPT_MODIFIER='\''(python-3.10.4) '\''
export CONDA_EXE='\''/gpfslocalsup/pub/anaconda-py3/2021.05/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/gpfslocalsup/pub/anaconda-py3/2021.05/bin/python'\''
. "/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/gdal-activate.sh"
. "/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/geotiff-activate.sh"
. "/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/libglib_activate.sh"
. "/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/proj4-activate.sh"'
++ eval 'PS1='\''(python-3.10.4) '\''
export PATH='\''/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/bin:/gpfslocalsup/pub/anaconda-py3/2021.05/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin:/gpfslocalsys/idrzap/current/bin'\''
export CONDA_PREFIX='\''/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''python-3.10.4'\''
export CONDA_PROMPT_MODIFIER='\''(python-3.10.4) '\''
export CONDA_EXE='\''/gpfslocalsup/pub/anaconda-py3/2021.05/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/gpfslocalsup/pub/anaconda-py3/2021.05/bin/python'\''
. "/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/gdal-activate.sh"
. "/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/geotiff-activate.sh"
. "/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/libglib_activate.sh"
. "/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/proj4-activate.sh"'
+++ PS1='(python-3.10.4) '
+++ export PATH=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/bin:/gpfslocalsup/pub/anaconda-py3/2021.05/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin:/gpfslocalsys/idrzap/current/bin
+++ PATH=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/bin:/gpfslocalsup/pub/anaconda-py3/2021.05/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin:/gpfslocalsys/idrzap/current/bin
+++ export CONDA_PREFIX=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4
+++ CONDA_PREFIX=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=python-3.10.4
+++ CONDA_DEFAULT_ENV=python-3.10.4
+++ export 'CONDA_PROMPT_MODIFIER=(python-3.10.4) '
+++ CONDA_PROMPT_MODIFIER='(python-3.10.4) '
+++ export CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2021.05/bin/conda
+++ CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2021.05/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2021.05/bin/python
+++ CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2021.05/bin/python
+++ . /gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/gdal-activate.sh
++++ [[ -n '' ]]
++++ [[ -n '' ]]
++++ '[' -d /gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/share/gdal ']'
++++ export GDAL_DATA=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/share/gdal
++++ GDAL_DATA=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/share/gdal
++++ export GDAL_DRIVER_PATH=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/lib/gdalplugins
++++ GDAL_DRIVER_PATH=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/lib/gdalplugins
++++ [[ ! -d /gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/lib/gdalplugins ]]
++++ unset GDAL_DRIVER_PATH
++++ export CPL_ZIP_ENCODING=UTF-8
++++ CPL_ZIP_ENCODING=UTF-8
+++ . /gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/geotiff-activate.sh
++++ [[ -n '' ]]
++++ '[' -d /gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/share/epsg_csv ']'
++++ '[' -d /gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/Library/share/epsg_csv ']'
+++ . /gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/libglib_activate.sh
++++ export GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=
++++ GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=
++++ export GSETTINGS_SCHEMA_DIR=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/share/glib-2.0/schemas
++++ GSETTINGS_SCHEMA_DIR=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/share/glib-2.0/schemas
+++ . /gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/etc/conda/activate.d/proj4-activate.sh
++++ '[' -n '' ']'
++++ '[' -d /gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/share/proj ']'
++++ export PROJ_LIB=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/share/proj
++++ PROJ_LIB=/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/share/proj
++++ '[' -f /gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/share/proj/copyright_and_licenses.csv ']'
++++ export PROJ_NETWORK=ON
++++ PROJ_NETWORK=ON
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
++ test 0
+ _mlstatus=0
+ '[' -n x ']'
+ IFS=' 	
'
+ unset _mlre _mlv _mlrv _mlIFS
+ '[' -n '' ']'
+ unset _mlshdbg
+ return 0
+ export PATH=/gpfswork/rech/btm/uei84ht/.local/bin:/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/bin:/gpfslocalsup/pub/anaconda-py3/2021.05/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin:/gpfslocalsys/idrzap/current/bin
+ PATH=/gpfswork/rech/btm/uei84ht/.local/bin:/gpfslocalsup/pub/anaconda-py3/2021.05/envs/python-3.10.4/bin:/gpfslocalsup/pub/anaconda-py3/2021.05/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin:/gpfslocalsys/idrzap/current/bin
+ accelerate launch LEGAL-PE/SIGIR_experiments/distributedTraining/dist_deepspeed_LLM_torch_lexglue_.py --to_train True --batch_size 6 --learning_rate 2e-6 --num_warmup_steps 1000 --to_test True --strat 0 --data_path LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/ --dataset_subset ildc --hggfc_model_name EleutherAI/gpt-neo-2.7B
2023-05-20 18:04:20.031239: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-20 18:04:21.640358: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-05-20 18:04:29.885470: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:04:29.885624: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:04:29.885640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-05-20 18:05:17.553820: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-05-20 18:05:17.553914: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-05-20 18:05:17.554085: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-05-20 18:05:17.554087: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-05-20 18:05:17.554819: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-05-20 18:05:17.562182: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-05-20 18:05:24.445062: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.445085: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.445092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.445102: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.445121: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.445125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.458893: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.458898: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.458914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-05-20 18:05:24.458931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-05-20 18:05:24.458957: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.458963: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.458992: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.458992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-05-20 18:05:24.458999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-05-20 18:05:24.459008: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
2023-05-20 18:05:24.459015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-05-20 18:05:24.459053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
--------------------
--------------------Report
--------------------
Report--------------------
----------------------------------------
--------------------
Report

--------------------
Report
Report
Namespace(to_train=True, batch_size=6, learning_rate=2e-06, epochs=3, num_warmup_steps=1000, to_test=True, testing_model_path=None, testing_model_epoch=None, load_and_retrain=False, retraining_model_path=None, strat=0, dataset_subset='ildc', data_path='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/', hggfc_model_name='EleutherAI/gpt-neo-2.7B', SAVE_DIR='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/tuned_model_lr2e-06_warmup1000/', trained_with_deepspeed_accelerate=None, trained_with_accelerate=None, trained_without_accelerate=None, convert_to_torch_model=False)Report
--------------------


--------------------Namespace(to_train=True, batch_size=6, learning_rate=2e-06, epochs=3, num_warmup_steps=1000, to_test=True, testing_model_path=None, testing_model_epoch=None, load_and_retrain=False, retraining_model_path=None, strat=0, dataset_subset='ildc', data_path='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/', hggfc_model_name='EleutherAI/gpt-neo-2.7B', SAVE_DIR='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/tuned_model_lr2e-06_warmup1000/', trained_with_deepspeed_accelerate=None, trained_with_accelerate=None, trained_without_accelerate=None, convert_to_torch_model=False)
------------------------------------------------------------

Namespace(to_train=True, batch_size=6, learning_rate=2e-06, epochs=3, num_warmup_steps=1000, to_test=True, testing_model_path=None, testing_model_epoch=None, load_and_retrain=False, retraining_model_path=None, strat=0, dataset_subset='ildc', data_path='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/', hggfc_model_name='EleutherAI/gpt-neo-2.7B', SAVE_DIR='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/tuned_model_lr2e-06_warmup1000/', trained_with_deepspeed_accelerate=None, trained_with_accelerate=None, trained_without_accelerate=None, convert_to_torch_model=False)


--------------------
Namespace(to_train=True, batch_size=6, learning_rate=2e-06, epochs=3, num_warmup_steps=1000, to_test=True, testing_model_path=None, testing_model_epoch=None, load_and_retrain=False, retraining_model_path=None, strat=0, dataset_subset='ildc', data_path='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/', hggfc_model_name='EleutherAI/gpt-neo-2.7B', SAVE_DIR='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/tuned_model_lr2e-06_warmup1000/', trained_with_deepspeed_accelerate=None, trained_with_accelerate=None, trained_without_accelerate=None, convert_to_torch_model=False)Namespace(to_train=True, batch_size=6, learning_rate=2e-06, epochs=3, num_warmup_steps=1000, to_test=True, testing_model_path=None, testing_model_epoch=None, load_and_retrain=False, retraining_model_path=None, strat=0, dataset_subset='ildc', data_path='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/', hggfc_model_name='EleutherAI/gpt-neo-2.7B', SAVE_DIR='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/tuned_model_lr2e-06_warmup1000/', trained_with_deepspeed_accelerate=None, trained_with_accelerate=None, trained_without_accelerate=None, convert_to_torch_model=False)
--------------------Namespace(to_train=True, batch_size=6, learning_rate=2e-06, epochs=3, num_warmup_steps=1000, to_test=True, testing_model_path=None, testing_model_epoch=None, load_and_retrain=False, retraining_model_path=None, strat=0, dataset_subset='ildc', data_path='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/', hggfc_model_name='EleutherAI/gpt-neo-2.7B', SAVE_DIR='LEGAL-P_E/SIGIR_experiments/finetuned_models/with_correct_pad/ildc/EleutherAI_gpt-j-6B/Strategy_0/Training_data/chunks_with_100_overlap_and_512_input-length/tuned_model_lr2e-06_warmup1000/', trained_with_deepspeed_accelerate=None, trained_with_accelerate=None, trained_without_accelerate=None, convert_to_torch_model=False)



------------------------------------------------------------


[2023-05-20 18:05:44,695] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
train_labels:torch.Size([392632])
train_labels:torch.Size([392632])
train_labels:torch.Size([392632])train_labels:torch.Size([392632])

train_labels:torch.Size([392632])
train_labels:torch.Size([392632])
[2023-05-20 18:06:28,452] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 2.65B parameters
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized because the shapes did not match:
- wte.weight: found shape torch.Size([50257, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- wpe.weight: found shape torch.Size([2048, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized because the shapes did not match:
- wte.weight: found shape torch.Size([50257, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- wpe.weight: found shape torch.Size([2048, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using pad_token, but it is not set yet.
Using pad_token, but it is not set yet.
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized because the shapes did not match:
- wte.weight: found shape torch.Size([50257, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- wpe.weight: found shape torch.Size([2048, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using pad_token, but it is not set yet.
/linkhome/rech/geniri01/uei84ht/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/linkhome/rech/geniri01/uei84ht/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/linkhome/rech/geniri01/uei84ht/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-05-20 18:06:28,555] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.1, git-hash=unknown, git-branch=unknown
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized because the shapes did not match:
- wte.weight: found shape torch.Size([50257, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- wpe.weight: found shape torch.Size([2048, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using pad_token, but it is not set yet.
/linkhome/rech/geniri01/uei84ht/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized because the shapes did not match:
- wte.weight: found shape torch.Size([50257, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- wpe.weight: found shape torch.Size([2048, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using pad_token, but it is not set yet.
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at /gpfsdswork/dataset/HuggingFace_Models/EleutherAI/gpt-neo-2.7B and are newly initialized because the shapes did not match:
- wte.weight: found shape torch.Size([50257, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- wpe.weight: found shape torch.Size([2048, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.0.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.1.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.2.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.3.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.4.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.5.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.6.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.7.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.8.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.9.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.10.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.11.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.12.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.13.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.14.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.15.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.16.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.17.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.18.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.19.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.20.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.21.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.22.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.23.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.24.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.25.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.26.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.27.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.28.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.29.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.30.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_1.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.k_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.v_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.q_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.weight: found shape torch.Size([2560, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.attn.attention.out_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.ln_2.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.weight: found shape torch.Size([10240, 2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_fc.bias: found shape torch.Size([10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.weight: found shape torch.Size([2560, 10240]) in the checkpoint and torch.Size([0]) in the model instantiated
- h.31.mlp.c_proj.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.weight: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
- ln_f.bias: found shape torch.Size([2560]) in the checkpoint and torch.Size([0]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using pad_token, but it is not set yet.
/linkhome/rech/geniri01/uei84ht/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/linkhome/rech/geniri01/uei84ht/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-05-20 18:06:28,628] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-05-20 18:06:28,630] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-05-20 18:06:28,630] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-05-20 18:06:28,647] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-05-20 18:06:28,647] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2023-05-20 18:06:28,647] [WARNING] [engine.py:1098:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2023-05-20 18:06:28,647] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2023-05-20 18:06:28,949] [INFO] [utils.py:785:see_memory_usage] Stage 3 initialize beginning
[2023-05-20 18:06:28,949] [INFO] [utils.py:786:see_memory_usage] MA 0.95 GB         Max_MA 1.04 GB         CA 3.01 GB         Max_CA 3 GB 
[2023-05-20 18:06:28,949] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 54.92 GB, percent = 10.9%
[2023-05-20 18:06:28,951] [INFO] [stage3.py:113:__init__] Reduce bucket size 500,000,000
[2023-05-20 18:06:28,951] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50,000,000
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Emitting ninja build file /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.3317124843597412 seconds
Loading extension module utils...Loading extension module utils...
Loading extension module utils...
Loading extension module utils...Loading extension module utils...


Time to load utils op: 0.3519935607910156 secondsTime to load utils op: 0.3529808521270752 seconds

Time to load utils op: 0.35239386558532715 seconds
Time to load utils op: 0.35187816619873047 seconds
Time to load utils op: 0.35259342193603516 seconds
[2023-05-20 18:06:46,359] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-05-20 18:06:46,359] [INFO] [utils.py:786:see_memory_usage] MA 0.95 GB         Max_MA 0.95 GB         CA 3.01 GB         Max_CA 3 GB 
[2023-05-20 18:06:46,359] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 55.03 GB, percent = 10.9%
Parameter Offload: Total persistent parameters: 829440 in 227 params
[2023-05-20 18:06:46,498] [INFO] [utils.py:785:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-05-20 18:06:46,499] [INFO] [utils.py:786:see_memory_usage] MA 0.95 GB         Max_MA 0.95 GB         CA 3.01 GB         Max_CA 3 GB 
[2023-05-20 18:06:46,499] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 55.03 GB, percent = 10.9%
[2023-05-20 18:06:46,622] [INFO] [utils.py:785:see_memory_usage] Before creating fp16 partitions
[2023-05-20 18:06:46,622] [INFO] [utils.py:786:see_memory_usage] MA 0.95 GB         Max_MA 0.95 GB         CA 3.01 GB         Max_CA 3 GB 
[2023-05-20 18:06:46,622] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 55.03 GB, percent = 10.9%
[2023-05-20 18:06:47,727] [INFO] [utils.py:785:see_memory_usage] After creating fp16 partitions: 1
[2023-05-20 18:06:47,728] [INFO] [utils.py:786:see_memory_usage] MA 0.95 GB         Max_MA 0.95 GB         CA 1.91 GB         Max_CA 3 GB 
[2023-05-20 18:06:47,728] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 58.28 GB, percent = 11.6%
[2023-05-20 18:06:47,854] [INFO] [utils.py:785:see_memory_usage] Before creating fp32 partitions
[2023-05-20 18:06:47,855] [INFO] [utils.py:786:see_memory_usage] MA 0.95 GB         Max_MA 0.95 GB         CA 1.91 GB         Max_CA 2 GB 
[2023-05-20 18:06:47,855] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 58.33 GB, percent = 11.6%
[2023-05-20 18:06:48,009] [INFO] [utils.py:785:see_memory_usage] After creating fp32 partitions
[2023-05-20 18:06:48,010] [INFO] [utils.py:786:see_memory_usage] MA 2.59 GB         Max_MA 3.42 GB         CA 4.38 GB         Max_CA 4 GB 
[2023-05-20 18:06:48,010] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 56.68 GB, percent = 11.3%
[2023-05-20 18:06:48,209] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-05-20 18:06:48,209] [INFO] [utils.py:786:see_memory_usage] MA 2.59 GB         Max_MA 2.59 GB         CA 4.38 GB         Max_CA 4 GB 
[2023-05-20 18:06:48,210] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 55.04 GB, percent = 10.9%
[2023-05-20 18:06:48,338] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-05-20 18:06:48,338] [INFO] [utils.py:786:see_memory_usage] MA 5.89 GB         Max_MA 9.18 GB         CA 10.97 GB         Max_CA 11 GB 
[2023-05-20 18:06:48,338] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 55.04 GB, percent = 10.9%
[2023-05-20 18:06:48,339] [INFO] [stage3.py:366:_setup_for_real_optimizer] optimizer state initialized
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0005834102630615234 seconds
Time to load utils op: 0.0005316734313964844 seconds
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006160736083984375 seconds
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006225109100341797 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0007171630859375 seconds
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s]======== Epoch 1 / 3 ========  0%|          | 0/3 [00:00<?, ?it/s]======== Epoch 1 / 3 ========  0%|          | 0/3 [00:00<?, ?it/s]
  0%|          | 0/3 [00:00<?, ?it/s]
Training...======== Epoch 1 / 3 ================ Epoch 1 / 3 ========Training...
======== Epoch 1 / 3 ========



Training...Training...Training...




  0%|          | 0/10907 [00:00<?, ?it/s][A  0%|          | 0/10907 [00:00<?, ?it/s][A
  0%|          | 0/10907 [00:00<?, ?it/s]
[A  0%|          | 0/10907 [00:00<?, ?it/s][A
  0%|          | 0/10907 [00:00<?, ?it/s][A[2023-05-20 18:06:48,613] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-05-20 18:06:48,613] [INFO] [utils.py:786:see_memory_usage] MA 7.64 GB         Max_MA 8.12 GB         CA 10.97 GB         Max_CA 11 GB 
[2023-05-20 18:06:48,613] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 55.04 GB, percent = 10.9%
[2023-05-20 18:06:48,614] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-05-20 18:06:48,614] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-05-20 18:06:48,614] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-05-20 18:06:48,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2023-05-20 18:06:48,614] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   amp_params ................... False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   bfloat16_enabled ............. True
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x150b9c0c4dc0>
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   dump_state ................... False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-05-20 18:06:48,615] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   fp16_auto_cast ............... None
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   fp16_enabled ................. False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 1
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   gradient_clipping ............ 1.0
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 1
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   loss_scale ................... 1.0
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   pld_params ................... False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   steps_per_print .............. inf
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   train_batch_size ............. 36
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  6
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   world_size ................... 6
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  True
[2023-05-20 18:06:48,616] [INFO] [config.py:957:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[2023-05-20 18:06:48,617] [INFO] [config.py:957:print]   zero_enabled ................. True
[2023-05-20 18:06:48,617] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-05-20 18:06:48,617] [INFO] [config.py:957:print]   zero_optimization_stage ...... 3
[2023-05-20 18:06:48,617] [INFO] [config.py:943:print_user_config]   json = {
    "train_batch_size": 36, 
    "train_micro_batch_size_per_gpu": 6, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none"
        }, 
        "offload_param": {
            "device": "none"
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
Using /gpfs7kw/linkhome/rech/geniri01/uei84ht/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00046515464782714844 seconds
  0%|          | 0/3 [00:00<?, ?it/s]======== Epoch 1 / 3 ========
Training...

  0%|          | 0/10907 [00:00<?, ?it/s][A
  0%|          | 1/10907 [00:04<13:47:09,  4.55s/it][A
  0%|          | 1/10907 [00:04<13:47:25,  4.55s/it][A
  0%|          | 1/10907 [00:04<13:50:30,  4.57s/it][A

  0%|          | 1/10907 [00:04<13:30:43,  4.46s/it][A  0%|          | 1/10907 [00:04<13:50:58,  4.57s/it][A
  0%|          | 1/10907 [00:04<13:51:04,  4.57s/it][A
  0%|          | 2/10907 [00:06<8:47:58,  2.90s/it] [A
  0%|          | 2/10907 [00:06<8:49:03,  2.91s/it] [A
  0%|          | 2/10907 [00:06<8:49:27,  2.91s/it] [A
  0%|          | 2/10907 [00:06<8:41:03,  2.87s/it] [A
  0%|          | 2/10907 [00:06<8:50:11,  2.92s/it] [A
  0%|          | 2/10907 [00:06<8:50:24,  2.92s/it] [A
  0%|          | 3/10907 [00:07<6:26:50,  2.13s/it][A
  0%|          | 3/10907 [00:07<6:33:34,  2.17s/it][A
  0%|          | 3/10907 [00:07<6:34:25,  2.17s/it][A
  0%|          | 3/10907 [00:07<6:34:27,  2.17s/it][A

  0%|          | 3/10907 [00:07<6:35:17,  2.18s/it][A  0%|          | 3/10907 [00:07<6:35:11,  2.17s/it][A
  0%|          | 4/10907 [00:08<5:31:23,  1.82s/it][A
  0%|          | 4/10907 [00:08<5:31:22,  1.82s/it][A
  0%|          | 4/10907 [00:08<5:32:05,  1.83s/it][A

  0%|          | 4/10907 [00:08<5:29:49,  1.82s/it][A  0%|          | 4/10907 [00:08<5:31:50,  1.83s/it][A
  0%|          | 4/10907 [00:08<5:32:16,  1.83s/it][A
  0%|          | 5/10907 [00:10<4:57:18,  1.64s/it][A
  0%|          | 5/10907 [00:10<4:57:10,  1.64s/it][A
  0%|          | 5/10907 [00:10<4:55:57,  1.63s/it][A
  0%|          | 5/10907 [00:10<4:57:32,  1.64s/it][A
  0%|          | 5/10907 [00:10<4:57:32,  1.64s/it][A
  0%|          | 5/10907 [00:10<4:57:31,  1.64s/it][A
  0%|          | 6/10907 [00:11<4:36:24,  1.52s/it][A
  0%|          | 6/10907 [00:11<4:36:37,  1.52s/it][A
  0%|          | 6/10907 [00:11<4:37:01,  1.52s/it][A

  0%|          | 6/10907 [00:11<4:37:12,  1.53s/it][A  0%|          | 6/10907 [00:11<4:36:21,  1.52s/it][A
  0%|          | 6/10907 [00:11<4:37:25,  1.53s/it][A
  0%|          | 7/10907 [00:12<4:23:56,  1.45s/it][A
  0%|          | 7/10907 [00:12<4:24:06,  1.45s/it][A


  0%|          | 7/10907 [00:12<4:23:59,  1.45s/it][A  0%|          | 7/10907 [00:12<4:23:14,  1.45s/it][A  0%|          | 7/10907 [00:12<4:23:49,  1.45s/it][A
  0%|          | 7/10907 [00:12<4:23:56,  1.45s/it][A
  0%|          | 8/10907 [00:14<4:14:18,  1.40s/it][A
  0%|          | 8/10907 [00:14<4:14:45,  1.40s/it][A

  0%|          | 8/10907 [00:14<4:15:07,  1.40s/it][A  0%|          | 8/10907 [00:14<4:15:14,  1.41s/it][A
  0%|          | 8/10907 [00:14<4:15:28,  1.41s/it][A
  0%|          | 8/10907 [00:14<4:15:08,  1.40s/it][A
  0%|          | 9/10907 [00:15<4:08:31,  1.37s/it][A
  0%|          | 9/10907 [00:15<4:08:57,  1.37s/it][A

  0%|          | 9/10907 [00:15<4:08:46,  1.37s/it][A  0%|          | 9/10907 [00:15<4:09:01,  1.37s/it][A
  0%|          | 9/10907 [00:15<4:09:12,  1.37s/it][A
  0%|          | 9/10907 [00:15<4:09:40,  1.37s/it][A
  0%|          | 10/10907 [00:16<4:05:35,  1.35s/it][A

  0%|          | 10/10907 [00:16<4:05:57,  1.35s/it][A  0%|          | 10/10907 [00:16<4:06:07,  1.36s/it][A

  0%|          | 10/10907 [00:16<4:05:32,  1.35s/it][A  0%|          | 10/10907 [00:16<4:05:48,  1.35s/it][A
  0%|          | 10/10907 [00:16<4:06:21,  1.36s/it][A
  0%|          | 11/10907 [00:18<4:03:18,  1.34s/it][A
  0%|          | 11/10907 [00:17<4:03:21,  1.34s/it][A
  0%|          | 11/10907 [00:18<4:03:36,  1.34s/it][A
  0%|          | 11/10907 [00:18<4:03:56,  1.34s/it][A
  0%|          | 11/10907 [00:18<4:04:01,  1.34s/it][A
  0%|          | 11/10907 [00:18<4:03:53,  1.34s/it][A
  0%|          | 12/10907 [00:19<4:01:28,  1.33s/it][A
  0%|          | 12/10907 [00:19<4:01:40,  1.33s/it][A
  0%|          | 12/10907 [00:19<4:01:54,  1.33s/it][A
  0%|          | 12/10907 [00:19<4:01:55,  1.33s/it][A

  0%|          | 12/10907 [00:19<4:01:56,  1.33s/it][A  0%|          | 12/10907 [00:19<4:01:48,  1.33s/it][A
  0%|          | 13/10907 [00:20<4:00:21,  1.32s/it][A
  0%|          | 13/10907 [00:20<4:00:38,  1.33s/it][A
  0%|          | 13/10907 [00:20<4:00:49,  1.33s/it][A
  0%|          | 13/10907 [00:20<4:00:38,  1.33s/it][A
  0%|          | 13/10907 [00:20<4:00:37,  1.33s/it][A
  0%|          | 13/10907 [00:20<4:00:48,  1.33s/it][A
  0%|          | 14/10907 [00:22<4:11:46,  1.39s/it][A
  0%|          | 14/10907 [00:22<4:12:12,  1.39s/it][A
  0%|          | 14/10907 [00:22<4:12:09,  1.39s/it][A

  0%|          | 14/10907 [00:22<4:12:23,  1.39s/it][A  0%|          | 14/10907 [00:22<4:12:54,  1.39s/it][A
  0%|          | 14/10907 [00:22<4:12:30,  1.39s/it][A
  0%|          | 15/10907 [00:23<4:13:02,  1.39s/it][A
  0%|          | 15/10907 [00:23<4:13:35,  1.40s/it][A
  0%|          | 15/10907 [00:23<4:13:37,  1.40s/it][A
  0%|          | 15/10907 [00:23<4:13:46,  1.40s/it][A
  0%|          | 15/10907 [00:23<4:14:25,  1.40s/it][A
  0%|          | 15/10907 [00:23<4:14:32,  1.40s/it][A
  0%|          | 16/10907 [00:24<4:09:16,  1.37s/it][A
  0%|          | 16/10907 [00:24<4:09:53,  1.38s/it][A
  0%|          | 16/10907 [00:24<4:09:39,  1.38s/it][A
  0%|          | 16/10907 [00:24<4:10:26,  1.38s/it][A
  0%|          | 16/10907 [00:24<4:09:49,  1.38s/it][A
  0%|          | 16/10907 [00:24<4:10:11,  1.38s/it][A

  0%|          | 17/10907 [00:26<4:06:17,  1.36s/it][A  0%|          | 17/10907 [00:26<4:06:21,  1.36s/it][A
  0%|          | 17/10907 [00:26<4:06:03,  1.36s/it][A
  0%|          | 17/10907 [00:26<4:06:26,  1.36s/it][A

  0%|          | 17/10907 [00:26<4:06:34,  1.36s/it][A  0%|          | 17/10907 [00:26<4:06:36,  1.36s/it][A
  0%|          | 18/10907 [00:27<4:04:23,  1.35s/it][A
  0%|          | 18/10907 [00:27<4:04:27,  1.35s/it][A
  0%|          | 18/10907 [00:27<4:04:42,  1.35s/it][A
  0%|          | 18/10907 [00:27<4:04:37,  1.35s/it][A

  0%|          | 18/10907 [00:27<4:04:58,  1.35s/it][A  0%|          | 18/10907 [00:27<4:04:39,  1.35s/it][A
  0%|          | 19/10907 [00:28<4:03:23,  1.34s/it][A
  0%|          | 19/10907 [00:28<4:03:37,  1.34s/it][A
  0%|          | 19/10907 [00:28<4:03:36,  1.34s/it][A

  0%|          | 19/10907 [00:28<4:03:30,  1.34s/it][A
  0%|          | 19/10907 [00:28<4:03:25,  1.34s/it][A  0%|          | 19/10907 [00:28<4:03:26,  1.34s/it][A

  0%|          | 20/10907 [00:30<4:02:01,  1.33s/it][A  0%|          | 20/10907 [00:30<4:02:01,  1.33s/it][A
  0%|          | 20/10907 [00:30<4:01:57,  1.33s/it][A
  0%|          | 20/10907 [00:30<4:02:05,  1.33s/it][A
  0%|          | 20/10907 [00:30<4:02:13,  1.33s/it][A
  0%|          | 20/10907 [00:30<4:02:14,  1.33s/it][A
  0%|          | 21/10907 [00:31<4:01:29,  1.33s/it][A
  0%|          | 21/10907 [00:31<4:01:36,  1.33s/it][A

  0%|          | 21/10907 [00:31<4:01:42,  1.33s/it][A
  0%|          | 21/10907 [00:31<4:01:46,  1.33s/it][A  0%|          | 21/10907 [00:31<4:01:34,  1.33s/it][A
  0%|          | 21/10907 [00:31<4:01:37,  1.33s/it][A
  0%|          | 22/10907 [00:32<4:00:56,  1.33s/it][A
  0%|          | 22/10907 [00:32<4:01:01,  1.33s/it][A
  0%|          | 22/10907 [00:32<4:00:56,  1.33s/it][A
  0%|          | 22/10907 [00:32<4:00:52,  1.33s/it][A
  0%|          | 22/10907 [00:32<4:00:55,  1.33s/it][A
  0%|          | 22/10907 [00:32<4:01:06,  1.33s/it][A
  0%|          | 23/10907 [00:34<4:00:23,  1.33s/it][A
  0%|          | 23/10907 [00:34<4:00:27,  1.33s/it][A
  0%|          | 23/10907 [00:34<4:00:13,  1.32s/it][A
  0%|          | 23/10907 [00:34<4:00:09,  1.32s/it][A
  0%|          | 23/10907 [00:34<4:00:23,  1.33s/it][A
  0%|          | 23/10907 [00:34<4:00:21,  1.32s/it][A
  0%|          | 24/10907 [00:35<3:59:11,  1.32s/it][A
  0%|          | 24/10907 [00:35<4:00:08,  1.32s/it][A
  0%|          | 24/10907 [00:35<3:59:26,  1.32s/it][A

  0%|          | 24/10907 [00:35<4:00:13,  1.32s/it][A  0%|          | 24/10907 [00:35<4:00:09,  1.32s/it][A
  0%|          | 24/10907 [00:35<4:00:17,  1.32s/it][A
  0%|          | 25/10907 [00:36<3:59:39,  1.32s/it][A
  0%|          | 25/10907 [00:36<4:00:01,  1.32s/it][A
  0%|          | 25/10907 [00:36<3:59:37,  1.32s/it][A
  0%|          | 25/10907 [00:36<3:59:47,  1.32s/it][A

  0%|          | 25/10907 [00:36<3:59:47,  1.32s/it][A  0%|          | 25/10907 [00:36<3:59:58,  1.32s/it][A
  0%|          | 26/10907 [00:38<3:59:28,  1.32s/it][A
  0%|          | 26/10907 [00:38<3:59:22,  1.32s/it][A
  0%|          | 26/10907 [00:38<3:59:58,  1.32s/it][A
  0%|          | 26/10907 [00:38<4:00:00,  1.32s/it][A
  0%|          | 26/10907 [00:38<4:00:06,  1.32s/it][A
  0%|          | 26/10907 [00:38<4:00:54,  1.33s/it][A
  0%|          | 27/10907 [00:39<3:59:49,  1.32s/it][A
  0%|          | 27/10907 [00:39<3:59:34,  1.32s/it][A
  0%|          | 27/10907 [00:39<4:00:13,  1.32s/it][A
  0%|          | 27/10907 [00:39<4:00:09,  1.32s/it][A

  0%|          | 27/10907 [00:39<4:00:02,  1.32s/it][A  0%|          | 27/10907 [00:39<4:00:32,  1.33s/it][A
  0%|          | 28/10907 [00:40<4:00:56,  1.33s/it][A
  0%|          | 28/10907 [00:40<4:00:33,  1.33s/it][A
  0%|          | 28/10907 [00:40<4:00:42,  1.33s/it][A


  0%|          | 28/10907 [00:40<4:00:22,  1.33s/it][A  0%|          | 28/10907 [00:40<4:00:40,  1.33s/it][A  0%|          | 28/10907 [00:40<4:00:29,  1.33s/it][A
  0%|          | 29/10907 [00:42<4:00:57,  1.33s/it][A
  0%|          | 29/10907 [00:42<4:00:35,  1.33s/it][A
  0%|          | 29/10907 [00:42<4:00:40,  1.33s/it][A
  0%|          | 29/10907 [00:42<4:00:33,  1.33s/it][A
  0%|          | 29/10907 [00:42<4:00:30,  1.33s/it]
[A  0%|          | 29/10907 [00:42<4:00:43,  1.33s/it][A
  0%|          | 30/10907 [00:43<4:07:15,  1.36s/it][A
  0%|          | 30/10907 [00:43<4:06:59,  1.36s/it][A
  0%|          | 30/10907 [00:43<4:07:13,  1.36s/it][A
  0%|          | 30/10907 [00:43<4:07:31,  1.37s/it][A
  0%|          | 30/10907 [00:43<4:07:23,  1.36s/it][A
  0%|          | 30/10907 [00:43<4:07:58,  1.37s/it][A
  0%|          | 31/10907 [00:44<4:09:25,  1.38s/it][A
  0%|          | 31/10907 [00:45<4:10:22,  1.38s/it][A
  0%|          | 31/10907 [00:45<4:10:30,  1.38s/it][A
  0%|          | 31/10907 [00:45<4:10:27,  1.38s/it][A

  0%|          | 31/10907 [00:45<4:10:29,  1.38s/it][A  0%|          | 31/10907 [00:45<4:10:44,  1.38s/it][A
  0%|          | 32/10907 [00:46<4:07:37,  1.37s/it][A
  0%|          | 32/10907 [00:46<4:07:39,  1.37s/it][A
  0%|          | 32/10907 [00:46<4:07:47,  1.37s/it][A
  0%|          | 32/10907 [00:46<4:07:59,  1.37s/it][A

  0%|          | 32/10907 [00:46<4:08:14,  1.37s/it][A  0%|          | 32/10907 [00:46<4:07:53,  1.37s/it][A
  0%|          | 33/10907 [00:47<4:06:46,  1.36s/it][A


  0%|          | 33/10907 [00:47<4:06:30,  1.36s/it][A  0%|          | 33/10907 [00:47<4:06:37,  1.36s/it][A  0%|          | 33/10907 [00:47<4:06:23,  1.36s/it][A
  0%|          | 33/10907 [00:47<4:06:40,  1.36s/it][A
  0%|          | 33/10907 [00:47<4:06:41,  1.36s/it][A
  0%|          | 34/10907 [00:49<4:04:53,  1.35s/it][A

  0%|          | 34/10907 [00:49<4:04:39,  1.35s/it][A  0%|          | 34/10907 [00:49<4:04:41,  1.35s/it][A
  0%|          | 34/10907 [00:49<4:04:42,  1.35s/it][A
  0%|          | 34/10907 [00:49<4:04:45,  1.35s/it][A
  0%|          | 34/10907 [00:48<4:05:05,  1.35s/it][A
  0%|          | 35/10907 [00:50<4:03:29,  1.34s/it][A
  0%|          | 35/10907 [00:50<4:03:25,  1.34s/it][A
  0%|          | 35/10907 [00:50<4:03:22,  1.34s/it][A
  0%|          | 35/10907 [00:50<4:03:20,  1.34s/it][A
  0%|          | 35/10907 [00:50<4:03:49,  1.35s/it][A
  0%|          | 35/10907 [00:50<4:03:51,  1.35s/it][A
  0%|          | 36/10907 [00:51<4:02:29,  1.34s/it][A
  0%|          | 36/10907 [00:51<4:02:23,  1.34s/it][A
  0%|          | 36/10907 [00:51<4:02:45,  1.34s/it][A
  0%|          | 36/10907 [00:51<4:02:38,  1.34s/it][A
  0%|          | 36/10907 [00:51<4:02:41,  1.34s/it][A
  0%|          | 36/10907 [00:51<4:02:47,  1.34s/it][A
  0%|          | 37/10907 [00:53<4:02:01,  1.34s/it][A

  0%|          | 37/10907 [00:53<4:01:59,  1.34s/it][A  0%|          | 37/10907 [00:53<4:01:57,  1.34s/it][A
  0%|          | 37/10907 [00:53<4:01:54,  1.34s/it][A
  0%|          | 37/10907 [00:52<4:02:00,  1.34s/it][A
  0%|          | 37/10907 [00:53<4:02:00,  1.34s/it][A
  0%|          | 38/10907 [00:54<4:01:53,  1.34s/it][A
  0%|          | 38/10907 [00:54<4:02:44,  1.34s/it][A

  0%|          | 38/10907 [00:54<4:01:55,  1.34s/it][A  0%|          | 38/10907 [00:54<4:01:59,  1.34s/it][A
  0%|          | 38/10907 [00:54<4:01:58,  1.34s/it][A
  0%|          | 38/10907 [00:54<4:02:07,  1.34s/it][A
  0%|          | 39/10907 [00:55<4:01:47,  1.33s/it][A
  0%|          | 39/10907 [00:55<4:01:44,  1.33s/it][A
  0%|          | 39/10907 [00:55<4:01:51,  1.34s/it][A
  0%|          | 39/10907 [00:55<4:01:50,  1.34s/it][A
  0%|          | 39/10907 [00:55<4:02:01,  1.34s/it][A
  0%|          | 39/10907 [00:55<4:01:55,  1.34s/it][A
  0%|          | 40/10907 [00:57<4:01:48,  1.34s/it][A
  0%|          | 40/10907 [00:56<4:01:42,  1.33s/it][A

  0%|          | 40/10907 [00:57<4:01:50,  1.34s/it][A  0%|          | 40/10907 [00:57<4:01:44,  1.33s/it][A
  0%|          | 40/10907 [00:57<4:01:54,  1.34s/it][A
  0%|          | 40/10907 [00:57<4:01:59,  1.34s/it][A
  0%|          | 41/10907 [00:58<4:00:54,  1.33s/it][A
  0%|          | 41/10907 [00:58<4:01:53,  1.34s/it][A
  0%|          | 41/10907 [00:58<4:01:28,  1.33s/it][A
  0%|          | 41/10907 [00:58<4:01:50,  1.34s/it][A
  0%|          | 41/10907 [00:58<4:01:55,  1.34s/it][A
  0%|          | 41/10907 [00:58<4:01:50,  1.34s/it][A
  0%|          | 42/10907 [00:59<4:01:56,  1.34s/it][A
  0%|          | 42/10907 [00:59<4:01:20,  1.33s/it][A
  0%|          | 42/10907 [00:59<4:01:25,  1.33s/it][A
  0%|          | 42/10907 [00:59<4:01:51,  1.34s/it][A
  0%|          | 42/10907 [00:59<4:01:49,  1.34s/it][A
  0%|          | 42/10907 [00:59<4:02:05,  1.34s/it][A
  0%|          | 43/10907 [01:01<4:00:56,  1.33s/it][A

  0%|          | 43/10907 [01:01<4:01:03,  1.33s/it][A  0%|          | 43/10907 [01:00<4:00:59,  1.33s/it][A
  0%|          | 43/10907 [01:01<4:01:11,  1.33s/it][A

  0%|          | 43/10907 [01:01<4:01:32,  1.33s/it][A  0%|          | 43/10907 [01:01<4:01:23,  1.33s/it][A
  0%|          | 44/10907 [01:02<4:00:23,  1.33s/it][A
  0%|          | 44/10907 [01:02<4:00:31,  1.33s/it][A
  0%|          | 44/10907 [01:02<4:01:33,  1.33s/it][A
  0%|          | 44/10907 [01:02<4:01:35,  1.33s/it][A
  0%|          | 44/10907 [01:02<4:01:32,  1.33s/it][A
  0%|          | 44/10907 [01:02<4:01:33,  1.33s/it][A
  0%|          | 45/10907 [01:03<4:11:36,  1.39s/it][A
  0%|          | 45/10907 [01:03<4:11:09,  1.39s/it][A
  0%|          | 45/10907 [01:03<4:11:06,  1.39s/it][A
  0%|          | 45/10907 [01:03<4:11:29,  1.39s/it][A
  0%|          | 45/10907 [01:03<4:11:44,  1.39s/it][A
  0%|          | 45/10907 [01:03<4:11:47,  1.39s/it][A
  0%|          | 46/10907 [01:05<4:12:46,  1.40s/it][A
  0%|          | 46/10907 [01:05<4:12:42,  1.40s/it][A
  0%|          | 46/10907 [01:05<4:12:59,  1.40s/it][A
  0%|          | 46/10907 [01:05<4:13:06,  1.40s/it][A
  0%|          | 46/10907 [01:05<4:13:21,  1.40s/it]
[A  0%|          | 46/10907 [01:05<4:13:10,  1.40s/it][A
  0%|          | 47/10907 [01:06<4:09:27,  1.38s/it][A
  0%|          | 47/10907 [01:06<4:09:35,  1.38s/it][A
  0%|          | 47/10907 [01:06<4:09:27,  1.38s/it][A
  0%|          | 47/10907 [01:06<4:09:40,  1.38s/it][A
  0%|          | 47/10907 [01:06<4:09:51,  1.38s/it][A
  0%|          | 47/10907 [01:06<4:09:53,  1.38s/it][A
  0%|          | 48/10907 [01:07<4:06:46,  1.36s/it][A
  0%|          | 48/10907 [01:07<4:06:36,  1.36s/it][A
  0%|          | 48/10907 [01:07<4:07:04,  1.37s/it][A
  0%|          | 48/10907 [01:07<4:07:02,  1.37s/it][A
  0%|          | 48/10907 [01:07<4:07:09,  1.37s/it][A
  0%|          | 48/10907 [01:07<4:07:08,  1.37s/it][A
  0%|          | 49/10907 [01:09<4:04:43,  1.35s/it][A

  0%|          | 49/10907 [01:09<4:04:51,  1.35s/it][A  0%|          | 49/10907 [01:09<4:04:56,  1.35s/it][A
  0%|          | 49/10907 [01:09<4:04:46,  1.35s/it][A
  0%|          | 49/10907 [01:09<4:04:51,  1.35s/it][A
  0%|          | 49/10907 [01:09<4:05:02,  1.35s/it][A
  0%|          | 50/10907 [01:10<4:03:43,  1.35s/it][A
  0%|          | 50/10907 [01:10<4:03:38,  1.35s/it][A


  0%|          | 50/10907 [01:10<4:03:40,  1.35s/it][A  0%|          | 50/10907 [01:10<4:03:38,  1.35s/it][A  0%|          | 50/10907 [01:10<4:03:35,  1.35s/it][A
  0%|          | 50/10907 [01:10<4:03:50,  1.35s/it][A
  0%|          | 51/10907 [01:11<4:01:45,  1.34s/it][A
  0%|          | 51/10907 [01:11<4:01:29,  1.33s/it][A
  0%|          | 51/10907 [01:11<4:02:24,  1.34s/it][A
  0%|          | 51/10907 [01:11<4:02:48,  1.34s/it][A

  0%|          | 51/10907 [01:11<4:02:44,  1.34s/it][A  0%|          | 51/10907 [01:11<4:02:47,  1.34s/it][A
  0%|          | 52/10907 [01:13<4:02:10,  1.34s/it][A

  0%|          | 52/10907 [01:13<4:01:52,  1.34s/it][A  0%|          | 52/10907 [01:13<4:01:48,  1.34s/it][A
  0%|          | 52/10907 [01:13<4:02:03,  1.34s/it][A

  0%|          | 52/10907 [01:13<4:02:02,  1.34s/it][A  0%|          | 52/10907 [01:13<4:02:29,  1.34s/it][A
  0%|          | 53/10907 [01:14<4:01:22,  1.33s/it][A

  0%|          | 53/10907 [01:14<4:01:18,  1.33s/it][A
  0%|          | 53/10907 [01:14<4:01:00,  1.33s/it][A  0%|          | 53/10907 [01:14<4:01:06,  1.33s/it][A
  0%|          | 53/10907 [01:14<4:01:33,  1.34s/it][A
  0%|          | 53/10907 [01:14<4:01:36,  1.34s/it][A
  0%|          | 54/10907 [01:15<4:01:17,  1.33s/it][A
  0%|          | 54/10907 [01:15<4:00:55,  1.33s/it][A
  0%|          | 54/10907 [01:15<4:01:14,  1.33s/it][A
  0%|          | 54/10907 [01:15<4:01:01,  1.33s/it][A
  0%|          | 54/10907 [01:15<4:01:13,  1.33s/it][A
  0%|          | 54/10907 [01:15<4:01:11,  1.33s/it][A
  1%|          | 55/10907 [01:17<4:01:35,  1.34s/it][A
  1%|          | 55/10907 [01:17<4:01:12,  1.33s/it][A
  1%|          | 55/10907 [01:17<4:01:45,  1.34s/it][A
  1%|          | 55/10907 [01:17<4:01:40,  1.34s/it][A
  1%|          | 55/10907 [01:17<4:01:51,  1.34s/it][A
  1%|          | 55/10907 [01:17<4:01:45,  1.34s/it][A
  1%|          | 56/10907 [01:18<4:01:19,  1.33s/it][A
  1%|          | 56/10907 [01:18<4:02:08,  1.34s/it][A
  1%|          | 56/10907 [01:18<4:01:38,  1.34s/it][A
  1%|          | 56/10907 [01:18<4:01:33,  1.34s/it][A

  1%|          | 56/10907 [01:18<4:01:50,  1.34s/it][A  1%|          | 56/10907 [01:18<4:01:43,  1.34s/it][A
  1%|          | 57/10907 [01:19<4:01:24,  1.33s/it][A

  1%|          | 57/10907 [01:19<4:01:32,  1.34s/it][A
  1%|          | 57/10907 [01:19<4:01:30,  1.34s/it][A  1%|          | 57/10907 [01:19<4:01:40,  1.34s/it][A

  1%|          | 57/10907 [01:19<4:01:37,  1.34s/it][A  1%|          | 57/10907 [01:19<4:01:46,  1.34s/it][A
  1%|          | 58/10907 [01:21<4:00:46,  1.33s/it][A
  1%|          | 58/10907 [01:21<4:00:54,  1.33s/it][A
  1%|          | 58/10907 [01:21<4:01:45,  1.34s/it][A
  1%|          | 58/10907 [01:21<4:01:12,  1.33s/it][A
  1%|          | 58/10907 [01:21<4:01:22,  1.33s/it][A
  1%|          | 58/10907 [01:21<4:01:40,  1.34s/it][A
  1%|          | 59/10907 [01:22<4:01:11,  1.33s/it][A
  1%|          | 59/10907 [01:22<4:01:08,  1.33s/it][A
  1%|          | 59/10907 [01:22<4:01:27,  1.34s/it][A

  1%|          | 59/10907 [01:22<4:01:23,  1.34s/it][A  1%|          | 59/10907 [01:22<4:01:39,  1.34s/it][A
  1%|          | 59/10907 [01:22<4:01:55,  1.34s/it][A
  1%|          | 60/10907 [01:23<4:01:00,  1.33s/it][A
  1%|          | 60/10907 [01:23<4:01:02,  1.33s/it][A
  1%|          | 60/10907 [01:23<4:01:08,  1.33s/it][A
  1%|          | 60/10907 [01:23<4:01:27,  1.34s/it][A
  1%|          | 60/10907 [01:23<4:01:20,  1.34s/it][A
  1%|          | 60/10907 [01:23<4:01:37,  1.34s/it][A
  1%|          | 61/10907 [01:25<4:13:14,  1.40s/it][A
  1%|          | 61/10907 [01:25<4:12:43,  1.40s/it][A
  1%|          | 61/10907 [01:25<4:12:55,  1.40s/it][A

  1%|          | 61/10907 [01:25<4:13:09,  1.40s/it][A  1%|          | 61/10907 [01:25<4:13:24,  1.40s/it][A
  1%|          | 61/10907 [01:25<4:13:28,  1.40s/it][A

  1%|          | 62/10907 [01:26<4:16:29,  1.42s/it][A  1%|          | 62/10907 [01:26<4:17:10,  1.42s/it][A
  1%|          | 62/10907 [01:26<4:16:52,  1.42s/it][A
  1%|          | 62/10907 [01:26<4:17:12,  1.42s/it][A
  1%|          | 62/10907 [01:26<4:17:14,  1.42s/it][A
  1%|          | 62/10907 [01:26<4:17:27,  1.42s/it][A
  1%|          | 63/10907 [01:28<4:11:01,  1.39s/it][A
  1%|          | 63/10907 [01:28<4:12:05,  1.39s/it][A
  1%|          | 63/10907 [01:28<4:12:19,  1.40s/it][A
  1%|          | 63/10907 [01:28<4:12:05,  1.39s/it][A
  1%|          | 63/10907 [01:28<4:12:33,  1.40s/it][A
  1%|          | 63/10907 [01:28<4:12:54,  1.40s/it][A
  1%|          | 64/10907 [01:29<4:09:06,  1.38s/it][A
  1%|          | 64/10907 [01:29<4:08:37,  1.38s/it][A
  1%|          | 64/10907 [01:29<4:08:59,  1.38s/it][A
  1%|          | 64/10907 [01:29<4:08:55,  1.38s/it][A

  1%|          | 64/10907 [01:29<4:08:52,  1.38s/it][A  1%|          | 64/10907 [01:29<4:09:11,  1.38s/it][A
  1%|          | 65/10907 [01:30<4:06:38,  1.36s/it][A
  1%|          | 65/10907 [01:30<4:06:31,  1.36s/it][A
  1%|          | 65/10907 [01:30<4:06:20,  1.36s/it][A
  1%|          | 65/10907 [01:30<4:06:42,  1.37s/it][A

  1%|          | 65/10907 [01:30<4:06:31,  1.36s/it][A  1%|          | 65/10907 [01:30<4:06:41,  1.37s/it][A
  1%|          | 66/10907 [01:32<4:05:29,  1.36s/it][A
  1%|          | 66/10907 [01:32<4:05:03,  1.36s/it][A
  1%|          | 66/10907 [01:32<4:05:02,  1.36s/it][A
  1%|          | 66/10907 [01:32<4:05:00,  1.36s/it][A

  1%|          | 66/10907 [01:32<4:05:03,  1.36s/it][A  1%|          | 66/10907 [01:32<4:05:11,  1.36s/it][A
  1%|          | 67/10907 [01:33<4:03:58,  1.35s/it][A
  1%|          | 67/10907 [01:33<4:03:37,  1.35s/it][A
  1%|          | 67/10907 [01:33<4:03:47,  1.35s/it][A
  1%|          | 67/10907 [01:33<4:03:59,  1.35s/it][A
  1%|          | 67/10907 [01:33<4:03:52,  1.35s/it][A
  1%|          | 67/10907 [01:33<4:04:05,  1.35s/it][A
  1%|          | 68/10907 [01:34<4:03:00,  1.35s/it][A
  1%|          | 68/10907 [01:34<4:02:43,  1.34s/it][A
  1%|          | 68/10907 [01:34<4:03:02,  1.35s/it][A
  1%|          | 68/10907 [01:34<4:02:55,  1.34s/it][A
  1%|          | 68/10907 [01:34<4:03:12,  1.35s/it][A
  1%|          | 68/10907 [01:34<4:03:17,  1.35s/it][A

  1%|          | 69/10907 [01:36<4:02:03,  1.34s/it][A  1%|          | 69/10907 [01:36<4:02:50,  1.34s/it][A
  1%|          | 69/10907 [01:36<4:02:07,  1.34s/it][A

  1%|          | 69/10907 [01:36<4:02:14,  1.34s/it][A  1%|          | 69/10907 [01:36<4:02:23,  1.34s/it][A
  1%|          | 69/10907 [01:36<4:02:27,  1.34s/it][A
  1%|          | 70/10907 [01:37<4:01:52,  1.34s/it][A
  1%|          | 70/10907 [01:37<4:02:01,  1.34s/it][A
  1%|          | 70/10907 [01:37<4:02:00,  1.34s/it][A
  1%|          | 70/10907 [01:37<4:02:18,  1.34s/it][A

  1%|          | 70/10907 [01:37<4:02:20,  1.34s/it][A  1%|          | 70/10907 [01:37<4:02:12,  1.34s/it][A
  1%|          | 71/10907 [01:38<4:01:59,  1.34s/it][A
  1%|          | 71/10907 [01:38<4:01:54,  1.34s/it][A
  1%|          | 71/10907 [01:38<4:01:57,  1.34s/it][A
  1%|          | 71/10907 [01:38<4:01:52,  1.34s/it][A
  1%|          | 71/10907 [01:38<4:02:04,  1.34s/it][A
  1%|          | 71/10907 [01:38<4:02:08,  1.34s/it][A
  1%|          | 72/10907 [01:40<4:01:49,  1.34s/it][A
  1%|          | 72/10907 [01:40<4:01:21,  1.34s/it][A
  1%|          | 72/10907 [01:40<4:01:47,  1.34s/it][A
  1%|          | 72/10907 [01:40<4:01:48,  1.34s/it][A
  1%|          | 72/10907 [01:40<4:01:57,  1.34s/it][A
  1%|          | 72/10907 [01:40<4:01:55,  1.34s/it][A
  1%|          | 73/10907 [01:41<4:00:05,  1.33s/it][A
  1%|          | 73/10907 [01:41<4:01:12,  1.34s/it][A
  1%|          | 73/10907 [01:41<4:00:42,  1.33s/it][A
  1%|          | 73/10907 [01:41<4:01:17,  1.34s/it][A
  1%|          | 73/10907 [01:41<4:01:19,  1.34s/it][A
  1%|          | 73/10907 [01:41<4:01:38,  1.34s/it][A
  1%|          | 74/10907 [01:42<4:00:39,  1.33s/it][A
  1%|          | 74/10907 [01:42<4:00:50,  1.33s/it][A


  1%|          | 74/10907 [01:42<4:01:05,  1.34s/it][A  1%|          | 74/10907 [01:42<4:00:37,  1.33s/it][A  1%|          | 74/10907 [01:42<4:00:55,  1.33s/it][A
  1%|          | 74/10907 [01:42<4:00:49,  1.33s/it][A
  1%|          | 75/10907 [01:44<4:01:22,  1.34s/it][A
  1%|          | 75/10907 [01:44<4:00:47,  1.33s/it][A
  1%|          | 75/10907 [01:44<4:00:49,  1.33s/it][A
  1%|          | 75/10907 [01:44<4:01:10,  1.34s/it][A
  1%|          | 75/10907 [01:44<4:01:11,  1.34s/it][A
  1%|          | 75/10907 [01:44<4:01:16,  1.34s/it][A
  1%|          | 76/10907 [01:45<4:00:47,  1.33s/it][A
  1%|          | 76/10907 [01:45<4:00:50,  1.33s/it][A
  1%|          | 76/10907 [01:45<4:01:42,  1.34s/it][A
  1%|          | 76/10907 [01:45<4:01:00,  1.34s/it][A
  1%|          | 76/10907 [01:45<4:01:15,  1.34s/it][A
  1%|          | 76/10907 [01:45<4:01:20,  1.34s/it][A
  1%|          | 77/10907 [01:47<4:07:14,  1.37s/it][A
  1%|          | 77/10907 [01:47<4:07:12,  1.37s/it][A

  1%|          | 77/10907 [01:47<4:07:38,  1.37s/it][A  1%|          | 77/10907 [01:47<4:07:33,  1.37s/it][A
  1%|          | 77/10907 [01:47<4:07:56,  1.37s/it][A
  1%|          | 77/10907 [01:47<4:08:27,  1.38s/it][A
  1%|          | 78/10907 [01:48<4:08:46,  1.38s/it][A
  1%|          | 78/10907 [01:48<4:09:28,  1.38s/it][A
  1%|          | 78/10907 [01:48<4:09:34,  1.38s/it][A
  1%|          | 78/10907 [01:48<4:09:31,  1.38s/it][A
  1%|          | 78/10907 [01:48<4:09:51,  1.38s/it][A
  1%|          | 78/10907 [01:48<4:10:03,  1.39s/it][A
  1%|          | 79/10907 [01:49<4:07:04,  1.37s/it][A
  1%|          | 79/10907 [01:49<4:07:07,  1.37s/it][A
  1%|          | 79/10907 [01:49<4:07:20,  1.37s/it][A
  1%|          | 79/10907 [01:49<4:07:38,  1.37s/it][A

  1%|          | 79/10907 [01:49<4:07:49,  1.37s/it]  1%|          | 79/10907 [01:49<4:08:03,  1.37s/it][A[A
  1%|          | 80/10907 [01:51<4:06:05,  1.36s/it][A
  1%|          | 80/10907 [01:51<4:05:16,  1.36s/it][A
  1%|          | 80/10907 [01:51<4:05:23,  1.36s/it][A
  1%|          | 80/10907 [01:51<4:05:44,  1.36s/it][A
  1%|          | 80/10907 [01:51<4:05:50,  1.36s/it][A
  1%|          | 80/10907 [01:51<4:05:50,  1.36s/it][A
  1%|          | 81/10907 [01:52<4:03:45,  1.35s/it][A
  1%|          | 81/10907 [01:52<4:03:42,  1.35s/it][A
  1%|          | 81/10907 [01:52<4:03:52,  1.35s/it][A
  1%|          | 81/10907 [01:52<4:03:53,  1.35s/it][A
  1%|          | 81/10907 [01:52<4:04:14,  1.35s/it][A
  1%|          | 81/10907 [01:52<4:04:21,  1.35s/it][A
  1%|          | 82/10907 [01:53<4:02:46,  1.35s/it][A
  1%|          | 82/10907 [01:53<4:02:45,  1.35s/it][A
  1%|          | 82/10907 [01:53<4:02:49,  1.35s/it][A
  1%|          | 82/10907 [01:53<4:02:57,  1.35s/it][A

  1%|          | 82/10907 [01:53<4:03:02,  1.35s/it][A  1%|          | 82/10907 [01:53<4:03:04,  1.35s/it][A
  1%|          | 83/10907 [01:55<4:01:22,  1.34s/it][A
  1%|          | 83/10907 [01:55<4:01:34,  1.34s/it][A
  1%|          | 83/10907 [01:55<4:02:16,  1.34s/it][A
  1%|          | 83/10907 [01:55<4:02:24,  1.34s/it][A
  1%|          | 83/10907 [01:55<4:02:32,  1.34s/it][A
  1%|          | 83/10907 [01:55<4:02:32,  1.34s/it][A
  1%|          | 84/10907 [01:56<4:01:57,  1.34s/it][A
  1%|          | 84/10907 [01:56<4:01:27,  1.34s/it][A
  1%|          | 84/10907 [01:56<4:01:34,  1.34s/it][A
  1%|          | 84/10907 [01:56<4:01:36,  1.34s/it][A
  1%|          | 84/10907 [01:56<4:01:57,  1.34s/it][A
  1%|          | 84/10907 [01:56<4:02:13,  1.34s/it][A
  1%|          | 85/10907 [01:57<4:01:43,  1.34s/it][A
  1%|          | 85/10907 [01:57<4:01:20,  1.34s/it][A
  1%|          | 85/10907 [01:57<4:01:30,  1.34s/it][A
  1%|          | 85/10907 [01:57<4:01:29,  1.34s/it][A
  1%|          | 85/10907 [01:57<4:01:29,  1.34s/it][A
  1%|          | 85/10907 [01:57<4:01:36,  1.34s/it][A
  1%|          | 86/10907 [01:59<4:00:24,  1.33s/it][A
  1%|          | 86/10907 [01:59<4:01:18,  1.34s/it][A
  1%|          | 86/10907 [01:59<4:01:13,  1.34s/it][A
  1%|          | 86/10907 [01:59<4:01:21,  1.34s/it][A

  1%|          | 86/10907 [01:59<4:01:22,  1.34s/it][A  1%|          | 86/10907 [01:59<4:01:23,  1.34s/it][A
  1%|          | 87/10907 [02:00<4:01:11,  1.34s/it][A
  1%|          | 87/10907 [02:00<4:01:01,  1.34s/it][A
  1%|          | 87/10907 [02:00<4:01:26,  1.34s/it][A
  1%|          | 87/10907 [02:00<4:01:12,  1.34s/it][A

  1%|          | 87/10907 [02:00<4:01:26,  1.34s/it][A  1%|          | 87/10907 [02:00<4:01:18,  1.34s/it][A
  1%|          | 88/10907 [02:01<4:01:15,  1.34s/it][A
  1%|          | 88/10907 [02:01<4:01:00,  1.34s/it][A
  1%|          | 88/10907 [02:01<4:01:19,  1.34s/it][A
  1%|          | 88/10907 [02:01<4:01:09,  1.34s/it]
[A  1%|          | 88/10907 [02:01<4:01:02,  1.34s/it][A
  1%|          | 88/10907 [02:01<4:01:09,  1.34s/it][A
  1%|          | 89/10907 [02:03<4:01:08,  1.34s/it][A
  1%|          | 89/10907 [02:03<4:00:41,  1.33s/it][A
  1%|          | 89/10907 [02:03<4:00:44,  1.34s/it][A
  1%|          | 89/10907 [02:03<4:00:59,  1.34s/it][A
  1%|          | 89/10907 [02:03<4:01:16,  1.34s/it][A
  1%|          | 89/10907 [02:03<4:01:06,  1.34s/it][A
  1%|          | 90/10907 [02:04<4:00:35,  1.33s/it][A
  1%|          | 90/10907 [02:04<4:00:33,  1.33s/it][A
  1%|          | 90/10907 [02:04<4:00:47,  1.34s/it][A
  1%|          | 90/10907 [02:04<4:00:56,  1.34s/it][A
  1%|          | 90/10907 [02:04<4:01:08,  1.34s/it][A
  1%|          | 90/10907 [02:04<4:01:10,  1.34s/it][A
  1%|          | 91/10907 [02:05<4:00:48,  1.34s/it][A
  1%|          | 91/10907 [02:05<4:00:34,  1.33s/it][A
  1%|          | 91/10907 [02:05<4:00:45,  1.34s/it][A
  1%|          | 91/10907 [02:05<4:00:38,  1.33s/it][A
  1%|          | 91/10907 [02:05<4:00:48,  1.34s/it][A
  1%|          | 91/10907 [02:05<4:00:51,  1.34s/it][A
  1%|          | 92/10907 [02:07<4:13:20,  1.41s/it][A
  1%|          | 92/10907 [02:07<4:13:27,  1.41s/it][A
  1%|          | 92/10907 [02:07<4:13:41,  1.41s/it][A
  1%|          | 92/10907 [02:07<4:13:50,  1.41s/it][A

  1%|          | 92/10907 [02:07<4:13:52,  1.41s/it][A  1%|          | 92/10907 [02:07<4:13:56,  1.41s/it][A
  1%|          | 93/10907 [02:08<4:16:10,  1.42s/it][A
  1%|          | 93/10907 [02:08<4:15:52,  1.42s/it][A
  1%|          | 93/10907 [02:08<4:17:05,  1.43s/it][A

  1%|          | 93/10907 [02:08<4:17:19,  1.43s/it][A  1%|          | 93/10907 [02:08<4:17:14,  1.43s/it][A
  1%|          | 93/10907 [02:08<4:17:21,  1.43s/it][A
  1%|          | 94/10907 [02:10<4:12:38,  1.40s/it][A
  1%|          | 94/10907 [02:10<4:11:48,  1.40s/it][A
  1%|          | 94/10907 [02:10<4:11:49,  1.40s/it][A
  1%|          | 94/10907 [02:10<4:11:51,  1.40s/it][A
  1%|          | 94/10907 [02:10<4:12:19,  1.40s/it][A
  1%|          | 94/10907 [02:10<4:12:00,  1.40s/it][A
  1%|          | 95/10907 [02:11<4:07:59,  1.38s/it][A
  1%|          | 95/10907 [02:11<4:07:53,  1.38s/it][A
  1%|          | 95/10907 [02:11<4:08:14,  1.38s/it][A
  1%|          | 95/10907 [02:11<4:08:21,  1.38s/it][A
  1%|          | 95/10907 [02:11<4:08:17,  1.38s/it][A
  1%|          | 95/10907 [02:11<4:08:37,  1.38s/it][A
  1%|          | 96/10907 [02:12<4:06:36,  1.37s/it][A
  1%|          | 96/10907 [02:12<4:06:17,  1.37s/it][A
  1%|          | 96/10907 [02:12<4:06:15,  1.37s/it][A
  1%|          | 96/10907 [02:12<4:06:41,  1.37s/it][A
  1%|          | 96/10907 [02:12<4:06:19,  1.37s/it][A
  1%|          | 96/10907 [02:12<4:06:29,  1.37s/it][A
  1%|          | 97/10907 [02:14<4:04:53,  1.36s/it][A
  1%|          | 97/10907 [02:14<4:04:23,  1.36s/it][A
  1%|          | 97/10907 [02:14<4:04:25,  1.36s/it][A
  1%|          | 97/10907 [02:14<4:04:33,  1.36s/it][A
  1%|          | 97/10907 [02:14<4:04:42,  1.36s/it][A
  1%|          | 97/10907 [02:14<4:04:37,  1.36s/it][A
  1%|          | 98/10907 [02:15<4:03:34,  1.35s/it][A
  1%|          | 98/10907 [02:15<4:03:05,  1.35s/it][A
  1%|          | 98/10907 [02:15<4:03:23,  1.35s/it][A
  1%|          | 98/10907 [02:15<4:03:41,  1.35s/it][A
  1%|          | 98/10907 [02:15<4:03:22,  1.35s/it][A
  1%|          | 98/10907 [02:15<4:03:42,  1.35s/it][A
  1%|          | 99/10907 [02:16<4:02:50,  1.35s/it][A
  1%|          | 99/10907 [02:16<4:02:22,  1.35s/it][A
  1%|          | 99/10907 [02:16<4:02:30,  1.35s/it][A

  1%|          | 99/10907 [02:16<4:02:42,  1.35s/it][A
  1%|          | 99/10907 [02:16<4:02:42,  1.35s/it][A  1%|          | 99/10907 [02:16<4:02:51,  1.35s/it][A
  1%|          | 100/10907 [02:18<4:00:54,  1.34s/it][A
  1%|          | 100/10907 [02:18<4:01:01,  1.34s/it][A
  1%|          | 100/10907 [02:18<4:02:17,  1.35s/it][A

  1%|          | 100/10907 [02:18<4:01:48,  1.34s/it][A  1%|          | 100/10907 [02:18<4:01:50,  1.34s/it][A
  1%|          | 100/10907 [02:18<4:01:58,  1.34s/it][A
  1%|          | 101/10907 [02:19<4:01:11,  1.34s/it][A
  1%|          | 101/10907 [02:19<4:01:49,  1.34s/it][A
  1%|          | 101/10907 [02:19<4:02:31,  1.35s/it][A

  1%|          | 101/10907 [02:19<4:02:08,  1.34s/it][A  1%|          | 101/10907 [02:19<4:02:11,  1.34s/it][A
  1%|          | 101/10907 [02:19<4:02:18,  1.35s/it][A
  1%|          | 102/10907 [02:20<4:01:54,  1.34s/it][A
  1%|          | 102/10907 [02:20<4:02:21,  1.35s/it][A
  1%|          | 102/10907 [02:20<4:02:04,  1.34s/it][A
  1%|          | 102/10907 [02:20<4:02:08,  1.34s/it][A
  1%|          | 102/10907 [02:20<4:02:25,  1.35s/it][A
  1%|          | 102/10907 [02:20<4:02:13,  1.35s/it][A
  1%|          | 103/10907 [02:22<4:01:23,  1.34s/it][A
  1%|          | 103/10907 [02:22<4:01:14,  1.34s/it][A
  1%|          | 103/10907 [02:22<4:01:17,  1.34s/it][A
  1%|          | 103/10907 [02:22<4:01:24,  1.34s/it][A
  1%|          | 103/10907 [02:22<4:01:40,  1.34s/it][A
  1%|          | 103/10907 [02:22<4:01:57,  1.34s/it][A
  1%|          | 104/10907 [02:23<4:00:56,  1.34s/it][A

  1%|          | 104/10907 [02:23<4:00:55,  1.34s/it][A  1%|          | 104/10907 [02:23<4:00:48,  1.34s/it][A
  1%|          | 104/10907 [02:23<4:00:55,  1.34s/it][A
  1%|          | 104/10907 [02:23<4:01:06,  1.34s/it][A
  1%|          | 104/10907 [02:23<4:01:10,  1.34s/it][A
  1%|          | 105/10907 [02:24<4:00:30,  1.34s/it][A
  1%|          | 105/10907 [02:24<4:00:30,  1.34s/it][A
  1%|          | 105/10907 [02:24<4:00:45,  1.34s/it][A
  1%|          | 105/10907 [02:24<4:00:43,  1.34s/it][A
  1%|          | 105/10907 [02:24<4:00:43,  1.34s/it][A
  1%|          | 105/10907 [02:24<4:00:48,  1.34s/it][A
  1%|          | 106/10907 [02:26<4:00:14,  1.33s/it][A

  1%|          | 106/10907 [02:26<3:59:49,  1.33s/it][A  1%|          | 106/10907 [02:26<3:59:55,  1.33s/it][A
  1%|          | 106/10907 [02:26<4:00:08,  1.33s/it][A
  1%|          | 106/10907 [02:26<4:00:15,  1.33s/it][A
  1%|          | 106/10907 [02:26<4:00:16,  1.33s/it][A
  1%|          | 107/10907 [02:27<3:59:39,  1.33s/it][A
  1%|          | 107/10907 [02:27<3:59:43,  1.33s/it][A
  1%|          | 107/10907 [02:27<4:00:24,  1.34s/it][A


  1%|          | 107/10907 [02:27<4:00:30,  1.34s/it][A  1%|          | 107/10907 [02:27<4:00:25,  1.34s/it][A  1%|          | 107/10907 [02:27<4:00:34,  1.34s/it][A
  1%|          | 108/10907 [02:29<4:07:18,  1.37s/it][A
  1%|          | 108/10907 [02:29<4:07:01,  1.37s/it][A
  1%|          | 108/10907 [02:29<4:07:19,  1.37s/it][A
  1%|          | 108/10907 [02:29<4:07:48,  1.38s/it][A
  1%|          | 108/10907 [02:29<4:07:44,  1.38s/it][A
  1%|          | 108/10907 [02:29<4:08:33,  1.38s/it][A
  1%|          | 109/10907 [02:30<4:10:04,  1.39s/it][A
  1%|          | 109/10907 [02:30<4:09:24,  1.39s/it][A
  1%|          | 109/10907 [02:30<4:09:52,  1.39s/it][A
  1%|          | 109/10907 [02:30<4:09:57,  1.39s/it][A
  1%|          | 109/10907 [02:30<4:09:45,  1.39s/it][A
  1%|          | 109/10907 [02:30<4:10:08,  1.39s/it][A
  1%|          | 110/10907 [02:31<4:06:44,  1.37s/it][A
  1%|          | 110/10907 [02:31<4:06:44,  1.37s/it][A

  1%|          | 110/10907 [02:31<4:06:49,  1.37s/it][A  1%|          | 110/10907 [02:31<4:06:48,  1.37s/it][A
  1%|          | 110/10907 [02:31<4:06:42,  1.37s/it][A
  1%|          | 110/10907 [02:31<4:07:01,  1.37s/it][A
  1%|          | 111/10907 [02:33<4:04:38,  1.36s/it][A
  1%|          | 111/10907 [02:33<4:04:34,  1.36s/it][A
  1%|          | 111/10907 [02:33<4:04:55,  1.36s/it][A
  1%|          | 111/10907 [02:33<4:04:55,  1.36s/it][A
  1%|          | 111/10907 [02:33<4:04:59,  1.36s/it][A
  1%|          | 111/10907 [02:33<4:05:04,  1.36s/it][A
  1%|          | 112/10907 [02:34<4:03:32,  1.35s/it][A
  1%|          | 112/10907 [02:34<4:03:29,  1.35s/it][A
  1%|          | 112/10907 [02:34<4:03:30,  1.35s/it][A
  1%|          | 112/10907 [02:34<4:03:25,  1.35s/it][A
  1%|          | 112/10907 [02:34<4:03:39,  1.35s/it][A
  1%|          | 112/10907 [02:34<4:03:50,  1.36s/it][A
  1%|          | 113/10907 [02:35<4:02:21,  1.35s/it][A
  1%|          | 113/10907 [02:35<4:02:32,  1.35s/it][A

  1%|          | 113/10907 [02:35<4:02:25,  1.35s/it]
[A  1%|          | 113/10907 [02:35<4:02:34,  1.35s/it][A  1%|          | 113/10907 [02:35<4:02:23,  1.35s/it][A
  1%|          | 113/10907 [02:35<4:02:32,  1.35s/it][A
  1%|          | 114/10907 [02:37<4:02:01,  1.35s/it][A
  1%|          | 114/10907 [02:37<4:01:36,  1.34s/it][A

  1%|          | 114/10907 [02:37<4:02:01,  1.35s/it][A  1%|          | 114/10907 [02:37<4:02:01,  1.35s/it][A
  1%|          | 114/10907 [02:37<4:01:56,  1.35s/it][A
  1%|          | 114/10907 [02:37<4:01:56,  1.34s/it][A
  1%|          | 115/10907 [02:38<4:00:25,  1.34s/it][A
  1%|          | 115/10907 [02:38<4:00:32,  1.34s/it][A
  1%|          | 115/10907 [02:38<4:01:17,  1.34s/it][A
  1%|          | 115/10907 [02:38<4:01:01,  1.34s/it][A
  1%|          | 115/10907 [02:38<4:01:20,  1.34s/it][A
  1%|          | 115/10907 [02:38<4:01:28,  1.34s/it][A
  1%|          | 116/10907 [02:39<3:59:59,  1.33s/it][A
  1%|          | 116/10907 [02:39<4:00:12,  1.34s/it][A
  1%|          | 116/10907 [02:39<4:00:12,  1.34s/it][A
  1%|          | 116/10907 [02:39<4:00:49,  1.34s/it][A

  1%|          | 116/10907 [02:39<4:00:32,  1.34s/it][A  1%|          | 116/10907 [02:39<4:00:43,  1.34s/it][A
  1%|          | 117/10907 [02:41<4:00:28,  1.34s/it][A
  1%|          | 117/10907 [02:41<4:00:03,  1.33s/it][A
  1%|          | 117/10907 [02:41<4:00:20,  1.34s/it][A
  1%|          | 117/10907 [02:41<4:00:06,  1.34s/it][A
  1%|          | 117/10907 [02:41<4:00:15,  1.34s/it][A
  1%|          | 117/10907 [02:41<4:00:21,  1.34s/it][A
  1%|          | 118/10907 [02:42<3:59:58,  1.33s/it][A
  1%|          | 118/10907 [02:42<3:59:51,  1.33s/it][A
  1%|          | 118/10907 [02:42<3:59:58,  1.33s/it][A
  1%|          | 118/10907 [02:42<4:00:11,  1.34s/it][A
  1%|          | 118/10907 [02:42<4:00:03,  1.34s/it][A
  1%|          | 118/10907 [02:42<4:00:12,  1.34s/it][A
  1%|          | 119/10907 [02:43<3:59:48,  1.33s/it][A
  1%|          | 119/10907 [02:43<3:59:46,  1.33s/it][A
  1%|          | 119/10907 [02:43<3:59:55,  1.33s/it][A
  1%|          | 119/10907 [02:43<4:00:00,  1.33s/it][A

  1%|          | 119/10907 [02:43<4:00:06,  1.34s/it][A  1%|          | 119/10907 [02:43<4:00:10,  1.34s/it][A
  1%|          | 120/10907 [02:45<4:00:26,  1.34s/it][A
  1%|          | 120/10907 [02:45<4:00:13,  1.34s/it][A
  1%|          | 120/10907 [02:45<4:00:18,  1.34s/it][A
  1%|          | 120/10907 [02:45<4:00:29,  1.34s/it][A
  1%|          | 120/10907 [02:45<4:00:26,  1.34s/it][A
  1%|          | 120/10907 [02:45<4:00:41,  1.34s/it][A
  1%|          | 121/10907 [02:46<3:59:48,  1.33s/it][A
  1%|          | 121/10907 [02:46<3:59:51,  1.33s/it][A
  1%|          | 121/10907 [02:46<3:59:53,  1.33s/it][A
  1%|          | 121/10907 [02:46<3:59:55,  1.33s/it][A
  1%|          | 121/10907 [02:46<4:00:08,  1.34s/it][A
  1%|          | 121/10907 [02:46<4:00:18,  1.34s/it][A
  1%|          | 122/10907 [02:47<4:00:05,  1.34s/it][A
  1%|          | 122/10907 [02:47<3:59:30,  1.33s/it][A


  1%|          | 122/10907 [02:47<3:59:53,  1.33s/it][A  1%|          | 122/10907 [02:47<3:59:48,  1.33s/it][A  1%|          | 122/10907 [02:47<3:59:45,  1.33s/it][A
  1%|          | 122/10907 [02:47<3:59:43,  1.33s/it][A
  1%|          | 123/10907 [02:49<4:10:31,  1.39s/it][A
  1%|          | 123/10907 [02:49<4:10:27,  1.39s/it][A
  1%|          | 123/10907 [02:49<4:11:25,  1.40s/it][A

  1%|          | 123/10907 [02:49<4:11:02,  1.40s/it][A  1%|          | 123/10907 [02:49<4:10:57,  1.40s/it][A
  1%|          | 123/10907 [02:49<4:10:57,  1.40s/it][A
  1%|          | 124/10907 [02:50<4:15:02,  1.42s/it][A
  1%|          | 124/10907 [02:50<4:15:13,  1.42s/it][A
  1%|          | 124/10907 [02:50<4:15:19,  1.42s/it][A
  1%|          | 124/10907 [02:50<4:15:37,  1.42s/it][A
  1%|          | 124/10907 [02:50<4:15:22,  1.42s/it][A
  1%|          | 124/10907 [02:50<4:15:39,  1.42s/it][A
  1%|          | 125/10907 [02:52<4:09:42,  1.39s/it][A
  1%|          | 125/10907 [02:52<4:09:52,  1.39s/it][A
  1%|          | 125/10907 [02:52<4:10:13,  1.39s/it][A
  1%|          | 125/10907 [02:52<4:10:15,  1.39s/it][A
  1%|          | 125/10907 [02:52<4:10:19,  1.39s/it][A
  1%|          | 125/10907 [02:52<4:10:30,  1.39s/it][A
  1%|          | 126/10907 [02:53<4:06:26,  1.37s/it][A
  1%|          | 126/10907 [02:53<4:06:16,  1.37s/it][A
  1%|          | 126/10907 [02:53<4:06:27,  1.37s/it][A
  1%|          | 126/10907 [02:53<4:06:30,  1.37s/it][A
  1%|          | 126/10907 [02:53<4:06:43,  1.37s/it][A
  1%|          | 126/10907 [02:53<4:06:37,  1.37s/it][A
  1%|          | 127/10907 [02:54<4:04:48,  1.36s/it][A
  1%|          | 127/10907 [02:54<4:04:31,  1.36s/it][A
  1%|          | 127/10907 [02:54<4:04:37,  1.36s/it][A
  1%|          | 127/10907 [02:54<4:04:45,  1.36s/it][A
  1%|          | 127/10907 [02:54<4:04:50,  1.36s/it][A
  1%|          | 127/10907 [02:54<4:04:52,  1.36s/it][A
  1%|          | 128/10907 [02:56<4:03:17,  1.35s/it][A
  1%|          | 128/10907 [02:56<4:02:56,  1.35s/it][A
  1%|          | 128/10907 [02:56<4:03:05,  1.35s/it][A
  1%|          | 128/10907 [02:56<4:03:09,  1.35s/it][A
  1%|          | 128/10907 [02:56<4:03:18,  1.35s/it][A
  1%|          | 128/10907 [02:56<4:03:28,  1.36s/it][A
  1%|          | 129/10907 [02:57<4:01:51,  1.35s/it][A
  1%|          | 129/10907 [02:57<4:01:41,  1.35s/it][A
  1%|          | 129/10907 [02:57<4:01:51,  1.35s/it][A

  1%|          | 129/10907 [02:57<4:02:09,  1.35s/it][A  1%|          | 129/10907 [02:57<4:02:12,  1.35s/it][A
  1%|          | 129/10907 [02:57<4:02:18,  1.35s/it][A
  1%|          | 130/10907 [02:58<4:01:07,  1.34s/it][A
  1%|          | 130/10907 [02:58<4:00:57,  1.34s/it][A
  1%|          | 130/10907 [02:58<4:01:19,  1.34s/it][A
  1%|          | 130/10907 [02:58<4:01:21,  1.34s/it][A
  1%|          | 130/10907 [02:58<4:01:36,  1.35s/it][A
  1%|          | 130/10907 [02:58<4:01:45,  1.35s/it][A
  1%|          | 131/10907 [03:00<4:01:31,  1.34s/it][A
  1%|          | 131/10907 [03:00<4:00:40,  1.34s/it][A

  1%|          | 131/10907 [03:00<4:00:53,  1.34s/it][A
  1%|          | 131/10907 [03:00<4:00:59,  1.34s/it][A  1%|          | 131/10907 [03:00<4:00:52,  1.34s/it][A
  1%|          | 131/10907 [03:00<4:01:12,  1.34s/it][A
  1%|          | 132/10907 [03:01<4:00:21,  1.34s/it][A
  1%|          | 132/10907 [03:01<3:59:46,  1.34s/it][A
  1%|          | 132/10907 [03:01<3:59:51,  1.34s/it][A
  1%|          | 132/10907 [03:01<4:00:19,  1.34s/it][A
  1%|          | 132/10907 [03:01<4:00:28,  1.34s/it][A
  1%|          | 132/10907 [03:01<4:00:41,  1.34s/it][A
  1%|          | 133/10907 [03:02<4:00:05,  1.34s/it][A

  1%|          | 133/10907 [03:02<4:00:26,  1.34s/it][A  1%|          | 133/10907 [03:02<4:00:08,  1.34s/it][A

  1%|          | 133/10907 [03:02<4:00:10,  1.34s/it][A  1%|          | 133/10907 [03:02<4:00:11,  1.34s/it][A
  1%|          | 133/10907 [03:02<4:00:21,  1.34s/it][A
  1%|          | 134/10907 [03:04<3:59:38,  1.33s/it][A
  1%|          | 134/10907 [03:04<3:59:31,  1.33s/it][A
  1%|          | 134/10907 [03:04<3:59:49,  1.34s/it][A
  1%|          | 134/10907 [03:04<3:59:43,  1.34s/it][A
  1%|          | 134/10907 [03:04<3:59:59,  1.34s/it][A
  1%|          | 134/10907 [03:04<4:00:03,  1.34s/it][A
  1%|          | 135/10907 [03:05<3:58:45,  1.33s/it][A
  1%|          | 135/10907 [03:05<3:58:55,  1.33s/it][A

  1%|          | 135/10907 [03:05<3:59:29,  1.33s/it][A  1%|          | 135/10907 [03:05<4:00:05,  1.34s/it][A
  1%|          | 135/10907 [03:05<3:59:40,  1.33s/it][A
  1%|          | 135/10907 [03:05<4:00:03,  1.34s/it][A
  1%|          | 136/10907 [03:06<3:59:38,  1.33s/it][A
  1%|          | 136/10907 [03:06<3:59:29,  1.33s/it][A
  1%|          | 136/10907 [03:06<4:00:09,  1.34s/it][A


  1%|          | 136/10907 [03:06<4:00:09,  1.34s/it][A  1%|          | 136/10907 [03:06<3:59:56,  1.34s/it][A  1%|          | 136/10907 [03:06<4:00:07,  1.34s/it][A
  1%|▏         | 137/10907 [03:08<3:59:51,  1.34s/it][A
  1%|▏         | 137/10907 [03:08<3:59:30,  1.33s/it][A
  1%|▏         | 137/10907 [03:08<3:59:39,  1.34s/it][A
  1%|▏         | 137/10907 [03:08<3:59:39,  1.34s/it][A

  1%|▏         | 137/10907 [03:08<3:59:42,  1.34s/it][A  1%|▏         | 137/10907 [03:08<3:59:50,  1.34s/it][A
  1%|▏         | 138/10907 [03:09<3:59:10,  1.33s/it][A
  1%|▏         | 138/10907 [03:09<3:59:21,  1.33s/it][A
  1%|▏         | 138/10907 [03:09<3:59:17,  1.33s/it][A
  1%|▏         | 138/10907 [03:09<3:59:15,  1.33s/it][A
  1%|▏         | 138/10907 [03:09<3:59:39,  1.34s/it][A
  1%|▏         | 138/10907 [03:09<3:59:47,  1.34s/it][A
  1%|▏         | 139/10907 [03:11<4:07:11,  1.38s/it][A
  1%|▏         | 139/10907 [03:11<4:07:04,  1.38s/it][A
  1%|▏         | 139/10907 [03:11<4:07:19,  1.38s/it][A
  1%|▏         | 139/10907 [03:11<4:07:37,  1.38s/it][A
  1%|▏         | 139/10907 [03:11<4:07:48,  1.38s/it][A
  1%|▏         | 139/10907 [03:10<4:08:13,  1.38s/it][A
  1%|▏         | 140/10907 [03:12<4:08:30,  1.38s/it][A
  1%|▏         | 140/10907 [03:12<4:09:36,  1.39s/it][A
  1%|▏         | 140/10907 [03:12<4:09:29,  1.39s/it][A
  1%|▏         | 140/10907 [03:12<4:09:19,  1.39s/it][A
  1%|▏         | 140/10907 [03:12<4:09:26,  1.39s/it][A
  1%|▏         | 140/10907 [03:12<4:09:28,  1.39s/it][A
  1%|▏         | 141/10907 [03:13<4:06:35,  1.37s/it][A
  1%|▏         | 141/10907 [03:13<4:06:31,  1.37s/it][A

  1%|▏         | 141/10907 [03:13<4:06:16,  1.37s/it][A  1%|▏         | 141/10907 [03:13<4:06:26,  1.37s/it][A
  1%|▏         | 141/10907 [03:13<4:06:26,  1.37s/it][A
  1%|▏         | 141/10907 [03:13<4:06:42,  1.37s/it][A
  1%|▏         | 142/10907 [03:15<4:04:24,  1.36s/it][A
  1%|▏         | 142/10907 [03:14<4:04:15,  1.36s/it][A

  1%|▏         | 142/10907 [03:15<4:04:20,  1.36s/it][A  1%|▏         | 142/10907 [03:15<4:04:13,  1.36s/it][A

  1%|▏         | 142/10907 [03:15<4:04:32,  1.36s/it][A  1%|▏         | 142/10907 [03:15<4:04:27,  1.36s/it][A
  1%|▏         | 143/10907 [03:16<4:02:07,  1.35s/it][A

  1%|▏         | 143/10907 [03:16<4:02:26,  1.35s/it][A  1%|▏         | 143/10907 [03:16<4:03:07,  1.36s/it][A
  1%|▏         | 143/10907 [03:16<4:02:35,  1.35s/it][A

  1%|▏         | 143/10907 [03:16<4:03:01,  1.35s/it][A  1%|▏         | 143/10907 [03:16<4:02:56,  1.35s/it][A
  1%|▏         | 144/10907 [03:17<4:01:15,  1.34s/it][A
  1%|▏         | 144/10907 [03:17<4:01:19,  1.35s/it][A
  1%|▏         | 144/10907 [03:17<4:01:30,  1.35s/it][A
  1%|▏         | 144/10907 [03:17<4:01:21,  1.35s/it][A
  1%|▏         | 144/10907 [03:17<4:01:51,  1.35s/it][A
  1%|▏         | 144/10907 [03:17<4:01:31,  1.35s/it][A
  1%|▏         | 145/10907 [03:19<3:59:51,  1.34s/it][A
  1%|▏         | 145/10907 [03:19<3:59:58,  1.34s/it][A
  1%|▏         | 145/10907 [03:19<4:00:13,  1.34s/it][A
  1%|▏         | 145/10907 [03:18<4:00:40,  1.34s/it][A
  1%|▏         | 145/10907 [03:19<4:00:40,  1.34s/it][A
  1%|▏         | 145/10907 [03:19<4:00:37,  1.34s/it][A
  1%|▏         | 146/10907 [03:20<3:59:53,  1.34s/it][A
  1%|▏         | 146/10907 [03:20<4:00:50,  1.34s/it][A
  1%|▏         | 146/10907 [03:20<3:59:58,  1.34s/it][A

  1%|▏         | 146/10907 [03:20<4:00:44,  1.34s/it][A  1%|▏         | 146/10907 [03:20<4:00:43,  1.34s/it][A
  1%|▏         | 146/10907 [03:20<4:00:58,  1.34s/it][A
  1%|▏         | 147/10907 [03:21<4:00:10,  1.34s/it][A
  1%|▏         | 147/10907 [03:21<4:00:19,  1.34s/it][A

  1%|▏         | 147/10907 [03:21<4:00:19,  1.34s/it][A

  1%|▏         | 147/10907 [03:21<4:00:27,  1.34s/it][A  1%|▏         | 147/10907 [03:21<4:00:35,  1.34s/it][A  1%|▏         | 147/10907 [03:21<4:00:22,  1.34s/it][A
  1%|▏         | 148/10907 [03:23<3:59:52,  1.34s/it][A
  1%|▏         | 148/10907 [03:23<3:59:33,  1.34s/it][A
  1%|▏         | 148/10907 [03:23<3:59:47,  1.34s/it][A
  1%|▏         | 148/10907 [03:23<4:00:06,  1.34s/it][A
  1%|▏         | 148/10907 [03:23<4:00:11,  1.34s/it]
[A  1%|▏         | 148/10907 [03:23<4:00:18,  1.34s/it][A
  1%|▏         | 149/10907 [03:24<4:00:06,  1.34s/it][A
  1%|▏         | 149/10907 [03:24<3:59:44,  1.34s/it][A
  1%|▏         | 149/10907 [03:24<3:59:40,  1.34s/it][A
  1%|▏         | 149/10907 [03:24<3:59:46,  1.34s/it][A
  1%|▏         | 149/10907 [03:24<3:59:53,  1.34s/it][A
  1%|▏         | 149/10907 [03:24<3:59:56,  1.34s/it][A
  1%|▏         | 150/10907 [03:25<3:59:45,  1.34s/it][A
  1%|▏         | 150/10907 [03:25<3:59:44,  1.34s/it][A
  1%|▏         | 150/10907 [03:25<3:59:37,  1.34s/it][A


  1%|▏         | 150/10907 [03:25<3:59:40,  1.34s/it][A  1%|▏         | 150/10907 [03:25<3:59:37,  1.34s/it][A  1%|▏         | 150/10907 [03:25<3:59:41,  1.34s/it][A
  1%|▏         | 151/10907 [03:27<3:59:06,  1.33s/it][A
  1%|▏         | 151/10907 [03:27<3:59:00,  1.33s/it][A
  1%|▏         | 151/10907 [03:27<3:59:01,  1.33s/it][A
  1%|▏         | 151/10907 [03:27<3:59:14,  1.33s/it][A

  1%|▏         | 151/10907 [03:27<3:59:19,  1.33s/it][A  1%|▏         | 151/10907 [03:27<3:59:17,  1.33s/it][A
  1%|▏         | 152/10907 [03:28<3:58:22,  1.33s/it][A
  1%|▏         | 152/10907 [03:28<3:58:20,  1.33s/it][A
  1%|▏         | 152/10907 [03:28<3:59:15,  1.33s/it][A

  1%|▏         | 152/10907 [03:28<3:58:42,  1.33s/it][A
  1%|▏         | 152/10907 [03:28<3:58:44,  1.33s/it][A  1%|▏         | 152/10907 [03:28<3:58:49,  1.33s/it][A
  1%|▏         | 153/10907 [03:29<3:58:21,  1.33s/it][A
  1%|▏         | 153/10907 [03:29<3:58:39,  1.33s/it][A


  1%|▏         | 153/10907 [03:29<3:59:00,  1.33s/it][A  1%|▏         | 153/10907 [03:29<3:58:45,  1.33s/it][A  1%|▏         | 153/10907 [03:29<3:58:42,  1.33s/it][A
  1%|▏         | 153/10907 [03:29<3:58:56,  1.33s/it][A
  1%|▏         | 154/10907 [03:31<4:11:20,  1.40s/it][A
  1%|▏         | 154/10907 [03:31<4:11:35,  1.40s/it][A
  1%|▏         | 154/10907 [03:31<4:11:34,  1.40s/it][A
  1%|▏         | 154/10907 [03:31<4:11:34,  1.40s/it][A

  1%|▏         | 154/10907 [03:31<4:11:33,  1.40s/it]  1%|▏         | 154/10907 [03:31<4:11:33,  1.40s/it][A[A
  1%|▏         | 155/10907 [03:32<4:12:31,  1.41s/it][A
  1%|▏         | 155/10907 [03:32<4:12:25,  1.41s/it][A
  1%|▏         | 155/10907 [03:32<4:12:47,  1.41s/it][A
  1%|▏         | 155/10907 [03:32<4:12:53,  1.41s/it][A
  1%|▏         | 155/10907 [03:32<4:12:46,  1.41s/it][A
  1%|▏         | 155/10907 [03:32<4:12:47,  1.41s/it][A
  1%|▏         | 156/10907 [03:34<4:08:42,  1.39s/it][A
  1%|▏         | 156/10907 [03:34<4:08:18,  1.39s/it][A

  1%|▏         | 156/10907 [03:34<4:08:23,  1.39s/it][A  1%|▏         | 156/10907 [03:34<4:08:34,  1.39s/it][A
  1%|▏         | 156/10907 [03:34<4:08:28,  1.39s/it][A
  1%|▏         | 156/10907 [03:33<4:08:40,  1.39s/it][A
  1%|▏         | 157/10907 [03:35<4:04:59,  1.37s/it][A
  1%|▏         | 157/10907 [03:35<4:05:13,  1.37s/it][A
  1%|▏         | 157/10907 [03:35<4:06:08,  1.37s/it][A
  1%|▏         | 157/10907 [03:35<4:05:30,  1.37s/it][A
  1%|▏         | 157/10907 [03:35<4:05:55,  1.37s/it][A
  1%|▏         | 157/10907 [03:35<4:06:10,  1.37s/it][A
  1%|▏         | 158/10907 [03:36<4:03:20,  1.36s/it][A
  1%|▏         | 158/10907 [03:36<4:03:04,  1.36s/it][A
  1%|▏         | 158/10907 [03:36<4:03:00,  1.36s/it][A
  1%|▏         | 158/10907 [03:36<4:03:32,  1.36s/it][A

  1%|▏         | 158/10907 [03:36<4:03:48,  1.36s/it][A  1%|▏         | 158/10907 [03:36<4:03:40,  1.36s/it][A
  1%|▏         | 159/10907 [03:38<4:01:29,  1.35s/it][A
  1%|▏         | 159/10907 [03:38<4:01:13,  1.35s/it][A
  1%|▏         | 159/10907 [03:38<4:01:43,  1.35s/it][A
  1%|▏         | 159/10907 [03:37<4:01:57,  1.35s/it][A
  1%|▏         | 159/10907 [03:38<4:01:46,  1.35s/it][A
  1%|▏         | 159/10907 [03:38<4:01:52,  1.35s/it][A
  1%|▏         | 160/10907 [03:39<4:01:00,  1.35s/it][A
  1%|▏         | 160/10907 [03:39<4:00:57,  1.35s/it][A
  1%|▏         | 160/10907 [03:39<4:01:07,  1.35s/it][A
  1%|▏         | 160/10907 [03:39<4:01:24,  1.35s/it][A
  1%|▏         | 160/10907 [03:39<4:01:23,  1.35s/it][A
  1%|▏         | 160/10907 [03:39<4:01:15,  1.35s/it][A
  1%|▏         | 161/10907 [03:40<4:00:15,  1.34s/it][A
  1%|▏         | 161/10907 [03:40<4:00:06,  1.34s/it][A
  1%|▏         | 161/10907 [03:40<4:00:14,  1.34s/it][A
  1%|▏         | 161/10907 [03:40<4:00:22,  1.34s/it][A
  1%|▏         | 161/10907 [03:40<4:00:14,  1.34s/it][A
  1%|▏         | 161/10907 [03:40<4:00:38,  1.34s/it][A
  1%|▏         | 162/10907 [03:42<3:59:57,  1.34s/it][A
  1%|▏         | 162/10907 [03:42<3:59:51,  1.34s/it][A
  1%|▏         | 162/10907 [03:42<3:59:55,  1.34s/it][A
  1%|▏         | 162/10907 [03:42<3:59:56,  1.34s/it][A
  1%|▏         | 162/10907 [03:41<4:00:11,  1.34s/it][A
  1%|▏         | 162/10907 [03:42<4:00:05,  1.34s/it][A
  1%|▏         | 163/10907 [03:43<3:59:15,  1.34s/it][A
  1%|▏         | 163/10907 [03:43<3:59:17,  1.34s/it][A

  1%|▏         | 163/10907 [03:43<3:59:09,  1.34s/it][A  1%|▏         | 163/10907 [03:43<3:59:13,  1.34s/it][A
  1%|▏         | 163/10907 [03:43<3:59:25,  1.34s/it][A
  1%|▏         | 163/10907 [03:43<3:59:27,  1.34s/it][A
  2%|▏         | 164/10907 [03:44<3:58:13,  1.33s/it][A
  2%|▏         | 164/10907 [03:44<3:58:18,  1.33s/it][A
  2%|▏         | 164/10907 [03:44<3:59:23,  1.34s/it][A
  2%|▏         | 164/10907 [03:44<3:59:03,  1.34s/it][A
  2%|▏         | 164/10907 [03:44<3:59:12,  1.34s/it][A
  2%|▏         | 164/10907 [03:44<3:59:10,  1.34s/it][A
  2%|▏         | 165/10907 [03:46<3:58:28,  1.33s/it][A
  2%|▏         | 165/10907 [03:46<3:58:30,  1.33s/it][A
  2%|▏         | 165/10907 [03:46<3:58:35,  1.33s/it]
[A  2%|▏         | 165/10907 [03:46<3:58:42,  1.33s/it][A
  2%|▏         | 165/10907 [03:45<3:59:00,  1.33s/it][A
  2%|▏         | 165/10907 [03:46<3:59:04,  1.34s/it][A
  2%|▏         | 166/10907 [03:47<3:58:30,  1.33s/it][A
  2%|▏         | 166/10907 [03:47<3:58:30,  1.33s/it][A
  2%|▏         | 166/10907 [03:47<3:58:40,  1.33s/it][A

  2%|▏         | 166/10907 [03:47<3:58:49,  1.33s/it][A
  2%|▏         | 166/10907 [03:47<3:58:42,  1.33s/it][A  2%|▏         | 166/10907 [03:47<3:58:38,  1.33s/it][A
  2%|▏         | 167/10907 [03:48<3:57:46,  1.33s/it][A
  2%|▏         | 167/10907 [03:48<3:57:41,  1.33s/it][A
  2%|▏         | 167/10907 [03:48<3:58:33,  1.33s/it][A
  2%|▏         | 167/10907 [03:48<3:58:22,  1.33s/it][A

  2%|▏         | 167/10907 [03:48<3:58:36,  1.33s/it][A  2%|▏         | 167/10907 [03:48<3:58:33,  1.33s/it][A
  2%|▏         | 168/10907 [03:50<3:57:54,  1.33s/it][A
  2%|▏         | 168/10907 [03:50<3:57:56,  1.33s/it][A
  2%|▏         | 168/10907 [03:50<3:57:58,  1.33s/it][A
  2%|▏         | 168/10907 [03:50<3:58:16,  1.33s/it][A
  2%|▏         | 168/10907 [03:50<3:58:15,  1.33s/it][A
  2%|▏         | 168/10907 [03:49<3:58:35,  1.33s/it][A
  2%|▏         | 169/10907 [03:51<3:57:36,  1.33s/it][A

  2%|▏         | 169/10907 [03:51<3:57:29,  1.33s/it][A  2%|▏         | 169/10907 [03:51<3:57:42,  1.33s/it][A
  2%|▏         | 169/10907 [03:51<3:58:04,  1.33s/it][A
  2%|▏         | 169/10907 [03:51<3:58:06,  1.33s/it][A
  2%|▏         | 169/10907 [03:51<3:58:23,  1.33s/it][A
  2%|▏         | 170/10907 [03:52<4:09:35,  1.39s/it][A
  2%|▏         | 170/10907 [03:52<4:09:16,  1.39s/it][A
  2%|▏         | 170/10907 [03:52<4:09:32,  1.39s/it][A
  2%|▏         | 170/10907 [03:52<4:09:48,  1.40s/it][A
  2%|▏         | 170/10907 [03:52<4:09:40,  1.40s/it][A
  2%|▏         | 170/10907 [03:52<4:09:46,  1.40s/it][A
  2%|▏         | 171/10907 [03:54<4:12:19,  1.41s/it][A
  2%|▏         | 171/10907 [03:54<4:13:09,  1.41s/it][A
  2%|▏         | 171/10907 [03:54<4:12:27,  1.41s/it][A
  2%|▏         | 171/10907 [03:54<4:12:48,  1.41s/it][A
  2%|▏         | 171/10907 [03:54<4:13:00,  1.41s/it][A
  2%|▏         | 171/10907 [03:54<4:13:14,  1.42s/it][A
  2%|▏         | 172/10907 [03:55<4:08:56,  1.39s/it][A
  2%|▏         | 172/10907 [03:55<4:08:24,  1.39s/it][A
  2%|▏         | 172/10907 [03:55<4:08:51,  1.39s/it][A
  2%|▏         | 172/10907 [03:55<4:08:40,  1.39s/it][A
  2%|▏         | 172/10907 [03:55<4:08:52,  1.39s/it][A
  2%|▏         | 172/10907 [03:55<4:08:58,  1.39s/it][A
  2%|▏         | 173/10907 [03:57<4:04:51,  1.37s/it][A
  2%|▏         | 173/10907 [03:57<4:04:46,  1.37s/it][A
  2%|▏         | 173/10907 [03:57<4:05:45,  1.37s/it][A
  2%|▏         | 173/10907 [03:57<4:05:33,  1.37s/it][A
  2%|▏         | 173/10907 [03:57<4:05:28,  1.37s/it][A
  2%|▏         | 173/10907 [03:56<4:05:33,  1.37s/it][A
  2%|▏         | 174/10907 [03:58<4:02:53,  1.36s/it][A
  2%|▏         | 174/10907 [03:58<4:02:57,  1.36s/it][A
  2%|▏         | 174/10907 [03:58<4:03:19,  1.36s/it][A
  2%|▏         | 174/10907 [03:58<4:03:22,  1.36s/it][A
  2%|▏         | 174/10907 [03:58<4:03:20,  1.36s/it][A
  2%|▏         | 174/10907 [03:58<4:03:18,  1.36s/it][A
  2%|▏         | 175/10907 [03:59<4:01:29,  1.35s/it][A
  2%|▏         | 175/10907 [03:59<4:01:22,  1.35s/it][A
  2%|▏         | 175/10907 [03:59<4:01:33,  1.35s/it][A
  2%|▏         | 175/10907 [03:59<4:01:42,  1.35s/it][A
  2%|▏         | 175/10907 [03:59<4:01:31,  1.35s/it][A
  2%|▏         | 175/10907 [03:59<4:01:45,  1.35s/it][A
  2%|▏         | 176/10907 [04:01<4:00:15,  1.34s/it][A
  2%|▏         | 176/10907 [04:01<4:00:43,  1.35s/it][A
  2%|▏         | 176/10907 [04:01<4:01:23,  1.35s/it][A
  2%|▏         | 176/10907 [04:01<4:01:08,  1.35s/it][A
  2%|▏         | 176/10907 [04:01<4:01:01,  1.35s/it][A
  2%|▏         | 176/10907 [04:00<4:01:10,  1.35s/it][A
  2%|▏         | 177/10907 [04:02<3:59:42,  1.34s/it][A
  2%|▏         | 177/10907 [04:02<3:59:24,  1.34s/it][A
  2%|▏         | 177/10907 [04:02<3:59:28,  1.34s/it][A
  2%|▏         | 177/10907 [04:02<4:00:01,  1.34s/it][A
  2%|▏         | 177/10907 [04:02<4:00:10,  1.34s/it][A
  2%|▏         | 177/10907 [04:02<4:00:41,  1.35s/it][A
  2%|▏         | 178/10907 [04:03<3:59:13,  1.34s/it][A
  2%|▏         | 178/10907 [04:03<3:59:29,  1.34s/it][A
  2%|▏         | 178/10907 [04:03<3:59:35,  1.34s/it][A
  2%|▏         | 178/10907 [04:03<3:59:30,  1.34s/it][A
  2%|▏         | 178/10907 [04:03<3:59:38,  1.34s/it][A
  2%|▏         | 178/10907 [04:03<3:59:38,  1.34s/it][A
  2%|▏         | 179/10907 [04:05<3:58:41,  1.33s/it][A
  2%|▏         | 179/10907 [04:05<3:58:48,  1.34s/it][A
  2%|▏         | 179/10907 [04:05<3:58:43,  1.34s/it][A
  2%|▏         | 179/10907 [04:05<3:58:49,  1.34s/it][A
  2%|▏         | 179/10907 [04:04<3:58:51,  1.34s/it][A
  2%|▏         | 179/10907 [04:05<3:58:56,  1.34s/it][A
  2%|▏         | 180/10907 [04:06<3:58:10,  1.33s/it][A

  2%|▏         | 180/10907 [04:06<3:58:13,  1.33s/it][A  2%|▏         | 180/10907 [04:06<3:58:02,  1.33s/it][A
  2%|▏         | 180/10907 [04:06<3:58:02,  1.33s/it][A
  2%|▏         | 180/10907 [04:06<3:58:12,  1.33s/it][A
  2%|▏         | 180/10907 [04:06<3:58:24,  1.33s/it][A
  2%|▏         | 181/10907 [04:07<3:58:12,  1.33s/it][A
  2%|▏         | 181/10907 [04:07<3:58:06,  1.33s/it][A
  2%|▏         | 181/10907 [04:07<3:58:22,  1.33s/it][A
  2%|▏         | 181/10907 [04:07<3:58:19,  1.33s/it][A

  2%|▏         | 181/10907 [04:07<3:58:20,  1.33s/it][A  2%|▏         | 181/10907 [04:07<3:58:25,  1.33s/it][A
  2%|▏         | 182/10907 [04:09<3:57:57,  1.33s/it][A
  2%|▏         | 182/10907 [04:09<3:58:40,  1.34s/it][A


  2%|▏         | 182/10907 [04:08<3:58:39,  1.34s/it]
[A  2%|▏         | 182/10907 [04:09<3:58:39,  1.34s/it][A  2%|▏         | 182/10907 [04:09<3:58:34,  1.33s/it][A  2%|▏         | 182/10907 [04:09<3:58:38,  1.34s/it][A
  2%|▏         | 183/10907 [04:10<3:58:45,  1.34s/it][A
  2%|▏         | 183/10907 [04:10<3:58:36,  1.33s/it][A
  2%|▏         | 183/10907 [04:10<3:58:36,  1.33s/it][A
  2%|▏         | 183/10907 [04:10<3:58:40,  1.34s/it][A

  2%|▏         | 183/10907 [04:10<3:58:47,  1.34s/it][A  2%|▏         | 183/10907 [04:10<3:58:45,  1.34s/it][A
  2%|▏         | 184/10907 [04:11<3:59:05,  1.34s/it][A
  2%|▏         | 184/10907 [04:11<3:58:43,  1.34s/it][A

  2%|▏         | 184/10907 [04:11<3:58:49,  1.34s/it]  2%|▏         | 184/10907 [04:11<3:58:46,  1.34s/it][A[A
  2%|▏         | 184/10907 [04:11<3:58:55,  1.34s/it][A
  2%|▏         | 184/10907 [04:11<3:58:49,  1.34s/it][A
  2%|▏         | 185/10907 [04:13<3:58:59,  1.34s/it][A

  2%|▏         | 185/10907 [04:13<3:58:53,  1.34s/it][A  2%|▏         | 185/10907 [04:13<3:58:51,  1.34s/it][A
  2%|▏         | 185/10907 [04:13<3:58:47,  1.34s/it][A
  2%|▏         | 185/10907 [04:12<3:58:51,  1.34s/it][A
  2%|▏         | 185/10907 [04:13<3:58:52,  1.34s/it][A
  2%|▏         | 186/10907 [04:14<4:06:29,  1.38s/it][A
  2%|▏         | 186/10907 [04:14<4:06:25,  1.38s/it][A
  2%|▏         | 186/10907 [04:14<4:06:34,  1.38s/it][A
  2%|▏         | 186/10907 [04:14<4:06:51,  1.38s/it][A
  2%|▏         | 186/10907 [04:14<4:06:57,  1.38s/it][A
  2%|▏         | 186/10907 [04:14<4:07:23,  1.38s/it][A
  2%|▏         | 187/10907 [04:15<4:08:10,  1.39s/it][A
  2%|▏         | 187/10907 [04:15<4:07:43,  1.39s/it][A
  2%|▏         | 187/10907 [04:15<4:08:09,  1.39s/it][A
  2%|▏         | 187/10907 [04:15<4:08:29,  1.39s/it][A
  2%|▏         | 187/10907 [04:15<4:08:22,  1.39s/it][A
  2%|▏         | 187/10907 [04:15<4:08:37,  1.39s/it][A
  2%|▏         | 188/10907 [04:17<4:06:15,  1.38s/it][A
  2%|▏         | 188/10907 [04:17<4:05:44,  1.38s/it][A
  2%|▏         | 188/10907 [04:17<4:06:00,  1.38s/it][A

  2%|▏         | 188/10907 [04:17<4:06:22,  1.38s/it][A
  2%|▏         | 188/10907 [04:17<4:06:18,  1.38s/it][A  2%|▏         | 188/10907 [04:17<4:06:19,  1.38s/it][A
  2%|▏         | 189/10907 [04:18<4:03:57,  1.37s/it][A
  2%|▏         | 189/10907 [04:18<4:03:40,  1.36s/it][A
  2%|▏         | 189/10907 [04:18<4:03:49,  1.36s/it][A
  2%|▏         | 189/10907 [04:18<4:03:47,  1.36s/it][A
  2%|▏         | 189/10907 [04:18<4:03:57,  1.37s/it][A
  2%|▏         | 189/10907 [04:18<4:03:59,  1.37s/it][A
  2%|▏         | 190/10907 [04:19<4:01:51,  1.35s/it][A
  2%|▏         | 190/10907 [04:19<4:01:57,  1.35s/it][A
  2%|▏         | 190/10907 [04:19<4:01:56,  1.35s/it][A
  2%|▏         | 190/10907 [04:19<4:01:59,  1.35s/it][A
  2%|▏         | 190/10907 [04:19<4:02:23,  1.36s/it][A
  2%|▏         | 190/10907 [04:19<4:02:28,  1.36s/it][A
  2%|▏         | 191/10907 [04:21<4:01:42,  1.35s/it][A

  2%|▏         | 191/10907 [04:21<4:01:18,  1.35s/it][A  2%|▏         | 191/10907 [04:21<4:01:20,  1.35s/it][A
  2%|▏         | 191/10907 [04:21<4:01:27,  1.35s/it][A
  2%|▏         | 191/10907 [04:21<4:01:29,  1.35s/it][A
  2%|▏         | 191/10907 [04:21<4:01:38,  1.35s/it][A
  2%|▏         | 192/10907 [04:22<4:00:31,  1.35s/it][A
  2%|▏         | 192/10907 [04:22<4:00:36,  1.35s/it][A

  2%|▏         | 192/10907 [04:22<4:00:50,  1.35s/it][A  2%|▏         | 192/10907 [04:22<4:00:50,  1.35s/it][A
  2%|▏         | 192/10907 [04:22<4:01:55,  1.35s/it][A
  2%|▏         | 192/10907 [04:22<4:01:19,  1.35s/it][A

  2%|▏         | 193/10907 [04:24<4:00:17,  1.35s/it][A  2%|▏         | 193/10907 [04:24<4:00:35,  1.35s/it][A
  2%|▏         | 193/10907 [04:24<4:00:39,  1.35s/it][A

  2%|▏         | 193/10907 [04:24<4:00:50,  1.35s/it][A  2%|▏         | 193/10907 [04:24<4:01:14,  1.35s/it][A
  2%|▏         | 193/10907 [04:23<4:00:52,  1.35s/it][A
  2%|▏         | 194/10907 [04:25<4:00:08,  1.34s/it][A
  2%|▏         | 194/10907 [04:25<4:00:35,  1.35s/it][A
  2%|▏         | 194/10907 [04:25<4:00:32,  1.35s/it][A
  2%|▏         | 194/10907 [04:25<4:00:30,  1.35s/it][A
  2%|▏         | 194/10907 [04:25<4:00:44,  1.35s/it][A
  2%|▏         | 194/10907 [04:25<4:00:44,  1.35s/it][A
  2%|▏         | 195/10907 [04:26<3:59:53,  1.34s/it][A
  2%|▏         | 195/10907 [04:26<3:59:59,  1.34s/it][A
  2%|▏         | 195/10907 [04:26<4:00:24,  1.35s/it][A
  2%|▏         | 195/10907 [04:26<4:00:33,  1.35s/it][A

  2%|▏         | 195/10907 [04:26<4:00:41,  1.35s/it][A  2%|▏         | 195/10907 [04:26<4:00:56,  1.35s/it][A
  2%|▏         | 196/10907 [04:28<3:59:22,  1.34s/it][A
  2%|▏         | 196/10907 [04:28<3:59:41,  1.34s/it][A
  2%|▏         | 196/10907 [04:28<4:00:00,  1.34s/it][A
  2%|▏         | 196/10907 [04:28<4:00:15,  1.35s/it][A
  2%|▏         | 196/10907 [04:28<4:00:23,  1.35s/it][A
  2%|▏         | 196/10907 [04:27<4:00:24,  1.35s/it][A
  2%|▏         | 197/10907 [04:29<4:00:48,  1.35s/it][A
  2%|▏         | 197/10907 [04:29<4:01:02,  1.35s/it][A
  2%|▏         | 197/10907 [04:29<4:01:02,  1.35s/it][A
  2%|▏         | 197/10907 [04:29<4:01:25,  1.35s/it][A
  2%|▏         | 197/10907 [04:29<4:01:08,  1.35s/it][A
  2%|▏         | 197/10907 [04:29<4:01:21,  1.35s/it][A
  2%|▏         | 198/10907 [04:30<4:00:34,  1.35s/it][A
  2%|▏         | 198/10907 [04:30<4:00:40,  1.35s/it][A
  2%|▏         | 198/10907 [04:30<4:01:00,  1.35s/it][A

  2%|▏         | 198/10907 [04:30<4:01:22,  1.35s/it][A  2%|▏         | 198/10907 [04:30<4:01:12,  1.35s/it][A
  2%|▏         | 198/10907 [04:30<4:01:26,  1.35s/it][A
  2%|▏         | 199/10907 [04:31<4:00:09,  1.35s/it][A
  2%|▏         | 199/10907 [04:32<4:00:19,  1.35s/it][A
  2%|▏         | 199/10907 [04:32<4:00:40,  1.35s/it][A
  2%|▏         | 199/10907 [04:32<4:00:34,  1.35s/it][A
  2%|▏         | 199/10907 [04:32<4:00:46,  1.35s/it][A
  2%|▏         | 199/10907 [04:32<4:00:56,  1.35s/it][A
  2%|▏         | 200/10907 [04:33<3:59:57,  1.34s/it][A
  2%|▏         | 200/10907 [04:33<4:00:20,  1.35s/it][A
  2%|▏         | 200/10907 [04:33<4:00:50,  1.35s/it][A
  2%|▏         | 200/10907 [04:33<4:00:40,  1.35s/it][A

  2%|▏         | 200/10907 [04:33<4:00:51,  1.35s/it][A  2%|▏         | 200/10907 [04:33<4:01:02,  1.35s/it][A
  2%|▏         | 201/10907 [04:35<4:11:47,  1.41s/it][A
  2%|▏         | 201/10907 [04:35<4:11:48,  1.41s/it][A

  2%|▏         | 201/10907 [04:34<4:12:03,  1.41s/it]  2%|▏         | 201/10907 [04:35<4:12:13,  1.41s/it][A[A
  2%|▏         | 201/10907 [04:35<4:12:18,  1.41s/it][A
  2%|▏         | 201/10907 [04:35<4:12:15,  1.41s/it][A
  2%|▏         | 202/10907 [04:36<4:13:05,  1.42s/it][A
  2%|▏         | 202/10907 [04:36<4:13:29,  1.42s/it][A
  2%|▏         | 202/10907 [04:36<4:13:55,  1.42s/it][A
  2%|▏         | 202/10907 [04:36<4:14:13,  1.42s/it][A
  2%|▏         | 202/10907 [04:36<4:14:21,  1.43s/it][A
  2%|▏         | 202/10907 [04:36<4:14:18,  1.43s/it][A
  2%|▏         | 203/10907 [04:37<4:09:00,  1.40s/it][A
  2%|▏         | 203/10907 [04:37<4:09:35,  1.40s/it][A
  2%|▏         | 203/10907 [04:37<4:09:35,  1.40s/it][A
  2%|▏         | 203/10907 [04:37<4:09:56,  1.40s/it][A
  2%|▏         | 203/10907 [04:37<4:09:48,  1.40s/it][A
  2%|▏         | 203/10907 [04:37<4:09:57,  1.40s/it][A
  2%|▏         | 204/10907 [04:39<4:06:45,  1.38s/it][A
  2%|▏         | 204/10907 [04:39<4:06:52,  1.38s/it][A
  2%|▏         | 204/10907 [04:39<4:06:56,  1.38s/it][A

  2%|▏         | 204/10907 [04:39<4:07:02,  1.38s/it][A  2%|▏         | 204/10907 [04:39<4:07:11,  1.39s/it][A
  2%|▏         | 204/10907 [04:39<4:07:09,  1.39s/it][A
  2%|▏         | 205/10907 [04:40<4:04:39,  1.37s/it][A
  2%|▏         | 205/10907 [04:40<4:04:32,  1.37s/it][A
  2%|▏         | 205/10907 [04:40<4:04:42,  1.37s/it][A
  2%|▏         | 205/10907 [04:40<4:04:59,  1.37s/it][A

  2%|▏         | 205/10907 [04:40<4:05:09,  1.37s/it][A  2%|▏         | 205/10907 [04:40<4:05:11,  1.37s/it][A
  2%|▏         | 206/10907 [04:41<4:03:02,  1.36s/it][A
  2%|▏         | 206/10907 [04:41<4:03:04,  1.36s/it][A
  2%|▏         | 206/10907 [04:41<4:03:50,  1.37s/it][A


  2%|▏         | 206/10907 [04:41<4:04:05,  1.37s/it][A  2%|▏         | 206/10907 [04:41<4:03:59,  1.37s/it][A  2%|▏         | 206/10907 [04:41<4:04:00,  1.37s/it][A

  2%|▏         | 207/10907 [04:43<4:02:06,  1.36s/it][A  2%|▏         | 207/10907 [04:43<4:02:07,  1.36s/it][A
  2%|▏         | 207/10907 [04:43<4:02:14,  1.36s/it][A


  2%|▏         | 207/10907 [04:43<4:02:55,  1.36s/it]  2%|▏         | 207/10907 [04:43<4:02:37,  1.36s/it][A[A  2%|▏         | 207/10907 [04:43<4:03:04,  1.36s/it][A
  2%|▏         | 208/10907 [04:44<4:02:13,  1.36s/it][A

  2%|▏         | 208/10907 [04:44<4:02:10,  1.36s/it][A  2%|▏         | 208/10907 [04:44<4:02:04,  1.36s/it][A
  2%|▏         | 208/10907 [04:44<4:02:18,  1.36s/it][A
  2%|▏         | 208/10907 [04:44<4:02:19,  1.36s/it][A
  2%|▏         | 208/10907 [04:44<4:02:27,  1.36s/it][A
  2%|▏         | 209/10907 [04:45<4:01:42,  1.36s/it][A
  2%|▏         | 209/10907 [04:45<4:01:55,  1.36s/it][A
  2%|▏         | 209/10907 [04:45<4:02:05,  1.36s/it][A

  2%|▏         | 209/10907 [04:45<4:02:22,  1.36s/it][A  2%|▏         | 209/10907 [04:45<4:02:15,  1.36s/it][A
  2%|▏         | 209/10907 [04:45<4:02:17,  1.36s/it][A
  2%|▏         | 210/10907 [04:47<4:00:33,  1.35s/it][A
  2%|▏         | 210/10907 [04:47<4:01:08,  1.35s/it][A
  2%|▏         | 210/10907 [04:47<4:01:28,  1.35s/it][A
  2%|▏         | 210/10907 [04:47<4:01:36,  1.36s/it][A
  2%|▏         | 210/10907 [04:47<4:01:46,  1.36s/it][A
  2%|▏         | 210/10907 [04:47<4:01:58,  1.36s/it][A
  2%|▏         | 211/10907 [04:48<4:01:01,  1.35s/it][A

  2%|▏         | 211/10907 [04:48<4:01:32,  1.35s/it][A  2%|▏         | 211/10907 [04:48<4:01:16,  1.35s/it][A
  2%|▏         | 211/10907 [04:48<4:01:21,  1.35s/it][A
  2%|▏         | 211/10907 [04:48<4:01:31,  1.35s/it][A
  2%|▏         | 211/10907 [04:48<4:01:25,  1.35s/it][A
  2%|▏         | 212/10907 [04:49<4:00:50,  1.35s/it][A

  2%|▏         | 212/10907 [04:49<4:00:48,  1.35s/it][A  2%|▏         | 212/10907 [04:49<4:00:47,  1.35s/it][A


  2%|▏         | 212/10907 [04:49<4:01:00,  1.35s/it][A  2%|▏         | 212/10907 [04:49<4:01:01,  1.35s/it][A  2%|▏         | 212/10907 [04:49<4:00:57,  1.35s/it][A
  2%|▏         | 213/10907 [04:51<4:00:14,  1.35s/it][A
  2%|▏         | 213/10907 [04:51<4:00:14,  1.35s/it][A
  2%|▏         | 213/10907 [04:51<4:01:14,  1.35s/it][A
  2%|▏         | 213/10907 [04:51<4:01:10,  1.35s/it][A
  2%|▏         | 213/10907 [04:51<4:01:11,  1.35s/it][A
  2%|▏         | 213/10907 [04:51<4:01:13,  1.35s/it][A
  2%|▏         | 214/10907 [04:52<4:01:06,  1.35s/it][A
  2%|▏         | 214/10907 [04:52<4:00:57,  1.35s/it][A
  2%|▏         | 214/10907 [04:52<4:01:02,  1.35s/it][A
  2%|▏         | 214/10907 [04:52<4:01:09,  1.35s/it][A

  2%|▏         | 214/10907 [04:52<4:01:23,  1.35s/it][A  2%|▏         | 214/10907 [04:52<4:01:25,  1.35s/it][A
  2%|▏         | 215/10907 [04:54<4:01:02,  1.35s/it][A
  2%|▏         | 215/10907 [04:54<4:01:04,  1.35s/it][A
  2%|▏         | 215/10907 [04:53<4:01:06,  1.35s/it][A
  2%|▏         | 215/10907 [04:54<4:01:15,  1.35s/it][A
  2%|▏         | 215/10907 [04:54<4:01:10,  1.35s/it][A
  2%|▏         | 215/10907 [04:54<4:01:14,  1.35s/it][A
  2%|▏         | 216/10907 [04:55<4:00:00,  1.35s/it][A
  2%|▏         | 216/10907 [04:55<4:00:11,  1.35s/it][A
  2%|▏         | 216/10907 [04:55<4:00:18,  1.35s/it][A
  2%|▏         | 216/10907 [04:55<4:00:34,  1.35s/it][A
  2%|▏         | 216/10907 [04:55<4:00:48,  1.35s/it][A
  2%|▏         | 216/10907 [04:55<4:00:50,  1.35s/it][A
  2%|▏         | 217/10907 [04:56<4:08:09,  1.39s/it][A
  2%|▏         | 217/10907 [04:56<4:08:11,  1.39s/it][A

  2%|▏         | 217/10907 [04:56<4:08:07,  1.39s/it][A  2%|▏         | 217/10907 [04:56<4:08:20,  1.39s/it][A
  2%|▏         | 217/10907 [04:56<4:08:12,  1.39s/it][A
  2%|▏         | 217/10907 [04:56<4:08:59,  1.40s/it][A
  2%|▏         | 218/10907 [04:58<4:07:55,  1.39s/it][A
  2%|▏         | 218/10907 [04:58<4:09:19,  1.40s/it][A
  2%|▏         | 218/10907 [04:58<4:10:05,  1.40s/it][A
  2%|▏         | 218/10907 [04:58<4:10:09,  1.40s/it][A
  2%|▏         | 218/10907 [04:58<4:10:16,  1.40s/it][A
  2%|▏         | 218/10907 [04:58<4:10:10,  1.40s/it][A
  2%|▏         | 219/10907 [04:59<4:06:40,  1.38s/it][A

  2%|▏         | 219/10907 [04:59<4:06:42,  1.38s/it][A  2%|▏         | 219/10907 [04:59<4:06:33,  1.38s/it][A
  2%|▏         | 219/10907 [04:59<4:06:41,  1.38s/it][A
  2%|▏         | 219/10907 [04:59<4:07:21,  1.39s/it][A
  2%|▏         | 219/10907 [04:59<4:07:16,  1.39s/it][A
  2%|▏         | 220/10907 [05:00<4:05:04,  1.38s/it][A
  2%|▏         | 220/10907 [05:00<4:05:04,  1.38s/it][A
  2%|▏         | 220/10907 [05:00<4:05:01,  1.38s/it][A
  2%|▏         | 220/10907 [05:00<4:04:54,  1.38s/it][A
  2%|▏         | 220/10907 [05:00<4:05:03,  1.38s/it][A
  2%|▏         | 220/10907 [05:00<4:05:03,  1.38s/it][A
  2%|▏         | 221/10907 [05:02<4:02:53,  1.36s/it][A
  2%|▏         | 221/10907 [05:02<4:02:46,  1.36s/it][A
  2%|▏         | 221/10907 [05:02<4:03:03,  1.36s/it][A

  2%|▏         | 221/10907 [05:02<4:03:25,  1.37s/it][A  2%|▏         | 221/10907 [05:02<4:03:07,  1.37s/it][A
  2%|▏         | 221/10907 [05:02<4:03:21,  1.37s/it][A
  2%|▏         | 222/10907 [05:03<4:02:26,  1.36s/it][A
  2%|▏         | 222/10907 [05:03<4:02:45,  1.36s/it][A

  2%|▏         | 222/10907 [05:03<4:03:26,  1.37s/it][A  2%|▏         | 222/10907 [05:03<4:03:13,  1.37s/it][A

  2%|▏         | 222/10907 [05:03<4:03:43,  1.37s/it][A  2%|▏         | 222/10907 [05:03<4:03:26,  1.37s/it][A
  2%|▏         | 223/10907 [05:05<4:01:16,  1.35s/it][A
  2%|▏         | 223/10907 [05:05<4:01:27,  1.36s/it][A
  2%|▏         | 223/10907 [05:05<4:02:23,  1.36s/it][A
  2%|▏         | 223/10907 [05:05<4:01:59,  1.36s/it][A

  2%|▏         | 223/10907 [05:04<4:02:36,  1.36s/it][A  2%|▏         | 223/10907 [05:05<4:02:05,  1.36s/it][A
  2%|▏         | 224/10907 [05:06<4:00:42,  1.35s/it][A
  2%|▏         | 224/10907 [05:06<4:00:50,  1.35s/it][A
  2%|▏         | 224/10907 [05:06<4:01:29,  1.36s/it][A
  2%|▏         | 224/10907 [05:06<4:01:20,  1.36s/it][A

  2%|▏         | 224/10907 [05:06<4:01:08,  1.35s/it][A  2%|▏         | 224/10907 [05:06<4:01:29,  1.36s/it][A
  2%|▏         | 225/10907 [05:07<4:00:06,  1.35s/it][A
  2%|▏         | 225/10907 [05:07<4:00:49,  1.35s/it][A
  2%|▏         | 225/10907 [05:07<4:00:37,  1.35s/it][A
  2%|▏         | 225/10907 [05:07<4:01:05,  1.35s/it][A

  2%|▏         | 225/10907 [05:07<4:01:32,  1.36s/it][A  2%|▏         | 225/10907 [05:07<4:01:17,  1.36s/it][A
  2%|▏         | 226/10907 [05:09<4:00:34,  1.35s/it][A

  2%|▏         | 226/10907 [05:09<4:00:21,  1.35s/it][A  2%|▏         | 226/10907 [05:09<4:00:11,  1.35s/it][A
  2%|▏         | 226/10907 [05:09<4:00:25,  1.35s/it][A
  2%|▏         | 226/10907 [05:09<4:00:20,  1.35s/it][A
  2%|▏         | 226/10907 [05:08<4:00:54,  1.35s/it][A
  2%|▏         | 227/10907 [05:10<3:59:51,  1.35s/it][A
  2%|▏         | 227/10907 [05:10<4:00:02,  1.35s/it][A
  2%|▏         | 227/10907 [05:10<4:00:17,  1.35s/it][A
  2%|▏         | 227/10907 [05:10<4:00:13,  1.35s/it][A
  2%|▏         | 227/10907 [05:10<4:00:17,  1.35s/it][A
  2%|▏         | 227/10907 [05:10<4:00:18,  1.35s/it][A
  2%|▏         | 228/10907 [05:11<3:59:27,  1.35s/it][A
  2%|▏         | 228/10907 [05:11<3:59:21,  1.34s/it][A
  2%|▏         | 228/10907 [05:11<3:59:51,  1.35s/it][A
  2%|▏         | 228/10907 [05:11<4:00:08,  1.35s/it][A

  2%|▏         | 228/10907 [05:11<4:00:02,  1.35s/it][A  2%|▏         | 228/10907 [05:11<4:00:00,  1.35s/it][A
  2%|▏         | 229/10907 [05:13<3:59:07,  1.34s/it][A
  2%|▏         | 229/10907 [05:13<3:59:32,  1.35s/it][A
  2%|▏         | 229/10907 [05:13<3:59:29,  1.35s/it][A
  2%|▏         | 229/10907 [05:13<3:59:17,  1.34s/it][A
  2%|▏         | 229/10907 [05:13<3:59:42,  1.35s/it][A
  2%|▏         | 229/10907 [05:12<4:00:02,  1.35s/it][A
  2%|▏         | 230/10907 [05:14<3:59:17,  1.34s/it][A
  2%|▏         | 230/10907 [05:14<3:59:13,  1.34s/it][A

  2%|▏         | 230/10907 [05:14<3:59:20,  1.35s/it][A
  2%|▏         | 230/10907 [05:14<3:59:32,  1.35s/it][A  2%|▏         | 230/10907 [05:14<3:59:35,  1.35s/it][A
  2%|▏         | 230/10907 [05:14<3:59:31,  1.35s/it][A
  2%|▏         | 231/10907 [05:15<3:58:34,  1.34s/it][A
  2%|▏         | 231/10907 [05:15<3:58:36,  1.34s/it][A
  2%|▏         | 231/10907 [05:15<3:59:24,  1.35s/it][A
  2%|▏         | 231/10907 [05:15<3:59:25,  1.35s/it][A
  2%|▏         | 231/10907 [05:15<3:59:28,  1.35s/it][A
  2%|▏         | 231/10907 [05:15<3:59:41,  1.35s/it][A
  2%|▏         | 232/10907 [05:17<4:10:01,  1.41s/it][A
  2%|▏         | 232/10907 [05:17<4:10:08,  1.41s/it][A

  2%|▏         | 232/10907 [05:17<4:10:22,  1.41s/it][A  2%|▏         | 232/10907 [05:17<4:10:21,  1.41s/it][A

  2%|▏         | 232/10907 [05:17<4:10:35,  1.41s/it][A  2%|▏         | 232/10907 [05:17<4:10:28,  1.41s/it][A
  2%|▏         | 233/10907 [05:18<4:12:41,  1.42s/it][A
  2%|▏         | 233/10907 [05:18<4:13:00,  1.42s/it][A
  2%|▏         | 233/10907 [05:18<4:13:13,  1.42s/it][A
  2%|▏         | 233/10907 [05:18<4:13:20,  1.42s/it][A
  2%|▏         | 233/10907 [05:18<4:13:32,  1.43s/it][A
  2%|▏         | 233/10907 [05:18<4:13:33,  1.43s/it][A
  2%|▏         | 234/10907 [05:20<4:09:01,  1.40s/it][A
  2%|▏         | 234/10907 [05:20<4:09:13,  1.40s/it][A
  2%|▏         | 234/10907 [05:20<4:09:13,  1.40s/it][A
  2%|▏         | 234/10907 [05:20<4:09:33,  1.40s/it][A
  2%|▏         | 234/10907 [05:20<4:09:24,  1.40s/it][A
  2%|▏         | 234/10907 [05:20<4:09:29,  1.40s/it][A
  2%|▏         | 235/10907 [05:21<4:06:21,  1.39s/it][A
  2%|▏         | 235/10907 [05:21<4:06:31,  1.39s/it][A
  2%|▏         | 235/10907 [05:21<4:06:44,  1.39s/it][A

  2%|▏         | 235/10907 [05:21<4:06:51,  1.39s/it][A  2%|▏         | 235/10907 [05:21<4:06:48,  1.39s/it][A
  2%|▏         | 235/10907 [05:21<4:06:40,  1.39s/it][A
  2%|▏         | 236/10907 [05:22<4:04:35,  1.38s/it][A
  2%|▏         | 236/10907 [05:22<4:04:44,  1.38s/it][A
  2%|▏         | 236/10907 [05:22<4:04:51,  1.38s/it][A
  2%|▏         | 236/10907 [05:22<4:05:04,  1.38s/it][A
  2%|▏         | 236/10907 [05:22<4:05:09,  1.38s/it][A
  2%|▏         | 236/10907 [05:22<4:05:09,  1.38s/it][A
  2%|▏         | 237/10907 [05:24<4:02:42,  1.36s/it][A
  2%|▏         | 237/10907 [05:24<4:02:46,  1.37s/it][A
  2%|▏         | 237/10907 [05:24<4:03:07,  1.37s/it][A
  2%|▏         | 237/10907 [05:24<4:03:03,  1.37s/it][A
  2%|▏         | 237/10907 [05:24<4:03:03,  1.37s/it][A
  2%|▏         | 237/10907 [05:24<4:03:11,  1.37s/it][A
  2%|▏         | 238/10907 [05:25<4:02:04,  1.36s/it][A
  2%|▏         | 238/10907 [05:25<4:02:00,  1.36s/it][A
  2%|▏         | 238/10907 [05:25<4:02:04,  1.36s/it][A

  2%|▏         | 238/10907 [05:25<4:02:39,  1.36s/it][A
  2%|▏         | 238/10907 [05:25<4:02:39,  1.36s/it][A  2%|▏         | 238/10907 [05:25<4:02:34,  1.36s/it][A
  2%|▏         | 239/10907 [05:26<4:00:15,  1.35s/it][A
  2%|▏         | 239/10907 [05:26<4:01:02,  1.36s/it][A
  2%|▏         | 239/10907 [05:26<4:01:27,  1.36s/it][A

  2%|▏         | 239/10907 [05:26<4:01:49,  1.36s/it]  2%|▏         | 239/10907 [05:26<4:02:06,  1.36s/it][A[A
  2%|▏         | 239/10907 [05:26<4:02:04,  1.36s/it][A
  2%|▏         | 240/10907 [05:28<4:00:30,  1.35s/it][A
  2%|▏         | 240/10907 [05:28<4:00:58,  1.36s/it][A
  2%|▏         | 240/10907 [05:28<4:01:01,  1.36s/it][A
  2%|▏         | 240/10907 [05:28<4:01:20,  1.36s/it][A

  2%|▏         | 240/10907 [05:28<4:01:23,  1.36s/it][A  2%|▏         | 240/10907 [05:28<4:01:41,  1.36s/it][A
  2%|▏         | 241/10907 [05:29<4:00:28,  1.35s/it][A
  2%|▏         | 241/10907 [05:29<4:00:39,  1.35s/it][A
  2%|▏         | 241/10907 [05:29<4:00:55,  1.36s/it][A
  2%|▏         | 241/10907 [05:29<4:01:07,  1.36s/it][A
  2%|▏         | 241/10907 [05:29<4:01:03,  1.36s/it][A
  2%|▏         | 241/10907 [05:29<4:01:16,  1.36s/it][A
  2%|▏         | 242/10907 [05:30<4:00:44,  1.35s/it][A

  2%|▏         | 242/10907 [05:30<4:00:53,  1.36s/it][A  2%|▏         | 242/10907 [05:30<4:00:49,  1.35s/it][A
  2%|▏         | 242/10907 [05:30<4:00:58,  1.36s/it][A
  2%|▏         | 242/10907 [05:30<4:01:07,  1.36s/it][A
  2%|▏         | 242/10907 [05:30<4:01:01,  1.36s/it][A
  2%|▏         | 243/10907 [05:32<4:00:24,  1.35s/it][A
  2%|▏         | 243/10907 [05:32<4:00:25,  1.35s/it][A
  2%|▏         | 243/10907 [05:32<4:00:36,  1.35s/it][A
  2%|▏         | 243/10907 [05:32<4:00:41,  1.35s/it][A
  2%|▏         | 243/10907 [05:32<4:00:55,  1.36s/it][A
  2%|▏         | 243/10907 [05:32<4:00:54,  1.36s/it][A
  2%|▏         | 244/10907 [05:33<3:59:17,  1.35s/it][A
  2%|▏         | 244/10907 [05:33<3:59:37,  1.35s/it][A
  2%|▏         | 244/10907 [05:33<3:59:55,  1.35s/it][A
  2%|▏         | 244/10907 [05:33<4:00:01,  1.35s/it][A

  2%|▏         | 244/10907 [05:33<4:00:12,  1.35s/it][A  2%|▏         | 244/10907 [05:33<4:00:07,  1.35s/it][A
  2%|▏         | 245/10907 [05:34<3:59:43,  1.35s/it][A

  2%|▏         | 245/10907 [05:34<3:59:43,  1.35s/it][A  2%|▏         | 245/10907 [05:34<3:59:46,  1.35s/it][A
  2%|▏         | 245/10907 [05:34<3:59:51,  1.35s/it][A
  2%|▏         | 245/10907 [05:35<4:00:03,  1.35s/it][A
  2%|▏         | 245/10907 [05:35<4:00:01,  1.35s/it][A
  2%|▏         | 246/10907 [05:36<3:59:42,  1.35s/it][A
  2%|▏         | 246/10907 [05:36<3:59:43,  1.35s/it][A
  2%|▏         | 246/10907 [05:36<4:00:02,  1.35s/it][A

  2%|▏         | 246/10907 [05:36<4:00:08,  1.35s/it][A  2%|▏         | 246/10907 [05:36<4:00:02,  1.35s/it][A
  2%|▏         | 246/10907 [05:36<4:00:13,  1.35s/it][A
  2%|▏         | 247/10907 [05:37<3:59:28,  1.35s/it][A
  2%|▏         | 247/10907 [05:37<3:59:23,  1.35s/it][A

  2%|▏         | 247/10907 [05:37<3:59:32,  1.35s/it][A  2%|▏         | 247/10907 [05:37<3:59:21,  1.35s/it][A
  2%|▏         | 247/10907 [05:37<3:59:38,  1.35s/it][A
  2%|▏         | 247/10907 [05:37<3:59:38,  1.35s/it][A
  2%|▏         | 248/10907 [05:39<4:06:56,  1.39s/it][A
  2%|▏         | 248/10907 [05:39<4:07:03,  1.39s/it][A
  2%|▏         | 248/10907 [05:39<4:07:13,  1.39s/it][A
  2%|▏         | 248/10907 [05:39<4:07:21,  1.39s/it][A
  2%|▏         | 248/10907 [05:39<4:07:57,  1.40s/it][A
  2%|▏         | 248/10907 [05:39<4:08:09,  1.40s/it][A
  2%|▏         | 249/10907 [05:40<4:08:16,  1.40s/it][A
  2%|▏         | 249/10907 [05:40<4:09:33,  1.40s/it][A
  2%|▏         | 249/10907 [05:40<4:09:28,  1.40s/it][A
  2%|▏         | 249/10907 [05:40<4:09:18,  1.40s/it][A

  2%|▏         | 249/10907 [05:40<4:09:39,  1.41s/it][A  2%|▏         | 249/10907 [05:40<4:09:30,  1.40s/it][A
  2%|▏         | 250/10907 [05:41<4:06:14,  1.39s/it][A
  2%|▏         | 250/10907 [05:41<4:06:35,  1.39s/it][A

  2%|▏         | 250/10907 [05:41<4:06:19,  1.39s/it][A  2%|▏         | 250/10907 [05:41<4:06:33,  1.39s/it][A
  2%|▏         | 250/10907 [05:41<4:06:32,  1.39s/it][A
  2%|▏         | 250/10907 [05:41<4:06:47,  1.39s/it][A
  2%|▏         | 251/10907 [05:43<4:03:40,  1.37s/it][A
  2%|▏         | 251/10907 [05:43<4:04:01,  1.37s/it][A
  2%|▏         | 251/10907 [05:43<4:04:04,  1.37s/it][A
  2%|▏         | 251/10907 [05:43<4:04:16,  1.38s/it][A
  2%|▏         | 251/10907 [05:43<4:04:28,  1.38s/it][A
  2%|▏         | 251/10907 [05:43<4:04:24,  1.38s/it][A
  2%|▏         | 252/10907 [05:44<4:02:33,  1.37s/it][A
  2%|▏         | 252/10907 [05:44<4:02:28,  1.37s/it][A
  2%|▏         | 252/10907 [05:44<4:02:28,  1.37s/it][A
  2%|▏         | 252/10907 [05:44<4:02:25,  1.37s/it][A
  2%|▏         | 252/10907 [05:44<4:02:42,  1.37s/it][A
  2%|▏         | 252/10907 [05:44<4:02:55,  1.37s/it][A
  2%|▏         | 253/10907 [05:46<4:01:26,  1.36s/it][A
  2%|▏         | 253/10907 [05:45<4:01:34,  1.36s/it][A
  2%|▏         | 253/10907 [05:46<4:01:46,  1.36s/it][A


  2%|▏         | 253/10907 [05:46<4:01:31,  1.36s/it][A  2%|▏         | 253/10907 [05:46<4:01:28,  1.36s/it][A  2%|▏         | 253/10907 [05:46<4:01:41,  1.36s/it][A
  2%|▏         | 254/10907 [05:47<4:00:14,  1.35s/it][A
  2%|▏         | 254/10907 [05:47<4:00:16,  1.35s/it][A
  2%|▏         | 254/10907 [05:47<4:00:55,  1.36s/it][A
  2%|▏         | 254/10907 [05:47<4:00:53,  1.36s/it][A
  2%|▏         | 254/10907 [05:47<4:01:03,  1.36s/it][A
  2%|▏         | 254/10907 [05:47<4:01:07,  1.36s/it][A
  2%|▏         | 255/10907 [05:48<3:59:54,  1.35s/it][A
  2%|▏         | 255/10907 [05:48<4:00:22,  1.35s/it][A
  2%|▏         | 255/10907 [05:48<4:00:12,  1.35s/it][A

  2%|▏         | 255/10907 [05:48<4:00:09,  1.35s/it][A  2%|▏         | 255/10907 [05:48<4:00:16,  1.35s/it][A
  2%|▏         | 255/10907 [05:48<4:00:44,  1.36s/it][A
  2%|▏         | 256/10907 [05:50<3:59:59,  1.35s/it][A

  2%|▏         | 256/10907 [05:50<3:59:54,  1.35s/it]  2%|▏         | 256/10907 [05:49<3:59:54,  1.35s/it][A[A


  2%|▏         | 256/10907 [05:50<3:59:52,  1.35s/it][A  2%|▏         | 256/10907 [05:50<3:59:55,  1.35s/it][A  2%|▏         | 256/10907 [05:50<4:00:11,  1.35s/it][A
  2%|▏         | 257/10907 [05:51<3:59:18,  1.35s/it][A
  2%|▏         | 257/10907 [05:51<3:59:25,  1.35s/it][A
  2%|▏         | 257/10907 [05:51<3:59:34,  1.35s/it][A

  2%|▏         | 257/10907 [05:51<3:59:55,  1.35s/it][A  2%|▏         | 257/10907 [05:51<3:59:55,  1.35s/it][A
  2%|▏         | 257/10907 [05:51<3:59:49,  1.35s/it][A
  2%|▏         | 258/10907 [05:52<3:59:13,  1.35s/it][A
  2%|▏         | 258/10907 [05:52<3:59:25,  1.35s/it][A
  2%|▏         | 258/10907 [05:52<3:59:28,  1.35s/it][A
  2%|▏         | 258/10907 [05:52<3:59:45,  1.35s/it][A
  2%|▏         | 258/10907 [05:52<3:59:49,  1.35s/it][A
  2%|▏         | 258/10907 [05:52<3:59:58,  1.35s/it][A
  2%|▏         | 259/10907 [05:54<3:58:44,  1.35s/it][A
  2%|▏         | 259/10907 [05:54<3:58:47,  1.35s/it][A
  2%|▏         | 259/10907 [05:54<3:58:58,  1.35s/it][A

  2%|▏         | 259/10907 [05:54<3:59:09,  1.35s/it][A  2%|▏         | 259/10907 [05:53<3:59:15,  1.35s/it][A
  2%|▏         | 259/10907 [05:54<3:59:08,  1.35s/it][A
  2%|▏         | 260/10907 [05:55<3:58:18,  1.34s/it][A
  2%|▏         | 260/10907 [05:55<3:58:23,  1.34s/it][A
  2%|▏         | 260/10907 [05:55<3:58:26,  1.34s/it][A
  2%|▏         | 260/10907 [05:55<3:58:25,  1.34s/it][A
  2%|▏         | 260/10907 [05:55<3:58:36,  1.34s/it][A
  2%|▏         | 260/10907 [05:55<3:58:42,  1.35s/it][A
  2%|▏         | 261/10907 [05:56<3:58:03,  1.34s/it][A
  2%|▏         | 261/10907 [05:56<3:57:49,  1.34s/it][A
  2%|▏         | 261/10907 [05:56<3:58:02,  1.34s/it][A
  2%|▏         | 261/10907 [05:56<3:58:18,  1.34s/it][A
  2%|▏         | 261/10907 [05:56<3:58:36,  1.34s/it][A
  2%|▏         | 261/10907 [05:56<3:58:43,  1.35s/it][A
  2%|▏         | 262/10907 [05:58<3:58:33,  1.34s/it][A
  2%|▏         | 262/10907 [05:58<3:58:38,  1.35s/it][A
  2%|▏         | 262/10907 [05:58<3:58:57,  1.35s/it][A
  2%|▏         | 262/10907 [05:58<3:58:58,  1.35s/it][A

  2%|▏         | 262/10907 [05:58<3:59:14,  1.35s/it][A  2%|▏         | 262/10907 [05:58<3:59:21,  1.35s/it][A
  2%|▏         | 263/10907 [05:59<4:08:59,  1.40s/it]
[A  2%|▏         | 263/10907 [05:59<4:08:58,  1.40s/it][A

  2%|▏         | 263/10907 [05:59<4:09:02,  1.40s/it][A  2%|▏         | 263/10907 [05:59<4:09:03,  1.40s/it][A
  2%|▏         | 263/10907 [05:59<4:09:13,  1.40s/it][A
  2%|▏         | 263/10907 [05:59<4:09:20,  1.41s/it][A

  2%|▏         | 264/10907 [06:01<4:10:59,  1.41s/it][A  2%|▏         | 264/10907 [06:01<4:11:00,  1.42s/it][A
  2%|▏         | 264/10907 [06:01<4:11:02,  1.42s/it][A
  2%|▏         | 264/10907 [06:00<4:11:21,  1.42s/it][A
  2%|▏         | 264/10907 [06:01<4:11:25,  1.42s/it][A
  2%|▏         | 264/10907 [06:01<4:11:35,  1.42s/it][A
  2%|▏         | 265/10907 [06:02<4:06:49,  1.39s/it][A
  2%|▏         | 265/10907 [06:02<4:06:49,  1.39s/it][A
  2%|▏         | 265/10907 [06:02<4:06:52,  1.39s/it][A
  2%|▏         | 265/10907 [06:02<4:07:19,  1.39s/it][A
  2%|▏         | 265/10907 [06:02<4:07:22,  1.39s/it][A
  2%|▏         | 265/10907 [06:02<4:07:23,  1.39s/it][A
  2%|▏         | 266/10907 [06:03<4:04:06,  1.38s/it][A
  2%|▏         | 266/10907 [06:03<4:04:17,  1.38s/it][A
  2%|▏         | 266/10907 [06:03<4:04:23,  1.38s/it][A
  2%|▏         | 266/10907 [06:03<4:04:22,  1.38s/it][A
  2%|▏         | 266/10907 [06:03<4:04:22,  1.38s/it][A
  2%|▏         | 266/10907 [06:03<4:04:36,  1.38s/it][A
  2%|▏         | 267/10907 [06:05<4:02:30,  1.37s/it][A
  2%|▏         | 267/10907 [06:05<4:02:36,  1.37s/it][A
  2%|▏         | 267/10907 [06:05<4:02:38,  1.37s/it][A
  2%|▏         | 267/10907 [06:05<4:02:40,  1.37s/it][A
  2%|▏         | 267/10907 [06:05<4:02:49,  1.37s/it][A
  2%|▏         | 267/10907 [06:05<4:02:58,  1.37s/it][A
  2%|▏         | 268/10907 [06:06<4:01:27,  1.36s/it][A
  2%|▏         | 268/10907 [06:06<4:01:17,  1.36s/it][A
  2%|▏         | 268/10907 [06:06<4:01:24,  1.36s/it][A
  2%|▏         | 268/10907 [06:06<4:01:43,  1.36s/it][A
  2%|▏         | 268/10907 [06:06<4:01:39,  1.36s/it][A
  2%|▏         | 268/10907 [06:06<4:01:45,  1.36s/it][A
  2%|▏         | 269/10907 [06:07<4:00:27,  1.36s/it][A
  2%|▏         | 269/10907 [06:07<4:00:39,  1.36s/it][A

  2%|▏         | 269/10907 [06:07<4:00:39,  1.36s/it][A  2%|▏         | 269/10907 [06:07<4:00:42,  1.36s/it][A
  2%|▏         | 269/10907 [06:07<4:00:44,  1.36s/it][A
  2%|▏         | 269/10907 [06:07<4:00:41,  1.36s/it][A
  2%|▏         | 270/10907 [06:09<3:59:49,  1.35s/it][A
  2%|▏         | 270/10907 [06:09<3:59:57,  1.35s/it][A
  2%|▏         | 270/10907 [06:09<4:00:03,  1.35s/it][A

  2%|▏         | 270/10907 [06:09<4:00:16,  1.36s/it][A  2%|▏         | 270/10907 [06:09<4:00:09,  1.35s/it][A
  2%|▏         | 270/10907 [06:09<4:00:14,  1.36s/it][A
  2%|▏         | 271/10907 [06:10<3:59:56,  1.35s/it][A
  2%|▏         | 271/10907 [06:10<3:59:55,  1.35s/it][A
  2%|▏         | 271/10907 [06:10<4:00:06,  1.35s/it][A

  2%|▏         | 271/10907 [06:10<4:00:19,  1.36s/it][A
  2%|▏         | 271/10907 [06:10<4:00:15,  1.36s/it][A  2%|▏         | 271/10907 [06:10<4:00:17,  1.36s/it][A
  2%|▏         | 272/10907 [06:11<3:59:23,  1.35s/it][A
  2%|▏         | 272/10907 [06:11<3:59:45,  1.35s/it][A
  2%|▏         | 272/10907 [06:11<4:00:15,  1.36s/it][A
  2%|▏         | 272/10907 [06:11<4:00:32,  1.36s/it][A

  2%|▏         | 272/10907 [06:11<4:00:24,  1.36s/it][A  2%|▏         | 272/10907 [06:11<4:00:22,  1.36s/it][A
  3%|▎         | 273/10907 [06:13<3:58:50,  1.35s/it][A
  3%|▎         | 273/10907 [06:13<3:59:38,  1.35s/it][A
  3%|▎         | 273/10907 [06:13<3:59:55,  1.35s/it][A
  3%|▎         | 273/10907 [06:13<4:00:13,  1.36s/it][A
  3%|▎         | 273/10907 [06:13<4:00:04,  1.35s/it][A
  3%|▎         | 273/10907 [06:13<4:00:27,  1.36s/it][A

  3%|▎         | 274/10907 [06:14<3:59:05,  1.35s/it][A  3%|▎         | 274/10907 [06:14<3:59:13,  1.35s/it][A

  3%|▎         | 274/10907 [06:14<3:59:32,  1.35s/it][A  3%|▎         | 274/10907 [06:14<3:59:58,  1.35s/it][A

  3%|▎         | 274/10907 [06:14<3:59:27,  1.35s/it][A  3%|▎         | 274/10907 [06:14<3:59:44,  1.35s/it][A

  3%|▎         | 275/10907 [06:15<3:59:17,  1.35s/it][A
  3%|▎         | 275/10907 [06:15<3:59:24,  1.35s/it][A
  3%|▎         | 275/10907 [06:15<3:59:28,  1.35s/it][A  3%|▎         | 275/10907 [06:15<3:59:31,  1.35s/it][A
  3%|▎         | 275/10907 [06:15<3:59:27,  1.35s/it][A
  3%|▎         | 275/10907 [06:15<3:59:55,  1.35s/it][A
  3%|▎         | 276/10907 [06:17<3:58:41,  1.35s/it][A
  3%|▎         | 276/10907 [06:17<3:59:14,  1.35s/it][A
  3%|▎         | 276/10907 [06:17<3:59:09,  1.35s/it][A
  3%|▎         | 276/10907 [06:17<3:59:21,  1.35s/it][A
  3%|▎         | 276/10907 [06:17<3:59:32,  1.35s/it][A
  3%|▎         | 276/10907 [06:17<3:59:38,  1.35s/it][A

  3%|▎         | 277/10907 [06:18<3:59:07,  1.35s/it][A

  3%|▎         | 277/10907 [06:18<3:59:06,  1.35s/it][A  3%|▎         | 277/10907 [06:18<3:59:05,  1.35s/it][A  3%|▎         | 277/10907 [06:18<3:59:09,  1.35s/it][A

  3%|▎         | 277/10907 [06:18<3:59:32,  1.35s/it][A  3%|▎         | 277/10907 [06:18<3:59:18,  1.35s/it][A
  3%|▎         | 278/10907 [06:19<3:58:32,  1.35s/it][A
  3%|▎         | 278/10907 [06:19<3:58:18,  1.35s/it][A
  3%|▎         | 278/10907 [06:19<3:58:32,  1.35s/it][A
  3%|▎         | 278/10907 [06:19<3:58:32,  1.35s/it][A

  3%|▎         | 278/10907 [06:19<3:58:32,  1.35s/it][A  3%|▎         | 278/10907 [06:19<3:58:32,  1.35s/it][A
  3%|▎         | 279/10907 [06:21<4:08:50,  1.40s/it][A
  3%|▎         | 279/10907 [06:21<4:09:06,  1.41s/it][A
  3%|▎         | 279/10907 [06:21<4:09:10,  1.41s/it][A
  3%|▎         | 279/10907 [06:21<4:09:28,  1.41s/it][A
  3%|▎         | 279/10907 [06:21<4:09:46,  1.41s/it][A
  3%|▎         | 279/10907 [06:21<4:09:44,  1.41s/it][A
  3%|▎         | 280/10907 [06:22<4:12:33,  1.43s/it][A
  3%|▎         | 280/10907 [06:22<4:12:32,  1.43s/it][A
  3%|▎         | 280/10907 [06:22<4:12:27,  1.43s/it][A
  3%|▎         | 280/10907 [06:22<4:12:39,  1.43s/it][A
  3%|▎         | 280/10907 [06:22<4:12:39,  1.43s/it][A
  3%|▎         | 280/10907 [06:22<4:12:46,  1.43s/it][A
  3%|▎         | 281/10907 [06:24<4:07:16,  1.40s/it][A
  3%|▎         | 281/10907 [06:24<4:07:58,  1.40s/it][A
  3%|▎         | 281/10907 [06:24<4:08:07,  1.40s/it][A
  3%|▎         | 281/10907 [06:24<4:08:14,  1.40s/it][A
  3%|▎         | 281/10907 [06:24<4:08:38,  1.40s/it][A
  3%|▎         | 281/10907 [06:24<4:08:32,  1.40s/it][A
  3%|▎         | 282/10907 [06:25<4:04:17,  1.38s/it][A
  3%|▎         | 282/10907 [06:25<4:04:24,  1.38s/it][A
  3%|▎         | 282/10907 [06:25<4:04:39,  1.38s/it][A
  3%|▎         | 282/10907 [06:25<4:04:45,  1.38s/it][A

  3%|▎         | 282/10907 [06:25<4:04:56,  1.38s/it][A  3%|▎         | 282/10907 [06:25<4:04:51,  1.38s/it][A
  3%|▎         | 283/10907 [06:27<4:02:59,  1.37s/it][A

  3%|▎         | 283/10907 [06:27<4:03:02,  1.37s/it][A  3%|▎         | 283/10907 [06:27<4:03:05,  1.37s/it][A
  3%|▎         | 283/10907 [06:27<4:03:02,  1.37s/it][A

  3%|▎         | 283/10907 [06:27<4:03:25,  1.37s/it][A  3%|▎         | 283/10907 [06:26<4:03:21,  1.37s/it][A
  3%|▎         | 284/10907 [06:28<4:01:30,  1.36s/it][A
  3%|▎         | 284/10907 [06:28<4:01:54,  1.37s/it][A
  3%|▎         | 284/10907 [06:28<4:01:52,  1.37s/it][A

  3%|▎         | 284/10907 [06:28<4:02:18,  1.37s/it][A  3%|▎         | 284/10907 [06:28<4:02:18,  1.37s/it][A
  3%|▎         | 284/10907 [06:28<4:02:21,  1.37s/it][A
  3%|▎         | 285/10907 [06:29<4:01:22,  1.36s/it][A
  3%|▎         | 285/10907 [06:29<4:01:29,  1.36s/it][A
  3%|▎         | 285/10907 [06:29<4:01:56,  1.37s/it][A
  3%|▎         | 285/10907 [06:29<4:01:56,  1.37s/it][A
  3%|▎         | 285/10907 [06:29<4:01:44,  1.37s/it][A
  3%|▎         | 285/10907 [06:29<4:01:56,  1.37s/it][A

  3%|▎         | 286/10907 [06:31<4:00:31,  1.36s/it][A  3%|▎         | 286/10907 [06:30<4:00:30,  1.36s/it][A
  3%|▎         | 286/10907 [06:31<4:01:04,  1.36s/it][A
  3%|▎         | 286/10907 [06:31<4:01:12,  1.36s/it][A
  3%|▎         | 286/10907 [06:31<4:01:20,  1.36s/it][A
  3%|▎         | 286/10907 [06:31<4:01:18,  1.36s/it][A
  3%|▎         | 287/10907 [06:32<3:59:30,  1.35s/it]
[A  3%|▎         | 287/10907 [06:32<3:59:43,  1.35s/it][A
  3%|▎         | 287/10907 [06:32<3:59:49,  1.35s/it][A
  3%|▎         | 287/10907 [06:32<4:00:10,  1.36s/it][A
  3%|▎         | 287/10907 [06:32<4:00:21,  1.36s/it][A
  3%|▎         | 287/10907 [06:32<4:00:35,  1.36s/it][A
  3%|▎         | 288/10907 [06:33<3:59:16,  1.35s/it][A
  3%|▎         | 288/10907 [06:33<3:59:15,  1.35s/it][A
  3%|▎         | 288/10907 [06:33<3:59:20,  1.35s/it][A
  3%|▎         | 288/10907 [06:33<3:59:30,  1.35s/it][A
  3%|▎         | 288/10907 [06:33<3:59:49,  1.36s/it][A
  3%|▎         | 288/10907 [06:33<3:59:42,  1.35s/it][A

  3%|▎         | 289/10907 [06:35<3:58:52,  1.35s/it][A  3%|▎         | 289/10907 [06:35<3:58:41,  1.35s/it][A
  3%|▎         | 289/10907 [06:35<3:58:39,  1.35s/it][A

  3%|▎         | 289/10907 [06:35<3:58:49,  1.35s/it][A  3%|▎         | 289/10907 [06:35<3:58:48,  1.35s/it][A
  3%|▎         | 289/10907 [06:35<3:59:03,  1.35s/it][A
  3%|▎         | 290/10907 [06:36<3:57:47,  1.34s/it][A
  3%|▎         | 290/10907 [06:36<3:57:54,  1.34s/it][A
  3%|▎         | 290/10907 [06:36<3:57:49,  1.34s/it][A

  3%|▎         | 290/10907 [06:36<3:58:30,  1.35s/it][A  3%|▎         | 290/10907 [06:36<3:58:22,  1.35s/it]
[A  3%|▎         | 290/10907 [06:36<3:58:17,  1.35s/it][A
  3%|▎         | 291/10907 [06:37<3:57:28,  1.34s/it][A
  3%|▎         | 291/10907 [06:37<3:57:39,  1.34s/it][A
  3%|▎         | 291/10907 [06:37<3:57:38,  1.34s/it][A
  3%|▎         | 291/10907 [06:37<3:57:53,  1.34s/it][A

  3%|▎         | 291/10907 [06:37<3:57:53,  1.34s/it][A  3%|▎         | 291/10907 [06:37<3:57:48,  1.34s/it][A
  3%|▎         | 292/10907 [06:39<3:57:29,  1.34s/it][A
  3%|▎         | 292/10907 [06:39<3:57:35,  1.34s/it][A
  3%|▎         | 292/10907 [06:39<3:57:44,  1.34s/it][A
  3%|▎         | 292/10907 [06:39<3:57:36,  1.34s/it][A
  3%|▎         | 292/10907 [06:39<3:57:42,  1.34s/it][A
  3%|▎         | 292/10907 [06:39<3:57:44,  1.34s/it][A
  3%|▎         | 293/10907 [06:40<3:57:26,  1.34s/it][A
  3%|▎         | 293/10907 [06:40<3:57:25,  1.34s/it][A
  3%|▎         | 293/10907 [06:40<3:57:56,  1.35s/it][A
  3%|▎         | 293/10907 [06:40<3:58:05,  1.35s/it][A
  3%|▎         | 293/10907 [06:40<3:58:08,  1.35s/it][A
  3%|▎         | 293/10907 [06:40<3:58:04,  1.35s/it][A
  3%|▎         | 294/10907 [06:41<3:57:02,  1.34s/it][A
  3%|▎         | 294/10907 [06:41<3:57:19,  1.34s/it][A

  3%|▎         | 294/10907 [06:41<3:57:18,  1.34s/it]  3%|▎         | 294/10907 [06:41<3:57:12,  1.34s/it][A[A
  3%|▎         | 294/10907 [06:41<3:57:35,  1.34s/it][A
  3%|▎         | 294/10907 [06:41<3:57:31,  1.34s/it][A
  3%|▎         | 295/10907 [06:43<4:03:57,  1.38s/it][A
  3%|▎         | 295/10907 [06:43<4:04:07,  1.38s/it][A
  3%|▎         | 295/10907 [06:43<4:04:26,  1.38s/it][A
  3%|▎         | 295/10907 [06:43<4:04:20,  1.38s/it][A
  3%|▎         | 295/10907 [06:43<4:04:48,  1.38s/it][A
  3%|▎         | 295/10907 [06:43<4:05:06,  1.39s/it][A
  3%|▎         | 296/10907 [06:44<4:05:42,  1.39s/it][A
  3%|▎         | 296/10907 [06:44<4:06:43,  1.40s/it][A
  3%|▎         | 296/10907 [06:44<4:06:37,  1.39s/it][A

  3%|▎         | 296/10907 [06:44<4:06:44,  1.40s/it][A  3%|▎         | 296/10907 [06:44<4:06:38,  1.39s/it]
[A  3%|▎         | 296/10907 [06:44<4:06:47,  1.40s/it][A
  3%|▎         | 297/10907 [06:46<4:04:22,  1.38s/it][A

  3%|▎         | 297/10907 [06:46<4:04:10,  1.38s/it][A  3%|▎         | 297/10907 [06:46<4:04:09,  1.38s/it][A
  3%|▎         | 297/10907 [06:46<4:04:16,  1.38s/it][A
  3%|▎         | 297/10907 [06:46<4:04:16,  1.38s/it][A
  3%|▎         | 297/10907 [06:45<4:04:39,  1.38s/it][A
  3%|▎         | 298/10907 [06:47<4:01:39,  1.37s/it][A
  3%|▎         | 298/10907 [06:47<4:02:08,  1.37s/it][A
  3%|▎         | 298/10907 [06:47<4:02:28,  1.37s/it][A
  3%|▎         | 298/10907 [06:47<4:02:30,  1.37s/it][A
  3%|▎         | 298/10907 [06:47<4:02:24,  1.37s/it][A
  3%|▎         | 298/10907 [06:47<4:02:41,  1.37s/it][A
  3%|▎         | 299/10907 [06:48<4:01:11,  1.36s/it][A
  3%|▎         | 299/10907 [06:48<4:01:28,  1.37s/it][A
  3%|▎         | 299/10907 [06:48<4:01:31,  1.37s/it][A
  3%|▎         | 299/10907 [06:48<4:01:43,  1.37s/it][A
  3%|▎         | 299/10907 [06:48<4:01:30,  1.37s/it][A
  3%|▎         | 299/10907 [06:48<4:01:55,  1.37s/it][A
  3%|▎         | 300/10907 [06:50<4:00:00,  1.36s/it][A
  3%|▎         | 300/10907 [06:50<3:59:55,  1.36s/it][A
  3%|▎         | 300/10907 [06:50<4:00:03,  1.36s/it][A
  3%|▎         | 300/10907 [06:50<3:59:53,  1.36s/it][A
  3%|▎         | 300/10907 [06:50<4:00:04,  1.36s/it][A
  3%|▎         | 300/10907 [06:49<4:00:05,  1.36s/it][A
  3%|▎         | 301/10907 [06:51<3:58:41,  1.35s/it][A
  3%|▎         | 301/10907 [06:51<3:59:02,  1.35s/it][A
  3%|▎         | 301/10907 [06:51<3:59:05,  1.35s/it][A
  3%|▎         | 301/10907 [06:51<3:59:16,  1.35s/it][A
  3%|▎         | 301/10907 [06:51<3:59:25,  1.35s/it][A
  3%|▎         | 301/10907 [06:51<3:59:13,  1.35s/it][A
  3%|▎         | 302/10907 [06:52<3:58:20,  1.35s/it][A

  3%|▎         | 302/10907 [06:52<3:58:27,  1.35s/it][A  3%|▎         | 302/10907 [06:52<3:58:18,  1.35s/it][A

  3%|▎         | 302/10907 [06:52<3:58:26,  1.35s/it][A  3%|▎         | 302/10907 [06:52<3:58:31,  1.35s/it][A
  3%|▎         | 302/10907 [06:52<3:58:37,  1.35s/it][A
  3%|▎         | 303/10907 [06:54<3:58:11,  1.35s/it][A

  3%|▎         | 303/10907 [06:54<3:58:17,  1.35s/it][A
  3%|▎         | 303/10907 [06:54<3:58:17,  1.35s/it][A  3%|▎         | 303/10907 [06:54<3:58:18,  1.35s/it][A
  3%|▎         | 303/10907 [06:54<3:58:15,  1.35s/it][A
  3%|▎         | 303/10907 [06:54<3:58:28,  1.35s/it][A
  3%|▎         | 304/10907 [06:55<3:57:26,  1.34s/it][A
  3%|▎         | 304/10907 [06:55<3:57:36,  1.34s/it][A
  3%|▎         | 304/10907 [06:55<3:58:04,  1.35s/it][A
  3%|▎         | 304/10907 [06:55<3:58:12,  1.35s/it][A

  3%|▎         | 304/10907 [06:55<3:58:13,  1.35s/it][A  3%|▎         | 304/10907 [06:55<3:58:14,  1.35s/it][A
  3%|▎         | 305/10907 [06:56<3:57:53,  1.35s/it][A
  3%|▎         | 305/10907 [06:56<3:58:26,  1.35s/it][A


  3%|▎         | 305/10907 [06:56<3:58:11,  1.35s/it][A
  3%|▎         | 305/10907 [06:56<3:58:08,  1.35s/it][A  3%|▎         | 305/10907 [06:56<3:58:08,  1.35s/it][A  3%|▎         | 305/10907 [06:56<3:58:26,  1.35s/it][A
  3%|▎         | 306/10907 [06:58<3:57:41,  1.35s/it][A
  3%|▎         | 306/10907 [06:58<3:57:44,  1.35s/it][A
  3%|▎         | 306/10907 [06:58<3:57:31,  1.34s/it][A
  3%|▎         | 306/10907 [06:58<3:57:47,  1.35s/it][A
  3%|▎         | 306/10907 [06:58<3:57:57,  1.35s/it][A
  3%|▎         | 306/10907 [06:58<3:57:47,  1.35s/it][A
  3%|▎         | 307/10907 [06:59<3:57:03,  1.34s/it][A
  3%|▎         | 307/10907 [06:59<3:57:22,  1.34s/it][A
  3%|▎         | 307/10907 [06:59<3:57:28,  1.34s/it][A
  3%|▎         | 307/10907 [06:59<3:57:34,  1.34s/it][A
  3%|▎         | 307/10907 [06:59<3:57:57,  1.35s/it][A
  3%|▎         | 307/10907 [06:59<3:57:58,  1.35s/it][A
  3%|▎         | 308/10907 [07:00<3:57:39,  1.35s/it][A
  3%|▎         | 308/10907 [07:00<3:57:35,  1.35s/it][A
  3%|▎         | 308/10907 [07:00<3:57:42,  1.35s/it][A
  3%|▎         | 308/10907 [07:00<3:57:53,  1.35s/it][A
  3%|▎         | 308/10907 [07:00<3:57:45,  1.35s/it][A
  3%|▎         | 308/10907 [07:00<3:57:44,  1.35s/it][A
  3%|▎         | 309/10907 [07:02<3:57:56,  1.35s/it][A
  3%|▎         | 309/10907 [07:02<3:57:54,  1.35s/it][A
  3%|▎         | 309/10907 [07:02<3:57:56,  1.35s/it][A
  3%|▎         | 309/10907 [07:02<3:58:15,  1.35s/it][A
  3%|▎         | 309/10907 [07:02<3:58:20,  1.35s/it][A
  3%|▎         | 309/10907 [07:02<3:58:30,  1.35s/it][A
  3%|▎         | 310/10907 [07:03<4:10:26,  1.42s/it][A
  3%|▎         | 310/10907 [07:03<4:10:31,  1.42s/it][A
  3%|▎         | 310/10907 [07:03<4:10:29,  1.42s/it][A
  3%|▎         | 310/10907 [07:03<4:10:40,  1.42s/it][A

  3%|▎         | 310/10907 [07:03<4:10:44,  1.42s/it]  3%|▎         | 310/10907 [07:03<4:10:43,  1.42s/it][A[A
  3%|▎         | 311/10907 [07:05<4:13:14,  1.43s/it][A
  3%|▎         | 311/10907 [07:05<4:13:17,  1.43s/it][A
  3%|▎         | 311/10907 [07:05<4:13:21,  1.43s/it][A
  3%|▎         | 311/10907 [07:05<4:13:37,  1.44s/it][A
  3%|▎         | 311/10907 [07:05<4:13:57,  1.44s/it][A
  3%|▎         | 311/10907 [07:05<4:14:09,  1.44s/it][A
  3%|▎         | 312/10907 [07:06<4:08:39,  1.41s/it][A
  3%|▎         | 312/10907 [07:06<4:08:46,  1.41s/it][A
  3%|▎         | 312/10907 [07:06<4:08:38,  1.41s/it][A
  3%|▎         | 312/10907 [07:06<4:08:47,  1.41s/it][A

  3%|▎         | 312/10907 [07:06<4:09:10,  1.41s/it][A  3%|▎         | 312/10907 [07:06<4:09:11,  1.41s/it][A
  3%|▎         | 313/10907 [07:07<4:05:12,  1.39s/it][A
  3%|▎         | 313/10907 [07:07<4:05:30,  1.39s/it][A

  3%|▎         | 313/10907 [07:07<4:05:23,  1.39s/it][A  3%|▎         | 313/10907 [07:07<4:05:21,  1.39s/it][A
  3%|▎         | 313/10907 [07:07<4:05:33,  1.39s/it][A
  3%|▎         | 313/10907 [07:07<4:05:35,  1.39s/it][A
  3%|▎         | 314/10907 [07:09<4:02:38,  1.37s/it][A
  3%|▎         | 314/10907 [07:09<4:02:44,  1.37s/it][A

  3%|▎         | 314/10907 [07:09<4:03:23,  1.38s/it][A  3%|▎         | 314/10907 [07:09<4:03:24,  1.38s/it][A
  3%|▎         | 314/10907 [07:09<4:03:32,  1.38s/it][A
  3%|▎         | 314/10907 [07:09<4:03:26,  1.38s/it][A
  3%|▎         | 315/10907 [07:10<4:01:01,  1.37s/it][A
  3%|▎         | 315/10907 [07:10<4:01:03,  1.37s/it][A
  3%|▎         | 315/10907 [07:10<4:01:03,  1.37s/it][A
  3%|▎         | 315/10907 [07:10<4:01:18,  1.37s/it][A
  3%|▎         | 315/10907 [07:10<4:01:27,  1.37s/it][A
  3%|▎         | 315/10907 [07:10<4:01:28,  1.37s/it][A
  3%|▎         | 316/10907 [07:11<3:59:56,  1.36s/it][A
  3%|▎         | 316/10907 [07:11<4:00:11,  1.36s/it][A


  3%|▎         | 316/10907 [07:12<4:00:24,  1.36s/it][A
  3%|▎         | 316/10907 [07:12<4:00:22,  1.36s/it][A  3%|▎         | 316/10907 [07:12<4:00:13,  1.36s/it][A  3%|▎         | 316/10907 [07:12<4:00:22,  1.36s/it][A
  3%|▎         | 317/10907 [07:13<3:58:53,  1.35s/it][A

  3%|▎         | 317/10907 [07:13<3:58:56,  1.35s/it][A  3%|▎         | 317/10907 [07:13<3:58:55,  1.35s/it][A
  3%|▎         | 317/10907 [07:13<3:59:06,  1.35s/it][A
  3%|▎         | 317/10907 [07:13<3:59:14,  1.36s/it][A
  3%|▎         | 317/10907 [07:13<3:59:13,  1.36s/it][A
  3%|▎         | 318/10907 [07:14<3:58:18,  1.35s/it][A
  3%|▎         | 318/10907 [07:14<3:58:34,  1.35s/it][A

  3%|▎         | 318/10907 [07:14<3:58:36,  1.35s/it][A  3%|▎         | 318/10907 [07:14<3:58:40,  1.35s/it][A

  3%|▎         | 318/10907 [07:14<3:58:45,  1.35s/it][A  3%|▎         | 318/10907 [07:14<3:58:43,  1.35s/it][A
  3%|▎         | 319/10907 [07:16<3:57:58,  1.35s/it][A
  3%|▎         | 319/10907 [07:16<3:58:20,  1.35s/it][A
  3%|▎         | 319/10907 [07:16<3:58:28,  1.35s/it][A

  3%|▎         | 319/10907 [07:15<3:58:48,  1.35s/it][A
  3%|▎         | 319/10907 [07:16<3:58:44,  1.35s/it][A  3%|▎         | 319/10907 [07:16<3:58:50,  1.35s/it][A
  3%|▎         | 320/10907 [07:17<3:57:11,  1.34s/it][A
  3%|▎         | 320/10907 [07:17<3:57:34,  1.35s/it][A
  3%|▎         | 320/10907 [07:17<3:57:42,  1.35s/it][A
  3%|▎         | 320/10907 [07:17<3:57:47,  1.35s/it][A
  3%|▎         | 320/10907 [07:17<3:57:46,  1.35s/it][A
  3%|▎         | 320/10907 [07:17<3:57:46,  1.35s/it][A
  3%|▎         | 321/10907 [07:18<3:57:00,  1.34s/it][A
  3%|▎         | 321/10907 [07:18<3:56:54,  1.34s/it][A
  3%|▎         | 321/10907 [07:18<3:57:49,  1.35s/it][A
  3%|▎         | 321/10907 [07:18<3:57:53,  1.35s/it][A

  3%|▎         | 321/10907 [07:18<3:58:17,  1.35s/it][A  3%|▎         | 321/10907 [07:18<3:58:13,  1.35s/it][A
  3%|▎         | 322/10907 [07:20<3:56:55,  1.34s/it][A
  3%|▎         | 322/10907 [07:20<3:57:27,  1.35s/it][A
  3%|▎         | 322/10907 [07:20<3:57:15,  1.34s/it][A
  3%|▎         | 322/10907 [07:20<3:57:46,  1.35s/it][A
  3%|▎         | 322/10907 [07:19<3:57:50,  1.35s/it][A
  3%|▎         | 322/10907 [07:20<3:58:03,  1.35s/it][A
  3%|▎         | 323/10907 [07:21<3:57:35,  1.35s/it][A
  3%|▎         | 323/10907 [07:21<3:57:36,  1.35s/it][A
  3%|▎         | 323/10907 [07:21<3:58:06,  1.35s/it][A

  3%|▎         | 323/10907 [07:21<3:58:26,  1.35s/it][A  3%|▎         | 323/10907 [07:21<3:58:04,  1.35s/it][A
  3%|▎         | 323/10907 [07:21<3:58:23,  1.35s/it][A
  3%|▎         | 324/10907 [07:22<3:57:52,  1.35s/it][A
  3%|▎         | 324/10907 [07:22<3:57:42,  1.35s/it][A
  3%|▎         | 324/10907 [07:22<3:57:59,  1.35s/it][A
  3%|▎         | 324/10907 [07:22<3:58:11,  1.35s/it][A
  3%|▎         | 324/10907 [07:22<3:58:20,  1.35s/it][A
  3%|▎         | 324/10907 [07:22<3:58:26,  1.35s/it][A
  3%|▎         | 325/10907 [07:24<3:57:46,  1.35s/it][A
  3%|▎         | 325/10907 [07:24<3:57:30,  1.35s/it][A
  3%|▎         | 325/10907 [07:24<3:57:43,  1.35s/it][A
  3%|▎         | 325/10907 [07:24<3:58:08,  1.35s/it][A
  3%|▎         | 325/10907 [07:24<3:57:55,  1.35s/it][A
  3%|▎         | 325/10907 [07:24<3:58:22,  1.35s/it][A
  3%|▎         | 326/10907 [07:25<4:04:04,  1.38s/it][A
  3%|▎         | 326/10907 [07:25<4:04:05,  1.38s/it][A
  3%|▎         | 326/10907 [07:25<4:04:06,  1.38s/it][A
  3%|▎         | 326/10907 [07:25<4:04:07,  1.38s/it][A
  3%|▎         | 326/10907 [07:25<4:04:33,  1.39s/it][A
  3%|▎         | 326/10907 [07:25<4:04:49,  1.39s/it][A
  3%|▎         | 327/10907 [07:26<4:05:05,  1.39s/it][A

  3%|▎         | 327/10907 [07:27<4:06:00,  1.40s/it][A  3%|▎         | 327/10907 [07:27<4:05:57,  1.39s/it][A
  3%|▎         | 327/10907 [07:27<4:05:55,  1.39s/it][A
  3%|▎         | 327/10907 [07:27<4:05:55,  1.39s/it][A
  3%|▎         | 327/10907 [07:27<4:06:12,  1.40s/it][A
  3%|▎         | 328/10907 [07:28<4:03:11,  1.38s/it][A
  3%|▎         | 328/10907 [07:28<4:03:14,  1.38s/it][A
  3%|▎         | 328/10907 [07:28<4:03:18,  1.38s/it][A
  3%|▎         | 328/10907 [07:28<4:03:23,  1.38s/it][A
  3%|▎         | 328/10907 [07:28<4:03:20,  1.38s/it][A
  3%|▎         | 328/10907 [07:28<4:03:31,  1.38s/it][A
  3%|▎         | 329/10907 [07:29<4:01:44,  1.37s/it][A
  3%|▎         | 329/10907 [07:29<4:01:50,  1.37s/it][A
  3%|▎         | 329/10907 [07:29<4:01:59,  1.37s/it][A
  3%|▎         | 329/10907 [07:29<4:01:57,  1.37s/it][A

  3%|▎         | 329/10907 [07:29<4:02:10,  1.37s/it][A  3%|▎         | 329/10907 [07:29<4:02:11,  1.37s/it][A
  3%|▎         | 330/10907 [07:31<4:00:17,  1.36s/it][A

  3%|▎         | 330/10907 [07:31<4:00:22,  1.36s/it][A
  3%|▎         | 330/10907 [07:31<4:00:21,  1.36s/it][A  3%|▎         | 330/10907 [07:31<4:00:23,  1.36s/it][A
  3%|▎         | 330/10907 [07:30<4:00:41,  1.37s/it][A
  3%|▎         | 330/10907 [07:31<4:00:41,  1.37s/it][A
  3%|▎         | 331/10907 [07:32<3:59:02,  1.36s/it][A
  3%|▎         | 331/10907 [07:32<3:59:05,  1.36s/it][A
  3%|▎         | 331/10907 [07:32<3:59:24,  1.36s/it][A
  3%|▎         | 331/10907 [07:32<3:59:26,  1.36s/it][A

  3%|▎         | 331/10907 [07:32<3:59:41,  1.36s/it][A  3%|▎         | 331/10907 [07:32<3:59:41,  1.36s/it][A
  3%|▎         | 332/10907 [07:33<3:58:22,  1.35s/it][A
  3%|▎         | 332/10907 [07:33<3:58:25,  1.35s/it][A
  3%|▎         | 332/10907 [07:33<3:58:38,  1.35s/it][A


  3%|▎         | 332/10907 [07:33<3:58:46,  1.35s/it][A  3%|▎         | 332/10907 [07:33<3:58:46,  1.35s/it][A  3%|▎         | 332/10907 [07:33<3:58:40,  1.35s/it][A
  3%|▎         | 333/10907 [07:35<3:57:32,  1.35s/it][A
  3%|▎         | 333/10907 [07:35<3:57:56,  1.35s/it][A
  3%|▎         | 333/10907 [07:35<3:58:05,  1.35s/it][A
  3%|▎         | 333/10907 [07:35<3:58:05,  1.35s/it][A
  3%|▎         | 333/10907 [07:35<3:58:04,  1.35s/it][A
  3%|▎         | 333/10907 [07:34<3:58:27,  1.35s/it][A
  3%|▎         | 334/10907 [07:36<3:57:53,  1.35s/it][A
  3%|▎         | 334/10907 [07:36<3:57:37,  1.35s/it][A
  3%|▎         | 334/10907 [07:36<3:57:53,  1.35s/it][A
  3%|▎         | 334/10907 [07:36<3:58:00,  1.35s/it][A
  3%|▎         | 334/10907 [07:36<3:58:05,  1.35s/it][A
  3%|▎         | 334/10907 [07:36<3:58:05,  1.35s/it][A

  3%|▎         | 335/10907 [07:37<3:57:52,  1.35s/it][A  3%|▎         | 335/10907 [07:37<3:57:40,  1.35s/it][A
  3%|▎         | 335/10907 [07:37<3:57:42,  1.35s/it][A

  3%|▎         | 335/10907 [07:37<3:57:44,  1.35s/it][A  3%|▎         | 335/10907 [07:37<3:57:44,  1.35s/it][A
  3%|▎         | 335/10907 [07:37<3:57:47,  1.35s/it][A
  3%|▎         | 336/10907 [07:39<3:56:36,  1.34s/it][A
  3%|▎         | 336/10907 [07:39<3:56:50,  1.34s/it][A
  3%|▎         | 336/10907 [07:39<3:57:13,  1.35s/it][A
  3%|▎         | 336/10907 [07:39<3:57:27,  1.35s/it][A
  3%|▎         | 336/10907 [07:39<3:57:44,  1.35s/it][A
  3%|▎         | 336/10907 [07:39<3:57:44,  1.35s/it][A
  3%|▎         | 337/10907 [07:40<3:56:40,  1.34s/it][A
  3%|▎         | 337/10907 [07:40<3:56:46,  1.34s/it][A
  3%|▎         | 337/10907 [07:40<3:56:59,  1.35s/it][A
  3%|▎         | 337/10907 [07:40<3:57:28,  1.35s/it][A
  3%|▎         | 337/10907 [07:40<3:57:41,  1.35s/it][A
  3%|▎         | 337/10907 [07:40<3:57:38,  1.35s/it][A
  3%|▎         | 338/10907 [07:41<3:57:08,  1.35s/it][A

  3%|▎         | 338/10907 [07:41<3:57:20,  1.35s/it][A  3%|▎         | 338/10907 [07:41<3:57:17,  1.35s/it][A


  3%|▎         | 338/10907 [07:41<3:57:23,  1.35s/it][A  3%|▎         | 338/10907 [07:41<3:57:20,  1.35s/it]  3%|▎         | 338/10907 [07:41<3:57:22,  1.35s/it][A[A
  3%|▎         | 339/10907 [07:43<3:56:58,  1.35s/it][A
  3%|▎         | 339/10907 [07:43<3:57:16,  1.35s/it][A

  3%|▎         | 339/10907 [07:43<3:57:24,  1.35s/it][A  3%|▎         | 339/10907 [07:43<3:57:24,  1.35s/it][A

  3%|▎         | 339/10907 [07:43<3:57:43,  1.35s/it][A  3%|▎         | 339/10907 [07:43<3:57:34,  1.35s/it][A
  3%|▎         | 340/10907 [07:44<3:56:33,  1.34s/it][A
  3%|▎         | 340/10907 [07:44<3:56:46,  1.34s/it][A
  3%|▎         | 340/10907 [07:44<3:57:00,  1.35s/it][A
  3%|▎         | 340/10907 [07:44<3:57:13,  1.35s/it][A
  3%|▎         | 340/10907 [07:44<3:57:22,  1.35s/it][A
  3%|▎         | 340/10907 [07:44<3:57:44,  1.35s/it][A
  3%|▎         | 341/10907 [07:46<4:07:58,  1.41s/it][A
  3%|▎         | 341/10907 [07:46<4:08:16,  1.41s/it][A
  3%|▎         | 341/10907 [07:45<4:08:36,  1.41s/it][A
  3%|▎         | 341/10907 [07:46<4:08:33,  1.41s/it][A
  3%|▎         | 341/10907 [07:46<4:08:30,  1.41s/it][A
  3%|▎         | 341/10907 [07:46<4:08:37,  1.41s/it][A

  3%|▎         | 342/10907 [07:47<4:11:32,  1.43s/it][A  3%|▎         | 342/10907 [07:47<4:11:32,  1.43s/it][A
  3%|▎         | 342/10907 [07:47<4:11:25,  1.43s/it][A
  3%|▎         | 342/10907 [07:47<4:11:43,  1.43s/it][A
  3%|▎         | 342/10907 [07:47<4:11:38,  1.43s/it][A
  3%|▎         | 342/10907 [07:47<4:11:41,  1.43s/it][A
  3%|▎         | 343/10907 [07:48<4:07:14,  1.40s/it][A
  3%|▎         | 343/10907 [07:48<4:07:40,  1.41s/it][A

  3%|▎         | 343/10907 [07:48<4:07:37,  1.41s/it][A
  3%|▎         | 343/10907 [07:48<4:07:38,  1.41s/it][A
  3%|▎         | 343/10907 [07:48<4:07:47,  1.41s/it][A  3%|▎         | 343/10907 [07:48<4:07:44,  1.41s/it][A
  3%|▎         | 344/10907 [07:50<4:04:39,  1.39s/it][A
  3%|▎         | 344/10907 [07:50<4:04:38,  1.39s/it][A

  3%|▎         | 344/10907 [07:50<4:04:43,  1.39s/it][A  3%|▎         | 344/10907 [07:50<4:04:38,  1.39s/it][A
  3%|▎         | 344/10907 [07:50<4:04:50,  1.39s/it][A
  3%|▎         | 344/10907 [07:50<4:04:49,  1.39s/it][A
  3%|▎         | 345/10907 [07:51<4:01:50,  1.37s/it][A
  3%|▎         | 345/10907 [07:51<4:01:51,  1.37s/it][A
  3%|▎         | 345/10907 [07:51<4:02:07,  1.38s/it][A
  3%|▎         | 345/10907 [07:51<4:02:30,  1.38s/it][A

  3%|▎         | 345/10907 [07:51<4:02:25,  1.38s/it][A  3%|▎         | 345/10907 [07:51<4:02:30,  1.38s/it][A
  3%|▎         | 346/10907 [07:52<4:00:45,  1.37s/it][A
  3%|▎         | 346/10907 [07:52<4:00:55,  1.37s/it][A

  3%|▎         | 346/10907 [07:52<4:01:00,  1.37s/it][A  3%|▎         | 346/10907 [07:52<4:00:46,  1.37s/it][A
  3%|▎         | 346/10907 [07:52<4:00:58,  1.37s/it][A
  3%|▎         | 346/10907 [07:52<4:00:49,  1.37s/it][A
  3%|▎         | 347/10907 [07:54<4:07:02,  1.40s/it][A
  3%|▎         | 347/10907 [07:54<4:08:03,  1.41s/it][A
  3%|▎         | 347/10907 [07:54<4:08:50,  1.41s/it][A
  3%|▎         | 347/10907 [07:54<4:08:51,  1.41s/it][A
  3%|▎         | 347/10907 [07:54<4:09:23,  1.42s/it][A
  3%|▎         | 347/10907 [07:54<4:09:30,  1.42s/it][A
  3%|▎         | 348/10907 [07:55<4:02:33,  1.38s/it][A

  3%|▎         | 348/10907 [07:55<4:02:37,  1.38s/it][A  3%|▎         | 348/10907 [07:55<4:02:28,  1.38s/it][A
  3%|▎         | 348/10907 [07:55<4:02:51,  1.38s/it][A
  3%|▎         | 348/10907 [07:55<4:03:06,  1.38s/it][A
  3%|▎         | 348/10907 [07:55<4:03:17,  1.38s/it][A
  3%|▎         | 349/10907 [07:57<4:00:13,  1.37s/it][A
  3%|▎         | 349/10907 [07:56<4:00:34,  1.37s/it][A
  3%|▎         | 349/10907 [07:57<4:00:43,  1.37s/it][A
  3%|▎         | 349/10907 [07:57<4:00:55,  1.37s/it][A
  3%|▎         | 349/10907 [07:57<4:00:55,  1.37s/it][A
  3%|▎         | 349/10907 [07:57<4:00:44,  1.37s/it][A
  3%|▎         | 350/10907 [07:58<3:58:49,  1.36s/it][A
  3%|▎         | 350/10907 [07:58<3:59:27,  1.36s/it][A
  3%|▎         | 350/10907 [07:58<3:59:48,  1.36s/it][A
  3%|▎         | 350/10907 [07:58<4:00:08,  1.36s/it][A
  3%|▎         | 350/10907 [07:58<4:00:02,  1.36s/it][A
  3%|▎         | 350/10907 [07:58<3:59:47,  1.36s/it][A
  3%|▎         | 351/10907 [07:59<3:58:48,  1.36s/it][A
  3%|▎         | 351/10907 [07:59<3:59:09,  1.36s/it][A
  3%|▎         | 351/10907 [07:59<3:59:20,  1.36s/it][A
  3%|▎         | 351/10907 [07:59<3:59:43,  1.36s/it][A
  3%|▎         | 351/10907 [07:59<3:59:47,  1.36s/it][A
  3%|▎         | 351/10907 [07:59<3:59:16,  1.36s/it][A

  3%|▎         | 352/10907 [08:01<3:58:23,  1.36s/it][A  3%|▎         | 352/10907 [08:01<3:58:22,  1.36s/it][A

  3%|▎         | 352/10907 [08:01<3:58:04,  1.35s/it][A  3%|▎         | 352/10907 [08:01<3:58:28,  1.36s/it][A
  3%|▎         | 352/10907 [08:01<3:58:28,  1.36s/it][A
  3%|▎         | 352/10907 [08:01<3:58:44,  1.36s/it][A
  3%|▎         | 353/10907 [08:02<3:57:37,  1.35s/it][A
  3%|▎         | 353/10907 [08:02<3:57:55,  1.35s/it][A
  3%|▎         | 353/10907 [08:02<3:57:58,  1.35s/it][A

  3%|▎         | 353/10907 [08:02<3:57:57,  1.35s/it][A  3%|▎         | 353/10907 [08:02<3:57:43,  1.35s/it][A
  3%|▎         | 353/10907 [08:02<3:57:54,  1.35s/it][A
  3%|▎         | 354/10907 [08:03<3:56:55,  1.35s/it][A
  3%|▎         | 354/10907 [08:03<3:56:58,  1.35s/it][A
  3%|▎         | 354/10907 [08:03<3:57:11,  1.35s/it][A
  3%|▎         | 354/10907 [08:03<3:57:35,  1.35s/it][A
  3%|▎         | 354/10907 [08:03<3:57:43,  1.35s/it][A
  3%|▎         | 354/10907 [08:03<3:57:44,  1.35s/it][A
  3%|▎         | 355/10907 [08:05<3:56:26,  1.34s/it][A
  3%|▎         | 355/10907 [08:05<3:56:43,  1.35s/it][A
  3%|▎         | 355/10907 [08:05<3:57:04,  1.35s/it][A
  3%|▎         | 355/10907 [08:05<3:56:57,  1.35s/it][A
  3%|▎         | 355/10907 [08:05<3:57:05,  1.35s/it][A
  3%|▎         | 355/10907 [08:05<3:57:20,  1.35s/it][A
  3%|▎         | 356/10907 [08:06<3:56:47,  1.35s/it][A
  3%|▎         | 356/10907 [08:06<3:56:50,  1.35s/it][A
  3%|▎         | 356/10907 [08:06<3:56:46,  1.35s/it][A
  3%|▎         | 356/10907 [08:06<3:57:13,  1.35s/it][A
  3%|▎         | 356/10907 [08:06<3:57:15,  1.35s/it][A
  3%|▎         | 356/10907 [08:06<3:57:36,  1.35s/it][A
  3%|▎         | 357/10907 [08:07<4:02:45,  1.38s/it][A
  3%|▎         | 357/10907 [08:08<4:03:52,  1.39s/it][A

  3%|▎         | 357/10907 [08:08<4:03:56,  1.39s/it][A  3%|▎         | 357/10907 [08:08<4:03:50,  1.39s/it][A
  3%|▎         | 357/10907 [08:08<4:04:02,  1.39s/it][A
  3%|▎         | 357/10907 [08:07<4:04:07,  1.39s/it][A
  3%|▎         | 358/10907 [08:09<4:05:35,  1.40s/it][A
  3%|▎         | 358/10907 [08:09<4:05:16,  1.40s/it][A
  3%|▎         | 358/10907 [08:09<4:05:25,  1.40s/it][A
  3%|▎         | 358/10907 [08:09<4:05:54,  1.40s/it][A
  3%|▎         | 358/10907 [08:09<4:06:09,  1.40s/it][A
  3%|▎         | 358/10907 [08:09<4:06:37,  1.40s/it][A
  3%|▎         | 359/10907 [08:10<4:02:34,  1.38s/it][A
  3%|▎         | 359/10907 [08:10<4:02:42,  1.38s/it][A
  3%|▎         | 359/10907 [08:10<4:02:47,  1.38s/it][A
  3%|▎         | 359/10907 [08:10<4:03:04,  1.38s/it][A
  3%|▎         | 359/10907 [08:10<4:03:19,  1.38s/it][A
  3%|▎         | 359/10907 [08:10<4:03:05,  1.38s/it][A
  3%|▎         | 360/10907 [08:12<4:01:09,  1.37s/it][A
  3%|▎         | 360/10907 [08:12<4:01:10,  1.37s/it][A
  3%|▎         | 360/10907 [08:12<4:01:15,  1.37s/it][A
  3%|▎         | 360/10907 [08:12<4:01:27,  1.37s/it][A
  3%|▎         | 360/10907 [08:12<4:01:19,  1.37s/it][A
  3%|▎         | 360/10907 [08:12<4:01:26,  1.37s/it][A
  3%|▎         | 361/10907 [08:13<3:59:08,  1.36s/it][A
  3%|▎         | 361/10907 [08:13<3:59:41,  1.36s/it][A

  3%|▎         | 361/10907 [08:13<3:59:46,  1.36s/it][A  3%|▎         | 361/10907 [08:13<3:59:51,  1.36s/it][A
  3%|▎         | 361/10907 [08:13<3:59:43,  1.36s/it][A
  3%|▎         | 361/10907 [08:13<3:59:39,  1.36s/it][A
  3%|▎         | 362/10907 [08:14<3:58:42,  1.36s/it][A
  3%|▎         | 362/10907 [08:14<3:58:41,  1.36s/it][A
  3%|▎         | 362/10907 [08:14<3:58:42,  1.36s/it][A
  3%|▎         | 362/10907 [08:14<3:58:56,  1.36s/it][A

  3%|▎         | 362/10907 [08:14<3:58:46,  1.36s/it]  3%|▎         | 362/10907 [08:14<3:59:00,  1.36s/it][A[A
  3%|▎         | 363/10907 [08:16<3:58:26,  1.36s/it][A
  3%|▎         | 363/10907 [08:16<3:58:21,  1.36s/it][A
  3%|▎         | 363/10907 [08:16<3:58:22,  1.36s/it][A
  3%|▎         | 363/10907 [08:16<3:58:27,  1.36s/it][A
  3%|▎         | 363/10907 [08:16<3:58:31,  1.36s/it][A
  3%|▎         | 363/10907 [08:16<3:58:35,  1.36s/it][A
  3%|▎         | 364/10907 [08:17<3:58:14,  1.36s/it][A

  3%|▎         | 364/10907 [08:17<3:58:32,  1.36s/it][A  3%|▎         | 364/10907 [08:17<3:58:31,  1.36s/it][A
  3%|▎         | 364/10907 [08:17<3:58:41,  1.36s/it][A
  3%|▎         | 364/10907 [08:17<3:58:34,  1.36s/it][A
  3%|▎         | 364/10907 [08:17<3:58:23,  1.36s/it][A

  3%|▎         | 365/10907 [08:18<3:57:28,  1.35s/it][A  3%|▎         | 365/10907 [08:18<3:57:34,  1.35s/it][A
  3%|▎         | 365/10907 [08:18<3:57:38,  1.35s/it][A
  3%|▎         | 365/10907 [08:18<3:57:40,  1.35s/it][A
  3%|▎         | 365/10907 [08:18<3:57:37,  1.35s/it]
[A  3%|▎         | 365/10907 [08:18<3:57:55,  1.35s/it][A
  3%|▎         | 366/10907 [08:20<3:56:35,  1.35s/it][A
  3%|▎         | 366/10907 [08:20<3:56:49,  1.35s/it][A
  3%|▎         | 366/10907 [08:20<3:56:59,  1.35s/it][A
  3%|▎         | 366/10907 [08:20<3:56:58,  1.35s/it][A

  3%|▎         | 366/10907 [08:20<3:57:08,  1.35s/it][A  3%|▎         | 366/10907 [08:20<3:56:57,  1.35s/it][A
  3%|▎         | 367/10907 [08:21<3:56:02,  1.34s/it][A
  3%|▎         | 367/10907 [08:21<3:56:08,  1.34s/it][A

  3%|▎         | 367/10907 [08:21<3:56:37,  1.35s/it][A  3%|▎         | 367/10907 [08:21<3:56:42,  1.35s/it][A
  3%|▎         | 367/10907 [08:21<3:56:34,  1.35s/it][A
  3%|▎         | 367/10907 [08:21<3:56:28,  1.35s/it][A
  3%|▎         | 368/10907 [08:22<3:56:15,  1.35s/it][A
  3%|▎         | 368/10907 [08:22<3:56:15,  1.35s/it][A
  3%|▎         | 368/10907 [08:22<3:56:12,  1.34s/it][A
  3%|▎         | 368/10907 [08:22<3:56:27,  1.35s/it][A
  3%|▎         | 368/10907 [08:22<3:56:43,  1.35s/it][A
  3%|▎         | 368/10907 [08:22<3:56:45,  1.35s/it][A
  3%|▎         | 369/10907 [08:24<3:56:26,  1.35s/it][A
  3%|▎         | 369/10907 [08:24<3:56:28,  1.35s/it][A
  3%|▎         | 369/10907 [08:24<3:56:58,  1.35s/it][A
  3%|▎         | 369/10907 [08:24<3:56:58,  1.35s/it][A
  3%|▎         | 369/10907 [08:24<3:57:07,  1.35s/it][A
  3%|▎         | 369/10907 [08:24<3:57:06,  1.35s/it][A
  3%|▎         | 370/10907 [08:25<3:56:10,  1.34s/it][A
  3%|▎         | 370/10907 [08:25<3:56:06,  1.34s/it][A
  3%|▎         | 370/10907 [08:25<3:56:32,  1.35s/it][A
  3%|▎         | 370/10907 [08:25<3:56:35,  1.35s/it][A
  3%|▎         | 370/10907 [08:25<3:56:52,  1.35s/it][A
  3%|▎         | 370/10907 [08:25<3:56:45,  1.35s/it][A
  3%|▎         | 371/10907 [08:26<3:56:36,  1.35s/it][A
  3%|▎         | 371/10907 [08:26<3:56:49,  1.35s/it][A
  3%|▎         | 371/10907 [08:26<3:56:43,  1.35s/it][A
  3%|▎         | 371/10907 [08:26<3:56:47,  1.35s/it][A
  3%|▎         | 371/10907 [08:26<3:56:40,  1.35s/it][A
  3%|▎         | 371/10907 [08:26<3:56:53,  1.35s/it][A
  3%|▎         | 372/10907 [08:28<4:06:28,  1.40s/it][A
  3%|▎         | 372/10907 [08:28<4:06:28,  1.40s/it][A
  3%|▎         | 372/10907 [08:28<4:06:47,  1.41s/it][A
  3%|▎         | 372/10907 [08:28<4:06:55,  1.41s/it][A
  3%|▎         | 372/10907 [08:28<4:07:03,  1.41s/it][A
  3%|▎         | 372/10907 [08:28<4:07:04,  1.41s/it][A
  3%|▎         | 373/10907 [08:29<4:08:14,  1.41s/it][A
  3%|▎         | 373/10907 [08:29<4:08:29,  1.42s/it][A
  3%|▎         | 373/10907 [08:29<4:08:45,  1.42s/it][A

  3%|▎         | 373/10907 [08:29<4:08:46,  1.42s/it][A
  3%|▎         | 373/10907 [08:29<4:08:47,  1.42s/it][A  3%|▎         | 373/10907 [08:29<4:08:44,  1.42s/it][A
  3%|▎         | 374/10907 [08:31<4:04:36,  1.39s/it][A
  3%|▎         | 374/10907 [08:31<4:04:41,  1.39s/it][A

  3%|▎         | 374/10907 [08:31<4:04:35,  1.39s/it][A  3%|▎         | 374/10907 [08:31<4:04:39,  1.39s/it][A
  3%|▎         | 374/10907 [08:31<4:04:45,  1.39s/it][A
  3%|▎         | 374/10907 [08:31<4:05:04,  1.40s/it][A
  3%|▎         | 375/10907 [08:32<4:01:59,  1.38s/it][A
  3%|▎         | 375/10907 [08:32<4:02:00,  1.38s/it][A
  3%|▎         | 375/10907 [08:32<4:02:41,  1.38s/it][A

  3%|▎         | 375/10907 [08:32<4:02:45,  1.38s/it][A  3%|▎         | 375/10907 [08:32<4:02:43,  1.38s/it][A
  3%|▎         | 375/10907 [08:32<4:02:44,  1.38s/it][A
  3%|▎         | 376/10907 [08:33<4:00:34,  1.37s/it][A
  3%|▎         | 376/10907 [08:33<4:00:40,  1.37s/it][A

  3%|▎         | 376/10907 [08:33<4:01:15,  1.37s/it][A  3%|▎         | 376/10907 [08:33<4:01:13,  1.37s/it][A
  3%|▎         | 376/10907 [08:33<4:00:54,  1.37s/it][A
  3%|▎         | 376/10907 [08:33<4:01:03,  1.37s/it][A
  3%|▎         | 377/10907 [08:35<3:59:00,  1.36s/it][A
  3%|▎         | 377/10907 [08:35<3:58:51,  1.36s/it][A
  3%|▎         | 377/10907 [08:35<3:58:59,  1.36s/it][A
  3%|▎         | 377/10907 [08:35<3:59:42,  1.37s/it][A

  3%|▎         | 377/10907 [08:35<3:59:31,  1.36s/it][A  3%|▎         | 377/10907 [08:35<3:59:34,  1.37s/it][A
  3%|▎         | 378/10907 [08:36<3:58:13,  1.36s/it][A
  3%|▎         | 378/10907 [08:36<3:58:05,  1.36s/it][A
  3%|▎         | 378/10907 [08:36<3:58:00,  1.36s/it][A
  3%|▎         | 378/10907 [08:36<3:58:19,  1.36s/it][A

  3%|▎         | 378/10907 [08:36<3:58:23,  1.36s/it][A  3%|▎         | 378/10907 [08:36<3:58:29,  1.36s/it][A
  3%|▎         | 379/10907 [08:37<3:57:02,  1.35s/it][A
  3%|▎         | 379/10907 [08:37<3:57:35,  1.35s/it][A


  3%|▎         | 379/10907 [08:37<3:57:45,  1.35s/it][A  3%|▎         | 379/10907 [08:37<3:57:30,  1.35s/it][A  3%|▎         | 379/10907 [08:37<3:57:26,  1.35s/it][A
  3%|▎         | 379/10907 [08:37<3:57:33,  1.35s/it][A
  3%|▎         | 380/10907 [08:39<3:56:41,  1.35s/it][A
  3%|▎         | 380/10907 [08:39<3:57:01,  1.35s/it][A

  3%|▎         | 380/10907 [08:39<3:57:05,  1.35s/it][A  3%|▎         | 380/10907 [08:39<3:57:05,  1.35s/it][A
  3%|▎         | 380/10907 [08:39<3:57:10,  1.35s/it][A
  3%|▎         | 380/10907 [08:39<3:57:25,  1.35s/it][A
  3%|▎         | 381/10907 [08:40<3:56:59,  1.35s/it][A
  3%|▎         | 381/10907 [08:40<3:57:11,  1.35s/it][A

  3%|▎         | 381/10907 [08:40<3:57:13,  1.35s/it][A  3%|▎         | 381/10907 [08:40<3:57:19,  1.35s/it][A
  3%|▎         | 381/10907 [08:40<3:57:27,  1.35s/it][A
  3%|▎         | 381/10907 [08:40<3:57:18,  1.35s/it][A
  4%|▎         | 382/10907 [08:41<3:56:17,  1.35s/it][A
  4%|▎         | 382/10907 [08:42<3:56:21,  1.35s/it][A
  4%|▎         | 382/10907 [08:42<3:56:28,  1.35s/it][A


  4%|▎         | 382/10907 [08:42<3:56:20,  1.35s/it][A  4%|▎         | 382/10907 [08:42<3:56:26,  1.35s/it][A  4%|▎         | 382/10907 [08:42<3:56:31,  1.35s/it][A

  4%|▎         | 383/10907 [08:43<3:56:10,  1.35s/it][A  4%|▎         | 383/10907 [08:43<3:56:14,  1.35s/it][A
  4%|▎         | 383/10907 [08:43<3:56:14,  1.35s/it][A

  4%|▎         | 383/10907 [08:43<3:56:44,  1.35s/it][A  4%|▎         | 383/10907 [08:43<3:56:39,  1.35s/it][A
  4%|▎         | 383/10907 [08:43<3:56:44,  1.35s/it][A
  4%|▎         | 384/10907 [08:44<3:56:11,  1.35s/it][A
  4%|▎         | 384/10907 [08:44<3:56:20,  1.35s/it][A
  4%|▎         | 384/10907 [08:44<3:56:33,  1.35s/it][A
  4%|▎         | 384/10907 [08:44<3:56:54,  1.35s/it][A
  4%|▎         | 384/10907 [08:44<3:56:59,  1.35s/it][A
  4%|▎         | 384/10907 [08:44<3:56:58,  1.35s/it][A
  4%|▎         | 385/10907 [08:46<3:55:58,  1.35s/it][A
  4%|▎         | 385/10907 [08:46<3:56:01,  1.35s/it][A
  4%|▎         | 385/10907 [08:45<3:56:20,  1.35s/it][A
  4%|▎         | 385/10907 [08:46<3:56:06,  1.35s/it][A
  4%|▎         | 385/10907 [08:46<3:56:15,  1.35s/it][A
  4%|▎         | 385/10907 [08:46<3:56:25,  1.35s/it][A
  4%|▎         | 386/10907 [08:47<3:56:08,  1.35s/it][A
  4%|▎         | 386/10907 [08:47<3:56:17,  1.35s/it]
[A  4%|▎         | 386/10907 [08:47<3:56:20,  1.35s/it][A
  4%|▎         | 386/10907 [08:47<3:56:25,  1.35s/it][A
  4%|▎         | 386/10907 [08:47<3:56:42,  1.35s/it][A
  4%|▎         | 386/10907 [08:47<3:56:31,  1.35s/it][A
  4%|▎         | 387/10907 [08:48<3:54:42,  1.34s/it][A
  4%|▎         | 387/10907 [08:48<3:55:26,  1.34s/it][A
  4%|▎         | 387/10907 [08:48<3:56:04,  1.35s/it][A
  4%|▎         | 387/10907 [08:48<3:56:15,  1.35s/it][A
  4%|▎         | 387/10907 [08:48<3:56:09,  1.35s/it][A
  4%|▎         | 387/10907 [08:48<3:56:27,  1.35s/it][A
  4%|▎         | 388/10907 [08:50<4:07:04,  1.41s/it][A
  4%|▎         | 388/10907 [08:50<4:06:42,  1.41s/it][A
  4%|▎         | 388/10907 [08:50<4:06:55,  1.41s/it][A

  4%|▎         | 388/10907 [08:50<4:06:52,  1.41s/it][A  4%|▎         | 388/10907 [08:50<4:06:56,  1.41s/it][A
  4%|▎         | 388/10907 [08:50<4:07:05,  1.41s/it][A
  4%|▎         | 389/10907 [08:51<4:09:49,  1.43s/it][A
  4%|▎         | 389/10907 [08:51<4:10:17,  1.43s/it][A
  4%|▎         | 389/10907 [08:51<4:10:19,  1.43s/it][A
  4%|▎         | 389/10907 [08:51<4:10:21,  1.43s/it][A
  4%|▎         | 389/10907 [08:51<4:10:20,  1.43s/it][A
  4%|▎         | 389/10907 [08:51<4:10:43,  1.43s/it][A
  4%|▎         | 390/10907 [08:53<4:05:34,  1.40s/it][A
  4%|▎         | 390/10907 [08:53<4:05:48,  1.40s/it][A
  4%|▎         | 390/10907 [08:53<4:05:52,  1.40s/it][A
  4%|▎         | 390/10907 [08:53<4:05:53,  1.40s/it][A
  4%|▎         | 390/10907 [08:53<4:06:02,  1.40s/it][A
  4%|▎         | 390/10907 [08:53<4:05:54,  1.40s/it][A
  4%|▎         | 391/10907 [08:54<4:02:39,  1.38s/it][A
  4%|▎         | 391/10907 [08:54<4:02:39,  1.38s/it][A
  4%|▎         | 391/10907 [08:54<4:02:53,  1.39s/it][A
  4%|▎         | 391/10907 [08:54<4:02:53,  1.39s/it][A
  4%|▎         | 391/10907 [08:54<4:03:01,  1.39s/it][A
  4%|▎         | 391/10907 [08:54<4:03:09,  1.39s/it][A
  4%|▎         | 392/10907 [08:55<4:00:28,  1.37s/it][A
  4%|▎         | 392/10907 [08:55<4:00:47,  1.37s/it][A

  4%|▎         | 392/10907 [08:55<4:00:50,  1.37s/it][A

  4%|▎         | 392/10907 [08:55<4:01:00,  1.38s/it][A  4%|▎         | 392/10907 [08:55<4:00:50,  1.37s/it]  4%|▎         | 392/10907 [08:55<4:00:48,  1.37s/it][A[A

  4%|▎         | 393/10907 [08:57<3:59:09,  1.36s/it][A  4%|▎         | 393/10907 [08:57<3:59:16,  1.37s/it][A
  4%|▎         | 393/10907 [08:57<3:59:13,  1.37s/it][A

  4%|▎         | 393/10907 [08:57<3:59:26,  1.37s/it][A  4%|▎         | 393/10907 [08:57<3:59:17,  1.37s/it][A
  4%|▎         | 393/10907 [08:57<3:59:33,  1.37s/it][A
  4%|▎         | 394/10907 [08:58<3:58:17,  1.36s/it][A
  4%|▎         | 394/10907 [08:58<3:58:25,  1.36s/it][A
  4%|▎         | 394/10907 [08:58<3:58:20,  1.36s/it][A
  4%|▎         | 394/10907 [08:58<3:58:33,  1.36s/it][A
  4%|▎         | 394/10907 [08:58<3:58:41,  1.36s/it][A
  4%|▎         | 394/10907 [08:58<3:58:37,  1.36s/it][A
  4%|▎         | 395/10907 [08:59<3:57:15,  1.35s/it][A
  4%|▎         | 395/10907 [08:59<3:57:33,  1.36s/it][A
  4%|▎         | 395/10907 [08:59<3:57:59,  1.36s/it][A
  4%|▎         | 395/10907 [08:59<3:58:00,  1.36s/it][A
  4%|▎         | 395/10907 [08:59<3:58:00,  1.36s/it][A
  4%|▎         | 395/10907 [08:59<3:58:05,  1.36s/it][A
  4%|▎         | 396/10907 [09:01<3:56:57,  1.35s/it][A
  4%|▎         | 396/10907 [09:01<3:56:58,  1.35s/it][A
  4%|▎         | 396/10907 [09:01<3:57:05,  1.35s/it][A
  4%|▎         | 396/10907 [09:01<3:57:18,  1.35s/it][A
  4%|▎         | 396/10907 [09:01<3:57:25,  1.36s/it][A
  4%|▎         | 396/10907 [09:01<3:57:39,  1.36s/it][A

  4%|▎         | 397/10907 [09:02<3:56:59,  1.35s/it][A  4%|▎         | 397/10907 [09:02<3:57:04,  1.35s/it][A
  4%|▎         | 397/10907 [09:02<3:57:18,  1.35s/it][A
  4%|▎         | 397/10907 [09:02<3:57:14,  1.35s/it][A
  4%|▎         | 397/10907 [09:02<3:57:37,  1.36s/it][A
  4%|▎         | 397/10907 [09:02<3:57:42,  1.36s/it][A
  4%|▎         | 398/10907 [09:03<3:56:16,  1.35s/it][A
  4%|▎         | 398/10907 [09:03<3:56:49,  1.35s/it][A
  4%|▎         | 398/10907 [09:03<3:56:53,  1.35s/it][A

  4%|▎         | 398/10907 [09:03<3:57:00,  1.35s/it][A  4%|▎         | 398/10907 [09:03<3:56:55,  1.35s/it][A
  4%|▎         | 398/10907 [09:03<3:57:01,  1.35s/it][A
  4%|▎         | 399/10907 [09:05<3:56:05,  1.35s/it][A
  4%|▎         | 399/10907 [09:05<3:56:19,  1.35s/it][A
  4%|▎         | 399/10907 [09:05<3:56:35,  1.35s/it][A
  4%|▎         | 399/10907 [09:05<3:56:41,  1.35s/it][A

  4%|▎         | 399/10907 [09:05<3:56:49,  1.35s/it][A  4%|▎         | 399/10907 [09:05<3:56:50,  1.35s/it][A
  4%|▎         | 400/10907 [09:06<3:56:24,  1.35s/it][A
  4%|▎         | 400/10907 [09:06<3:56:29,  1.35s/it][A
  4%|▎         | 400/10907 [09:06<3:56:48,  1.35s/it][A
  4%|▎         | 400/10907 [09:06<3:56:53,  1.35s/it][A

  4%|▎         | 400/10907 [09:06<3:56:48,  1.35s/it][A  4%|▎         | 400/10907 [09:06<3:56:43,  1.35s/it][A
  4%|▎         | 401/10907 [09:07<3:55:39,  1.35s/it][A
  4%|▎         | 401/10907 [09:07<3:55:57,  1.35s/it][A
  4%|▎         | 401/10907 [09:07<3:55:54,  1.35s/it][A
  4%|▎         | 401/10907 [09:07<3:56:09,  1.35s/it][A
  4%|▎         | 401/10907 [09:07<3:56:21,  1.35s/it][A
  4%|▎         | 401/10907 [09:07<3:56:29,  1.35s/it][A
  4%|▎         | 402/10907 [09:09<3:55:10,  1.34s/it][A
  4%|▎         | 402/10907 [09:09<3:55:20,  1.34s/it][A
  4%|▎         | 402/10907 [09:09<3:56:10,  1.35s/it][A
  4%|▎         | 402/10907 [09:09<3:56:15,  1.35s/it][A
  4%|▎         | 402/10907 [09:09<3:56:37,  1.35s/it][A
  4%|▎         | 402/10907 [09:09<3:56:43,  1.35s/it][A
  4%|▎         | 403/10907 [09:10<3:55:33,  1.35s/it][A
  4%|▎         | 403/10907 [09:10<3:55:56,  1.35s/it][A
  4%|▎         | 403/10907 [09:10<3:56:07,  1.35s/it][A
  4%|▎         | 403/10907 [09:10<3:55:57,  1.35s/it][A

  4%|▎         | 403/10907 [09:10<3:55:58,  1.35s/it][A  4%|▎         | 403/10907 [09:10<3:56:09,  1.35s/it][A
  4%|▎         | 404/10907 [09:12<4:02:42,  1.39s/it][A
  4%|▎         | 404/10907 [09:12<4:02:39,  1.39s/it][A
  4%|▎         | 404/10907 [09:12<4:02:53,  1.39s/it][A
  4%|▎         | 404/10907 [09:12<4:03:11,  1.39s/it][A
  4%|▎         | 404/10907 [09:12<4:03:22,  1.39s/it][A
  4%|▎         | 404/10907 [09:12<4:03:45,  1.39s/it][A
  4%|▎         | 405/10907 [09:13<4:03:41,  1.39s/it][A
  4%|▎         | 405/10907 [09:13<4:04:54,  1.40s/it][A
  4%|▎         | 405/10907 [09:13<4:04:56,  1.40s/it][A
  4%|▎         | 405/10907 [09:13<4:05:18,  1.40s/it][A

  4%|▎         | 405/10907 [09:13<4:05:32,  1.40s/it][A  4%|▎         | 405/10907 [09:13<4:05:25,  1.40s/it][A
  4%|▎         | 406/10907 [09:14<4:02:22,  1.38s/it][A
  4%|▎         | 406/10907 [09:14<4:02:24,  1.39s/it][A

  4%|▎         | 406/10907 [09:14<4:02:43,  1.39s/it][A  4%|▎         | 406/10907 [09:14<4:02:36,  1.39s/it][A

  4%|▎         | 406/10907 [09:14<4:02:36,  1.39s/it][A  4%|▎         | 406/10907 [09:14<4:02:33,  1.39s/it][A
  4%|▎         | 407/10907 [09:16<4:00:14,  1.37s/it][A
  4%|▎         | 407/10907 [09:16<4:00:31,  1.37s/it][A
  4%|▎         | 407/10907 [09:16<4:00:26,  1.37s/it][A

  4%|▎         | 407/10907 [09:16<4:00:25,  1.37s/it][A  4%|▎         | 407/10907 [09:16<4:00:35,  1.37s/it][A
  4%|▎         | 407/10907 [09:16<4:00:39,  1.38s/it][A
  4%|▎         | 408/10907 [09:17<3:58:26,  1.36s/it][A
  4%|▎         | 408/10907 [09:17<3:58:21,  1.36s/it][A
  4%|▎         | 408/10907 [09:17<3:58:41,  1.36s/it][A

  4%|▎         | 408/10907 [09:17<3:59:07,  1.37s/it][A  4%|▎         | 408/10907 [09:17<3:59:00,  1.37s/it][A
  4%|▎         | 408/10907 [09:17<3:59:14,  1.37s/it][A
  4%|▎         | 409/10907 [09:18<3:57:16,  1.36s/it][A
  4%|▎         | 409/10907 [09:18<3:57:37,  1.36s/it][A
  4%|▎         | 409/10907 [09:18<3:57:45,  1.36s/it][A
  4%|▎         | 409/10907 [09:18<3:57:55,  1.36s/it][A

  4%|▎         | 409/10907 [09:18<3:57:57,  1.36s/it][A  4%|▎         | 409/10907 [09:18<3:58:06,  1.36s/it][A
  4%|▍         | 410/10907 [09:20<3:56:30,  1.35s/it][A
  4%|▍         | 410/10907 [09:20<3:56:34,  1.35s/it][A
  4%|▍         | 410/10907 [09:20<3:56:51,  1.35s/it][A
  4%|▍         | 410/10907 [09:20<3:56:45,  1.35s/it][A
  4%|▍         | 410/10907 [09:20<3:56:49,  1.35s/it][A
  4%|▍         | 410/10907 [09:20<3:57:00,  1.35s/it][A
  4%|▍         | 411/10907 [09:21<3:55:30,  1.35s/it][A
  4%|▍         | 411/10907 [09:21<3:55:58,  1.35s/it][A
  4%|▍         | 411/10907 [09:21<3:56:28,  1.35s/it][A

  4%|▍         | 411/10907 [09:21<3:56:22,  1.35s/it][A  4%|▍         | 411/10907 [09:21<3:56:20,  1.35s/it][A
  4%|▍         | 411/10907 [09:21<3:56:32,  1.35s/it][A
  4%|▍         | 412/10907 [09:22<3:55:58,  1.35s/it][A
  4%|▍         | 412/10907 [09:22<3:56:05,  1.35s/it][A

  4%|▍         | 412/10907 [09:22<3:56:05,  1.35s/it][A  4%|▍         | 412/10907 [09:22<3:56:17,  1.35s/it][A
  4%|▍         | 412/10907 [09:22<3:56:06,  1.35s/it][A
  4%|▍         | 412/10907 [09:22<3:56:25,  1.35s/it][A
  4%|▍         | 413/10907 [09:24<3:55:38,  1.35s/it][A
  4%|▍         | 413/10907 [09:24<3:55:41,  1.35s/it][A
  4%|▍         | 413/10907 [09:24<3:55:52,  1.35s/it][A
  4%|▍         | 413/10907 [09:24<3:55:55,  1.35s/it][A
  4%|▍         | 413/10907 [09:24<3:55:53,  1.35s/it][A
  4%|▍         | 413/10907 [09:24<3:56:06,  1.35s/it][A
  4%|▍         | 414/10907 [09:25<3:55:42,  1.35s/it][A
  4%|▍         | 414/10907 [09:25<3:55:47,  1.35s/it][A
  4%|▍         | 414/10907 [09:25<3:55:59,  1.35s/it][A
  4%|▍         | 414/10907 [09:25<3:56:04,  1.35s/it][A
  4%|▍         | 414/10907 [09:25<3:55:59,  1.35s/it][A
  4%|▍         | 414/10907 [09:25<3:56:16,  1.35s/it][A
  4%|▍         | 415/10907 [09:27<3:55:31,  1.35s/it][A
  4%|▍         | 415/10907 [09:27<3:55:37,  1.35s/it][A

  4%|▍         | 415/10907 [09:27<3:55:43,  1.35s/it][A  4%|▍         | 415/10907 [09:27<3:55:44,  1.35s/it][A

  4%|▍         | 415/10907 [09:27<3:55:40,  1.35s/it][A  4%|▍         | 415/10907 [09:26<3:55:49,  1.35s/it][A
  4%|▍         | 416/10907 [09:28<3:55:45,  1.35s/it][A
  4%|▍         | 416/10907 [09:28<3:55:44,  1.35s/it][A
  4%|▍         | 416/10907 [09:28<3:55:42,  1.35s/it][A
  4%|▍         | 416/10907 [09:28<3:56:00,  1.35s/it][A
  4%|▍         | 416/10907 [09:28<3:56:02,  1.35s/it][A
  4%|▍         | 416/10907 [09:28<3:56:02,  1.35s/it][A
  4%|▍         | 417/10907 [09:29<3:55:20,  1.35s/it][A
  4%|▍         | 417/10907 [09:29<3:55:20,  1.35s/it][A
  4%|▍         | 417/10907 [09:29<3:55:22,  1.35s/it][A
  4%|▍         | 417/10907 [09:29<3:55:49,  1.35s/it][A
  4%|▍         | 417/10907 [09:29<3:55:50,  1.35s/it][A
  4%|▍         | 417/10907 [09:29<3:56:05,  1.35s/it][A
  4%|▍         | 418/10907 [09:31<3:55:20,  1.35s/it][A

  4%|▍         | 418/10907 [09:31<3:55:36,  1.35s/it][A
  4%|▍         | 418/10907 [09:30<3:55:26,  1.35s/it][A  4%|▍         | 418/10907 [09:31<3:55:28,  1.35s/it][A
  4%|▍         | 418/10907 [09:31<3:55:44,  1.35s/it][A
  4%|▍         | 418/10907 [09:31<3:55:40,  1.35s/it][A
  4%|▍         | 419/10907 [09:32<4:08:12,  1.42s/it][A
  4%|▍         | 419/10907 [09:32<4:08:23,  1.42s/it][A
  4%|▍         | 419/10907 [09:32<4:08:21,  1.42s/it][A
  4%|▍         | 419/10907 [09:32<4:08:24,  1.42s/it][A
  4%|▍         | 419/10907 [09:32<4:08:30,  1.42s/it][A
  4%|▍         | 419/10907 [09:32<4:08:35,  1.42s/it][A
  4%|▍         | 420/10907 [09:34<4:10:38,  1.43s/it][A
  4%|▍         | 420/10907 [09:34<4:11:20,  1.44s/it][A
  4%|▍         | 420/10907 [09:34<4:11:25,  1.44s/it][A
  4%|▍         | 420/10907 [09:34<4:11:30,  1.44s/it][A

  4%|▍         | 420/10907 [09:34<4:11:25,  1.44s/it][A  4%|▍         | 420/10907 [09:34<4:11:30,  1.44s/it][A
  4%|▍         | 421/10907 [09:35<4:06:19,  1.41s/it][A
  4%|▍         | 421/10907 [09:35<4:06:35,  1.41s/it][A
  4%|▍         | 421/10907 [09:35<4:06:44,  1.41s/it][A
  4%|▍         | 421/10907 [09:35<4:06:51,  1.41s/it][A
  4%|▍         | 421/10907 [09:35<4:06:53,  1.41s/it][A
  4%|▍         | 421/10907 [09:35<4:07:06,  1.41s/it][A
  4%|▍         | 422/10907 [09:36<4:03:05,  1.39s/it][A

  4%|▍         | 422/10907 [09:36<4:02:58,  1.39s/it][A  4%|▍         | 422/10907 [09:36<4:03:02,  1.39s/it][A
  4%|▍         | 422/10907 [09:36<4:03:10,  1.39s/it][A
  4%|▍         | 422/10907 [09:36<4:03:20,  1.39s/it][A
  4%|▍         | 422/10907 [09:36<4:03:23,  1.39s/it][A
  4%|▍         | 423/10907 [09:38<4:00:27,  1.38s/it]
[A  4%|▍         | 423/10907 [09:38<4:00:28,  1.38s/it][A
  4%|▍         | 423/10907 [09:38<4:00:39,  1.38s/it][A
  4%|▍         | 423/10907 [09:38<4:00:46,  1.38s/it][A
  4%|▍         | 423/10907 [09:38<4:00:52,  1.38s/it][A
  4%|▍         | 423/10907 [09:38<4:00:51,  1.38s/it][A
  4%|▍         | 424/10907 [09:39<3:58:39,  1.37s/it][A
  4%|▍         | 424/10907 [09:39<3:58:32,  1.37s/it][A
  4%|▍         | 424/10907 [09:39<3:58:56,  1.37s/it][A
  4%|▍         | 424/10907 [09:39<3:59:11,  1.37s/it][A

  4%|▍         | 424/10907 [09:39<3:59:36,  1.37s/it][A  4%|▍         | 424/10907 [09:39<3:59:31,  1.37s/it][A
  4%|▍         | 425/10907 [09:40<3:57:04,  1.36s/it][A
  4%|▍         | 425/10907 [09:40<3:57:09,  1.36s/it][A
  4%|▍         | 425/10907 [09:40<3:57:49,  1.36s/it][A
  4%|▍         | 425/10907 [09:40<3:58:23,  1.36s/it][A

  4%|▍         | 425/10907 [09:40<3:58:10,  1.36s/it][A  4%|▍         | 425/10907 [09:40<3:58:14,  1.36s/it][A
  4%|▍         | 426/10907 [09:42<3:56:25,  1.35s/it][A
  4%|▍         | 426/10907 [09:42<3:56:25,  1.35s/it][A
  4%|▍         | 426/10907 [09:42<3:56:26,  1.35s/it][A
  4%|▍         | 426/10907 [09:42<3:56:39,  1.35s/it][A
  4%|▍         | 426/10907 [09:42<3:56:51,  1.36s/it][A
  4%|▍         | 426/10907 [09:42<3:57:10,  1.36s/it][A
  4%|▍         | 427/10907 [09:43<3:56:16,  1.35s/it][A
  4%|▍         | 427/10907 [09:43<3:56:29,  1.35s/it][A
  4%|▍         | 427/10907 [09:43<3:56:22,  1.35s/it][A
  4%|▍         | 427/10907 [09:43<3:56:29,  1.35s/it][A

  4%|▍         | 427/10907 [09:43<3:56:39,  1.35s/it][A  4%|▍         | 427/10907 [09:43<3:56:33,  1.35s/it][A
  4%|▍         | 428/10907 [09:44<3:56:30,  1.35s/it][A
  4%|▍         | 428/10907 [09:44<3:56:21,  1.35s/it][A

  4%|▍         | 428/10907 [09:44<3:56:49,  1.36s/it][A  4%|▍         | 428/10907 [09:44<3:56:33,  1.35s/it][A

  4%|▍         | 428/10907 [09:44<3:56:28,  1.35s/it][A  4%|▍         | 428/10907 [09:44<3:56:41,  1.36s/it][A
  4%|▍         | 429/10907 [09:46<3:56:12,  1.35s/it][A
  4%|▍         | 429/10907 [09:46<3:56:54,  1.36s/it][A
  4%|▍         | 429/10907 [09:46<3:56:32,  1.35s/it][A
  4%|▍         | 429/10907 [09:46<3:56:42,  1.36s/it][A
  4%|▍         | 429/10907 [09:46<3:56:40,  1.36s/it][A
  4%|▍         | 429/10907 [09:46<3:56:54,  1.36s/it][A
  4%|▍         | 430/10907 [09:47<3:55:52,  1.35s/it][A
  4%|▍         | 430/10907 [09:47<3:56:21,  1.35s/it][A
  4%|▍         | 430/10907 [09:47<3:56:18,  1.35s/it][A
  4%|▍         | 430/10907 [09:47<3:56:16,  1.35s/it][A
  4%|▍         | 430/10907 [09:47<3:56:34,  1.35s/it][A
  4%|▍         | 430/10907 [09:47<3:56:26,  1.35s/it][A
  4%|▍         | 431/10907 [09:48<3:55:35,  1.35s/it][A


  4%|▍         | 431/10907 [09:48<3:55:40,  1.35s/it][A  4%|▍         | 431/10907 [09:48<3:55:53,  1.35s/it][A  4%|▍         | 431/10907 [09:48<3:55:48,  1.35s/it][A

  4%|▍         | 431/10907 [09:48<3:55:51,  1.35s/it][A  4%|▍         | 431/10907 [09:48<3:55:48,  1.35s/it][A
  4%|▍         | 432/10907 [09:50<3:54:51,  1.35s/it][A
  4%|▍         | 432/10907 [09:50<3:55:15,  1.35s/it][A
  4%|▍         | 432/10907 [09:50<3:55:29,  1.35s/it][A
  4%|▍         | 432/10907 [09:50<3:55:29,  1.35s/it][A
  4%|▍         | 432/10907 [09:50<3:55:36,  1.35s/it][A
  4%|▍         | 432/10907 [09:50<3:55:38,  1.35s/it][A
  4%|▍         | 433/10907 [09:51<3:56:07,  1.35s/it][A

  4%|▍         | 433/10907 [09:51<3:55:52,  1.35s/it][A  4%|▍         | 433/10907 [09:51<3:55:59,  1.35s/it][A

  4%|▍         | 433/10907 [09:51<3:56:02,  1.35s/it][A  4%|▍         | 433/10907 [09:51<3:56:10,  1.35s/it][A
  4%|▍         | 433/10907 [09:51<3:56:02,  1.35s/it][A
  4%|▍         | 434/10907 [09:53<3:55:11,  1.35s/it][A
  4%|▍         | 434/10907 [09:53<3:55:37,  1.35s/it][A
  4%|▍         | 434/10907 [09:53<3:55:29,  1.35s/it][A
  4%|▍         | 434/10907 [09:53<3:55:33,  1.35s/it][A
  4%|▍         | 434/10907 [09:52<3:55:40,  1.35s/it][A
  4%|▍         | 434/10907 [09:53<3:55:43,  1.35s/it][A
  4%|▍         | 435/10907 [09:54<4:01:33,  1.38s/it][A
  4%|▍         | 435/10907 [09:54<4:01:35,  1.38s/it][A
  4%|▍         | 435/10907 [09:54<4:02:32,  1.39s/it][A
  4%|▍         | 435/10907 [09:54<4:02:20,  1.39s/it][A
  4%|▍         | 435/10907 [09:54<4:02:20,  1.39s/it][A
  4%|▍         | 435/10907 [09:54<4:02:59,  1.39s/it][A
  4%|▍         | 436/10907 [09:55<4:03:03,  1.39s/it][A
  4%|▍         | 436/10907 [09:55<4:04:14,  1.40s/it][A
  4%|▍         | 436/10907 [09:55<4:04:20,  1.40s/it][A
  4%|▍         | 436/10907 [09:55<4:04:31,  1.40s/it][A

  4%|▍         | 436/10907 [09:55<4:04:36,  1.40s/it][A  4%|▍         | 436/10907 [09:55<4:04:41,  1.40s/it][A
  4%|▍         | 437/10907 [09:57<4:01:41,  1.39s/it][A

  4%|▍         | 437/10907 [09:57<4:01:29,  1.38s/it][A  4%|▍         | 437/10907 [09:57<4:01:25,  1.38s/it][A

  4%|▍         | 437/10907 [09:57<4:01:59,  1.39s/it][A  4%|▍         | 437/10907 [09:57<4:01:27,  1.38s/it][A
  4%|▍         | 437/10907 [09:57<4:01:49,  1.39s/it][A
  4%|▍         | 438/10907 [09:58<3:59:11,  1.37s/it][A
  4%|▍         | 438/10907 [09:58<3:58:50,  1.37s/it][A
  4%|▍         | 438/10907 [09:58<3:59:31,  1.37s/it][A

  4%|▍         | 438/10907 [09:58<3:59:40,  1.37s/it][A  4%|▍         | 438/10907 [09:58<3:59:47,  1.37s/it][A
  4%|▍         | 438/10907 [09:58<3:59:40,  1.37s/it][A
  4%|▍         | 439/10907 [09:59<3:58:16,  1.37s/it][A
  4%|▍         | 439/10907 [09:59<3:58:11,  1.37s/it][A
  4%|▍         | 439/10907 [09:59<3:58:24,  1.37s/it][A
  4%|▍         | 439/10907 [09:59<3:58:22,  1.37s/it][A

  4%|▍         | 439/10907 [09:59<3:58:41,  1.37s/it]  4%|▍         | 439/10907 [09:59<3:58:57,  1.37s/it][A[A
  4%|▍         | 440/10907 [10:01<3:57:06,  1.36s/it][A
  4%|▍         | 440/10907 [10:01<3:57:05,  1.36s/it][A
  4%|▍         | 440/10907 [10:01<3:57:04,  1.36s/it][A
  4%|▍         | 440/10907 [10:01<3:57:10,  1.36s/it][A
  4%|▍         | 440/10907 [10:01<3:57:30,  1.36s/it]
[A  4%|▍         | 440/10907 [10:01<3:57:24,  1.36s/it][A
  4%|▍         | 441/10907 [10:02<3:55:41,  1.35s/it][A
  4%|▍         | 441/10907 [10:02<3:56:03,  1.35s/it][A
  4%|▍         | 441/10907 [10:02<3:56:24,  1.36s/it][A
  4%|▍         | 441/10907 [10:02<3:56:24,  1.36s/it][A

  4%|▍         | 441/10907 [10:02<3:56:33,  1.36s/it][A  4%|▍         | 441/10907 [10:02<3:56:32,  1.36s/it][A
  4%|▍         | 442/10907 [10:03<3:55:25,  1.35s/it][A
  4%|▍         | 442/10907 [10:04<3:55:41,  1.35s/it][A
  4%|▍         | 442/10907 [10:04<3:55:40,  1.35s/it][A

  4%|▍         | 442/10907 [10:04<3:55:57,  1.35s/it][A  4%|▍         | 442/10907 [10:04<3:56:12,  1.35s/it][A
  4%|▍         | 442/10907 [10:04<3:55:53,  1.35s/it][A
  4%|▍         | 443/10907 [10:05<3:55:03,  1.35s/it][A
  4%|▍         | 443/10907 [10:05<3:55:35,  1.35s/it][A
  4%|▍         | 443/10907 [10:05<3:55:50,  1.35s/it][A
  4%|▍         | 443/10907 [10:05<3:56:02,  1.35s/it][A

  4%|▍         | 443/10907 [10:05<3:55:55,  1.35s/it][A  4%|▍         | 443/10907 [10:05<3:55:50,  1.35s/it][A

  4%|▍         | 444/10907 [10:06<3:55:50,  1.35s/it][A  4%|▍         | 444/10907 [10:06<3:55:43,  1.35s/it][A
  4%|▍         | 444/10907 [10:06<3:55:39,  1.35s/it][A
  4%|▍         | 444/10907 [10:06<3:55:40,  1.35s/it][A
  4%|▍         | 444/10907 [10:06<3:56:05,  1.35s/it][A
  4%|▍         | 444/10907 [10:06<3:56:09,  1.35s/it][A
  4%|▍         | 445/10907 [10:07<3:55:12,  1.35s/it][A
  4%|▍         | 445/10907 [10:08<3:55:17,  1.35s/it][A
  4%|▍         | 445/10907 [10:08<3:55:13,  1.35s/it][A
  4%|▍         | 445/10907 [10:08<3:55:14,  1.35s/it][A
  4%|▍         | 445/10907 [10:08<3:55:30,  1.35s/it][A
  4%|▍         | 445/10907 [10:08<3:55:22,  1.35s/it][A
  4%|▍         | 446/10907 [10:09<3:55:11,  1.35s/it][A
  4%|▍         | 446/10907 [10:09<3:55:12,  1.35s/it][A
  4%|▍         | 446/10907 [10:09<3:55:32,  1.35s/it][A

  4%|▍         | 446/10907 [10:09<3:55:55,  1.35s/it][A  4%|▍         | 446/10907 [10:09<3:55:54,  1.35s/it][A
  4%|▍         | 446/10907 [10:09<3:55:47,  1.35s/it][A
  4%|▍         | 447/10907 [10:10<3:55:23,  1.35s/it][A
  4%|▍         | 447/10907 [10:10<3:55:34,  1.35s/it][A

  4%|▍         | 447/10907 [10:10<3:55:25,  1.35s/it][A  4%|▍         | 447/10907 [10:10<3:55:18,  1.35s/it][A
  4%|▍         | 447/10907 [10:10<3:55:25,  1.35s/it][A
  4%|▍         | 447/10907 [10:10<3:55:33,  1.35s/it][A
  4%|▍         | 448/10907 [10:12<3:53:59,  1.34s/it][A
  4%|▍         | 448/10907 [10:12<3:54:33,  1.35s/it][A
  4%|▍         | 448/10907 [10:12<3:54:41,  1.35s/it][A
  4%|▍         | 448/10907 [10:12<3:54:55,  1.35s/it][A
  4%|▍         | 448/10907 [10:11<3:55:08,  1.35s/it][A
  4%|▍         | 448/10907 [10:12<3:55:12,  1.35s/it][A
  4%|▍         | 449/10907 [10:13<3:53:57,  1.34s/it][A
  4%|▍         | 449/10907 [10:13<3:54:12,  1.34s/it][A
  4%|▍         | 449/10907 [10:13<3:54:33,  1.35s/it]
[A
  4%|▍         | 449/10907 [10:13<3:55:00,  1.35s/it][A  4%|▍         | 449/10907 [10:13<3:54:57,  1.35s/it]
[A  4%|▍         | 449/10907 [10:13<3:54:42,  1.35s/it][A
  4%|▍         | 450/10907 [10:14<4:05:37,  1.41s/it][A
  4%|▍         | 450/10907 [10:14<4:05:40,  1.41s/it][A
  4%|▍         | 450/10907 [10:15<4:05:35,  1.41s/it][A
  4%|▍         | 450/10907 [10:15<4:05:37,  1.41s/it][A

  4%|▍         | 450/10907 [10:14<4:05:49,  1.41s/it][A  4%|▍         | 450/10907 [10:15<4:05:59,  1.41s/it][A
  4%|▍         | 451/10907 [10:16<4:08:47,  1.43s/it][A
  4%|▍         | 451/10907 [10:16<4:08:45,  1.43s/it][A
  4%|▍         | 451/10907 [10:16<4:08:57,  1.43s/it][A
  4%|▍         | 451/10907 [10:16<4:08:45,  1.43s/it][A
  4%|▍         | 451/10907 [10:16<4:09:15,  1.43s/it][A
  4%|▍         | 451/10907 [10:16<4:09:08,  1.43s/it][A
  4%|▍         | 452/10907 [10:17<4:04:07,  1.40s/it][A
  4%|▍         | 452/10907 [10:17<4:04:30,  1.40s/it][A
  4%|▍         | 452/10907 [10:17<4:04:44,  1.40s/it][A
  4%|▍         | 452/10907 [10:17<4:04:56,  1.41s/it][A
  4%|▍         | 452/10907 [10:17<4:04:52,  1.41s/it][A
  4%|▍         | 452/10907 [10:17<4:05:04,  1.41s/it][A
  4%|▍         | 453/10907 [10:19<4:01:31,  1.39s/it][A
  4%|▍         | 453/10907 [10:19<4:01:37,  1.39s/it][A
  4%|▍         | 453/10907 [10:19<4:01:55,  1.39s/it][A
  4%|▍         | 453/10907 [10:19<4:01:44,  1.39s/it][A
  4%|▍         | 453/10907 [10:19<4:01:47,  1.39s/it][A
  4%|▍         | 453/10907 [10:19<4:02:00,  1.39s/it][A
  4%|▍         | 454/10907 [10:20<3:59:17,  1.37s/it][A
  4%|▍         | 454/10907 [10:20<3:59:10,  1.37s/it][A
  4%|▍         | 454/10907 [10:20<3:59:16,  1.37s/it][A
  4%|▍         | 454/10907 [10:20<3:59:55,  1.38s/it][A
  4%|▍         | 454/10907 [10:20<4:00:06,  1.38s/it][A
  4%|▍         | 454/10907 [10:20<4:00:09,  1.38s/it][A
  4%|▍         | 455/10907 [10:21<3:58:07,  1.37s/it][A
  4%|▍         | 455/10907 [10:21<3:58:01,  1.37s/it][A
  4%|▍         | 455/10907 [10:21<3:58:13,  1.37s/it][A
  4%|▍         | 455/10907 [10:21<3:58:10,  1.37s/it][A
  4%|▍         | 455/10907 [10:21<3:58:20,  1.37s/it][A
  4%|▍         | 455/10907 [10:21<3:58:16,  1.37s/it][A
  4%|▍         | 456/10907 [10:23<3:57:20,  1.36s/it][A
  4%|▍         | 456/10907 [10:23<3:57:21,  1.36s/it][A

  4%|▍         | 456/10907 [10:23<3:57:26,  1.36s/it]
[A  4%|▍         | 456/10907 [10:23<3:57:33,  1.36s/it][A  4%|▍         | 456/10907 [10:23<3:57:33,  1.36s/it][A
  4%|▍         | 456/10907 [10:23<3:57:37,  1.36s/it][A
  4%|▍         | 457/10907 [10:24<3:56:37,  1.36s/it][A
  4%|▍         | 457/10907 [10:24<3:56:53,  1.36s/it][A
  4%|▍         | 457/10907 [10:24<3:56:55,  1.36s/it][A
  4%|▍         | 457/10907 [10:24<3:56:59,  1.36s/it][A
  4%|▍         | 457/10907 [10:24<3:56:58,  1.36s/it][A
  4%|▍         | 457/10907 [10:24<3:57:10,  1.36s/it][A
  4%|▍         | 458/10907 [10:25<3:55:40,  1.35s/it][A
  4%|▍         | 458/10907 [10:25<3:55:55,  1.35s/it][A
  4%|▍         | 458/10907 [10:25<3:56:21,  1.36s/it][A
  4%|▍         | 458/10907 [10:25<3:56:27,  1.36s/it][A
  4%|▍         | 458/10907 [10:25<3:56:36,  1.36s/it][A
  4%|▍         | 458/10907 [10:25<3:56:37,  1.36s/it][A
  4%|▍         | 459/10907 [10:27<3:55:23,  1.35s/it][A
  4%|▍         | 459/10907 [10:27<3:55:28,  1.35s/it][A
  4%|▍         | 459/10907 [10:27<3:55:57,  1.36s/it][A
  4%|▍         | 459/10907 [10:27<3:55:46,  1.35s/it][A
  4%|▍         | 459/10907 [10:27<3:55:47,  1.35s/it][A
  4%|▍         | 459/10907 [10:27<3:55:57,  1.36s/it][A
  4%|▍         | 460/10907 [10:28<3:55:25,  1.35s/it][A

  4%|▍         | 460/10907 [10:28<3:55:31,  1.35s/it][A  4%|▍         | 460/10907 [10:28<3:55:40,  1.35s/it][A
  4%|▍         | 460/10907 [10:28<3:55:26,  1.35s/it][A
  4%|▍         | 460/10907 [10:28<3:55:46,  1.35s/it][A
  4%|▍         | 460/10907 [10:28<3:55:45,  1.35s/it][A
  4%|▍         | 461/10907 [10:29<3:54:35,  1.35s/it][A
  4%|▍         | 461/10907 [10:29<3:54:36,  1.35s/it][A
  4%|▍         | 461/10907 [10:29<3:55:00,  1.35s/it][A
  4%|▍         | 461/10907 [10:29<3:55:31,  1.35s/it][A

  4%|▍         | 461/10907 [10:29<3:55:21,  1.35s/it][A  4%|▍         | 461/10907 [10:29<3:55:26,  1.35s/it][A
  4%|▍         | 462/10907 [10:31<3:54:14,  1.35s/it][A
  4%|▍         | 462/10907 [10:31<3:54:47,  1.35s/it][A
  4%|▍         | 462/10907 [10:31<3:54:30,  1.35s/it][A
  4%|▍         | 462/10907 [10:31<3:54:49,  1.35s/it][A
  4%|▍         | 462/10907 [10:31<3:55:15,  1.35s/it][A
  4%|▍         | 462/10907 [10:31<3:55:13,  1.35s/it][A
  4%|▍         | 463/10907 [10:32<3:54:14,  1.35s/it][A
  4%|▍         | 463/10907 [10:32<3:54:45,  1.35s/it][A
  4%|▍         | 463/10907 [10:32<3:54:39,  1.35s/it][A
  4%|▍         | 463/10907 [10:32<3:54:35,  1.35s/it][A

  4%|▍         | 463/10907 [10:32<3:54:53,  1.35s/it][A  4%|▍         | 463/10907 [10:32<3:54:39,  1.35s/it][A
  4%|▍         | 464/10907 [10:33<3:54:23,  1.35s/it][A


  4%|▍         | 464/10907 [10:34<3:54:25,  1.35s/it][A  4%|▍         | 464/10907 [10:34<3:54:28,  1.35s/it][A  4%|▍         | 464/10907 [10:34<3:54:24,  1.35s/it][A

  4%|▍         | 464/10907 [10:34<3:54:37,  1.35s/it][A  4%|▍         | 464/10907 [10:33<3:54:39,  1.35s/it][A
  4%|▍         | 465/10907 [10:35<3:53:30,  1.34s/it][A
  4%|▍         | 465/10907 [10:35<3:53:40,  1.34s/it][A
  4%|▍         | 465/10907 [10:35<3:53:53,  1.34s/it][A
  4%|▍         | 465/10907 [10:35<3:53:56,  1.34s/it][A
  4%|▍         | 465/10907 [10:35<3:54:11,  1.35s/it][A
  4%|▍         | 465/10907 [10:35<3:54:19,  1.35s/it][A
  4%|▍         | 466/10907 [10:36<4:01:21,  1.39s/it][A
  4%|▍         | 466/10907 [10:36<4:01:42,  1.39s/it][A
  4%|▍         | 466/10907 [10:36<4:01:52,  1.39s/it][A
  4%|▍         | 466/10907 [10:36<4:01:53,  1.39s/it][A
  4%|▍         | 466/10907 [10:36<4:01:49,  1.39s/it][A
  4%|▍         | 466/10907 [10:36<4:02:25,  1.39s/it][A
  4%|▍         | 467/10907 [10:38<4:02:51,  1.40s/it][A
  4%|▍         | 467/10907 [10:38<4:03:24,  1.40s/it][A
  4%|▍         | 467/10907 [10:38<4:03:38,  1.40s/it][A
  4%|▍         | 467/10907 [10:38<4:03:39,  1.40s/it][A
  4%|▍         | 467/10907 [10:38<4:03:51,  1.40s/it][A
  4%|▍         | 467/10907 [10:38<4:03:51,  1.40s/it][A
  4%|▍         | 468/10907 [10:39<4:00:07,  1.38s/it][A
  4%|▍         | 468/10907 [10:39<4:00:10,  1.38s/it][A
  4%|▍         | 468/10907 [10:39<4:00:46,  1.38s/it][A


  4%|▍         | 468/10907 [10:39<4:00:53,  1.38s/it][A  4%|▍         | 468/10907 [10:39<4:00:44,  1.38s/it][A  4%|▍         | 468/10907 [10:39<4:00:34,  1.38s/it][A
  4%|▍         | 469/10907 [10:40<3:58:18,  1.37s/it][A

  4%|▍         | 469/10907 [10:40<3:58:47,  1.37s/it][A  4%|▍         | 469/10907 [10:40<3:58:26,  1.37s/it][A

  4%|▍         | 469/10907 [10:40<3:58:55,  1.37s/it][A  4%|▍         | 469/10907 [10:40<3:58:47,  1.37s/it][A
  4%|▍         | 469/10907 [10:40<3:58:41,  1.37s/it][A
  4%|▍         | 470/10907 [10:42<3:56:25,  1.36s/it][A
  4%|▍         | 470/10907 [10:42<3:57:03,  1.36s/it][A
  4%|▍         | 470/10907 [10:42<3:57:18,  1.36s/it][A
  4%|▍         | 470/10907 [10:42<3:57:17,  1.36s/it][A
  4%|▍         | 470/10907 [10:42<3:57:46,  1.37s/it][A
  4%|▍         | 470/10907 [10:42<3:57:19,  1.36s/it][A
  4%|▍         | 471/10907 [10:43<3:56:05,  1.36s/it][A
  4%|▍         | 471/10907 [10:43<3:55:51,  1.36s/it][A
  4%|▍         | 471/10907 [10:43<3:56:18,  1.36s/it][A
  4%|▍         | 471/10907 [10:43<3:56:23,  1.36s/it][A
  4%|▍         | 471/10907 [10:43<3:56:22,  1.36s/it][A
  4%|▍         | 471/10907 [10:43<3:56:46,  1.36s/it][A

  4%|▍         | 472/10907 [10:44<3:55:38,  1.35s/it][A  4%|▍         | 472/10907 [10:44<3:55:44,  1.36s/it][A
  4%|▍         | 472/10907 [10:44<3:55:29,  1.35s/it][A
  4%|▍         | 472/10907 [10:44<3:55:36,  1.35s/it][A
  4%|▍         | 472/10907 [10:44<3:55:46,  1.36s/it][A
  4%|▍         | 472/10907 [10:44<3:55:41,  1.36s/it][A
  4%|▍         | 473/10907 [10:46<3:54:52,  1.35s/it][A
  4%|▍         | 473/10907 [10:46<3:55:11,  1.35s/it][A
  4%|▍         | 473/10907 [10:46<3:55:29,  1.35s/it][A
  4%|▍         | 473/10907 [10:46<3:55:55,  1.36s/it][A
  4%|▍         | 473/10907 [10:46<3:55:46,  1.36s/it][A
  4%|▍         | 473/10907 [10:46<3:55:50,  1.36s/it][A
  4%|▍         | 474/10907 [10:47<3:55:09,  1.35s/it][A

  4%|▍         | 474/10907 [10:47<3:55:07,  1.35s/it][A  4%|▍         | 474/10907 [10:47<3:54:50,  1.35s/it][A

  4%|▍         | 474/10907 [10:47<3:55:19,  1.35s/it]
[A  4%|▍         | 474/10907 [10:47<3:55:25,  1.35s/it][A  4%|▍         | 474/10907 [10:47<3:55:10,  1.35s/it][A


  4%|▍         | 475/10907 [10:49<3:54:38,  1.35s/it][A  4%|▍         | 475/10907 [10:49<3:54:40,  1.35s/it][A  4%|▍         | 475/10907 [10:49<3:54:36,  1.35s/it][A
  4%|▍         | 475/10907 [10:48<3:54:47,  1.35s/it][A
  4%|▍         | 475/10907 [10:49<3:54:39,  1.35s/it][A
  4%|▍         | 475/10907 [10:49<3:54:43,  1.35s/it][A
  4%|▍         | 476/10907 [10:50<3:54:07,  1.35s/it][A
  4%|▍         | 476/10907 [10:50<3:54:22,  1.35s/it][A
  4%|▍         | 476/10907 [10:50<3:54:27,  1.35s/it][A

  4%|▍         | 476/10907 [10:50<3:54:47,  1.35s/it][A  4%|▍         | 476/10907 [10:50<3:54:47,  1.35s/it][A
  4%|▍         | 476/10907 [10:50<3:54:49,  1.35s/it][A
  4%|▍         | 477/10907 [10:51<3:54:22,  1.35s/it][A
  4%|▍         | 477/10907 [10:51<3:54:19,  1.35s/it][A
  4%|▍         | 477/10907 [10:51<3:54:27,  1.35s/it][A
  4%|▍         | 477/10907 [10:51<3:54:28,  1.35s/it][A

  4%|▍         | 477/10907 [10:51<3:54:41,  1.35s/it][A  4%|▍         | 477/10907 [10:51<3:54:35,  1.35s/it][A

  4%|▍         | 478/10907 [10:53<3:54:23,  1.35s/it][A  4%|▍         | 478/10907 [10:53<3:54:22,  1.35s/it][A
  4%|▍         | 478/10907 [10:53<3:54:33,  1.35s/it][A

  4%|▍         | 478/10907 [10:53<3:54:33,  1.35s/it]
[A  4%|▍         | 478/10907 [10:52<3:54:38,  1.35s/it][A  4%|▍         | 478/10907 [10:53<3:54:39,  1.35s/it][A

  4%|▍         | 479/10907 [10:54<3:54:32,  1.35s/it][A  4%|▍         | 479/10907 [10:54<3:54:25,  1.35s/it][A
  4%|▍         | 479/10907 [10:54<3:54:30,  1.35s/it][A
  4%|▍         | 479/10907 [10:54<3:54:34,  1.35s/it][A
  4%|▍         | 479/10907 [10:54<3:54:49,  1.35s/it][A
  4%|▍         | 479/10907 [10:54<3:54:45,  1.35s/it][A
  4%|▍         | 480/10907 [10:55<3:53:53,  1.35s/it][A
  4%|▍         | 480/10907 [10:55<3:54:00,  1.35s/it][A
  4%|▍         | 480/10907 [10:55<3:54:03,  1.35s/it][A
  4%|▍         | 480/10907 [10:55<3:54:13,  1.35s/it][A
  4%|▍         | 480/10907 [10:55<3:54:11,  1.35s/it][A
  4%|▍         | 480/10907 [10:55<3:54:11,  1.35s/it][A
  4%|▍         | 481/10907 [10:57<4:03:37,  1.40s/it][A
  4%|▍         | 481/10907 [10:57<4:03:40,  1.40s/it][A
  4%|▍         | 481/10907 [10:57<4:04:14,  1.41s/it][A
  4%|▍         | 481/10907 [10:57<4:04:10,  1.41s/it][A
  4%|▍         | 481/10907 [10:57<4:04:27,  1.41s/it][A
  4%|▍         | 481/10907 [10:57<4:04:32,  1.41s/it][A
  4%|▍         | 482/10907 [10:58<4:06:23,  1.42s/it][A
  4%|▍         | 482/10907 [10:58<4:06:26,  1.42s/it][A

  4%|▍         | 482/10907 [10:58<4:06:32,  1.42s/it][A  4%|▍         | 482/10907 [10:58<4:06:27,  1.42s/it][A
  4%|▍         | 482/10907 [10:58<4:06:48,  1.42s/it][A
  4%|▍         | 482/10907 [10:58<4:06:49,  1.42s/it][A
  4%|▍         | 483/10907 [10:59<4:02:21,  1.40s/it][A
  4%|▍         | 483/10907 [11:00<4:02:25,  1.40s/it][A
  4%|▍         | 483/10907 [11:00<4:02:39,  1.40s/it][A
  4%|▍         | 483/10907 [11:00<4:03:03,  1.40s/it][A
  4%|▍         | 483/10907 [11:00<4:03:06,  1.40s/it][A
  4%|▍         | 483/10907 [11:00<4:03:09,  1.40s/it][A

  4%|▍         | 484/10907 [11:01<3:59:38,  1.38s/it][A  4%|▍         | 484/10907 [11:01<3:59:47,  1.38s/it][A
  4%|▍         | 484/10907 [11:01<3:59:48,  1.38s/it][A
  4%|▍         | 484/10907 [11:01<3:59:49,  1.38s/it][A
  4%|▍         | 484/10907 [11:01<4:00:04,  1.38s/it][A
  4%|▍         | 484/10907 [11:01<4:00:06,  1.38s/it][A
  4%|▍         | 485/10907 [11:02<3:57:54,  1.37s/it][A
  4%|▍         | 485/10907 [11:02<3:57:59,  1.37s/it][A
  4%|▍         | 485/10907 [11:02<3:58:22,  1.37s/it][A

  4%|▍         | 485/10907 [11:02<3:58:18,  1.37s/it][A  4%|▍         | 485/10907 [11:02<3:58:14,  1.37s/it][A
  4%|▍         | 485/10907 [11:02<3:58:22,  1.37s/it][A
  4%|▍         | 486/10907 [11:04<3:56:20,  1.36s/it][A
  4%|▍         | 486/10907 [11:04<3:56:35,  1.36s/it][A
  4%|▍         | 486/10907 [11:04<3:56:53,  1.36s/it][A
  4%|▍         | 486/10907 [11:04<3:56:50,  1.36s/it][A
  4%|▍         | 486/10907 [11:04<3:56:56,  1.36s/it][A
  4%|▍         | 486/10907 [11:04<3:57:10,  1.37s/it][A
  4%|▍         | 487/10907 [11:05<3:55:19,  1.36s/it][A
  4%|▍         | 487/10907 [11:05<3:55:18,  1.35s/it][A

  4%|▍         | 487/10907 [11:05<3:55:49,  1.36s/it][A  4%|▍         | 487/10907 [11:05<3:55:45,  1.36s/it][A
  4%|▍         | 487/10907 [11:05<3:55:56,  1.36s/it][A
  4%|▍         | 487/10907 [11:05<3:55:59,  1.36s/it][A
  4%|▍         | 488/10907 [11:06<3:54:47,  1.35s/it][A
  4%|▍         | 488/10907 [11:06<3:54:54,  1.35s/it][A

  4%|▍         | 488/10907 [11:06<3:55:12,  1.35s/it][A  4%|▍         | 488/10907 [11:06<3:55:09,  1.35s/it][A
  4%|▍         | 488/10907 [11:06<3:55:05,  1.35s/it][A
  4%|▍         | 488/10907 [11:06<3:55:16,  1.35s/it][A
  4%|▍         | 489/10907 [11:08<3:54:50,  1.35s/it][A


  4%|▍         | 489/10907 [11:08<3:54:53,  1.35s/it][A  4%|▍         | 489/10907 [11:08<3:55:05,  1.35s/it][A  4%|▍         | 489/10907 [11:08<3:55:03,  1.35s/it][A
  4%|▍         | 489/10907 [11:08<3:55:15,  1.35s/it]
[A  4%|▍         | 489/10907 [11:08<3:55:18,  1.36s/it][A
  4%|▍         | 490/10907 [11:09<3:55:13,  1.35s/it][A
  4%|▍         | 490/10907 [11:09<3:55:28,  1.36s/it][A
  4%|▍         | 490/10907 [11:09<3:55:35,  1.36s/it][A
  4%|▍         | 490/10907 [11:09<3:55:34,  1.36s/it][A
  4%|▍         | 490/10907 [11:09<3:55:30,  1.36s/it][A
  4%|▍         | 490/10907 [11:09<3:55:37,  1.36s/it][A
  5%|▍         | 491/10907 [11:10<3:54:26,  1.35s/it][A
  5%|▍         | 491/10907 [11:10<3:54:26,  1.35s/it][A
  5%|▍         | 491/10907 [11:10<3:54:49,  1.35s/it][A
  5%|▍         | 491/10907 [11:10<3:54:48,  1.35s/it][A
  5%|▍         | 491/10907 [11:10<3:55:00,  1.35s/it][A
  5%|▍         | 491/10907 [11:10<3:55:11,  1.35s/it][A
  5%|▍         | 492/10907 [11:12<3:54:51,  1.35s/it][A
  5%|▍         | 492/10907 [11:12<3:54:48,  1.35s/it][A
  5%|▍         | 492/10907 [11:12<3:55:03,  1.35s/it][A
  5%|▍         | 492/10907 [11:12<3:55:11,  1.35s/it][A
  5%|▍         | 492/10907 [11:12<3:55:25,  1.36s/it][A
  5%|▍         | 492/10907 [11:12<3:55:37,  1.36s/it][A
  5%|▍         | 493/10907 [11:13<3:53:59,  1.35s/it][A
  5%|▍         | 493/10907 [11:13<3:54:18,  1.35s/it][A
  5%|▍         | 493/10907 [11:13<3:54:40,  1.35s/it][A
  5%|▍         | 493/10907 [11:13<3:55:00,  1.35s/it][A
  5%|▍         | 493/10907 [11:13<3:54:58,  1.35s/it][A
  5%|▍         | 493/10907 [11:13<3:55:02,  1.35s/it][A

  5%|▍         | 494/10907 [11:14<3:53:57,  1.35s/it][A  5%|▍         | 494/10907 [11:14<3:54:00,  1.35s/it][A

  5%|▍         | 494/10907 [11:14<3:53:59,  1.35s/it][A  5%|▍         | 494/10907 [11:14<3:54:10,  1.35s/it][A
  5%|▍         | 494/10907 [11:14<3:54:11,  1.35s/it][A
  5%|▍         | 494/10907 [11:14<3:54:21,  1.35s/it][A
  5%|▍         | 495/10907 [11:16<3:54:00,  1.35s/it][A
  5%|▍         | 495/10907 [11:16<3:53:58,  1.35s/it][A
  5%|▍         | 495/10907 [11:16<3:54:08,  1.35s/it][A

  5%|▍         | 495/10907 [11:16<3:54:17,  1.35s/it][A  5%|▍         | 495/10907 [11:16<3:54:20,  1.35s/it][A
  5%|▍         | 495/10907 [11:16<3:54:34,  1.35s/it][A
  5%|▍         | 496/10907 [11:17<3:53:51,  1.35s/it][A
  5%|▍         | 496/10907 [11:17<3:53:54,  1.35s/it][A
  5%|▍         | 496/10907 [11:17<3:54:10,  1.35s/it][A
  5%|▍         | 496/10907 [11:17<3:54:18,  1.35s/it][A
  5%|▍         | 496/10907 [11:17<3:54:22,  1.35s/it][A
  5%|▍         | 496/10907 [11:17<3:54:35,  1.35s/it][A
  5%|▍         | 497/10907 [11:19<4:04:32,  1.41s/it][A
  5%|▍         | 497/10907 [11:19<4:04:54,  1.41s/it][A
  5%|▍         | 497/10907 [11:19<4:04:52,  1.41s/it][A
  5%|▍         | 497/10907 [11:19<4:05:02,  1.41s/it][A
  5%|▍         | 497/10907 [11:19<4:04:56,  1.41s/it][A
  5%|▍         | 497/10907 [11:19<4:05:02,  1.41s/it][A
  5%|▍         | 498/10907 [11:20<4:06:30,  1.42s/it][A
  5%|▍         | 498/10907 [11:20<4:06:50,  1.42s/it][A
  5%|▍         | 498/10907 [11:20<4:07:17,  1.43s/it][A
  5%|▍         | 498/10907 [11:20<4:07:26,  1.43s/it][A
  5%|▍         | 498/10907 [11:20<4:07:22,  1.43s/it][A
  5%|▍         | 498/10907 [11:20<4:07:37,  1.43s/it][A
  5%|▍         | 499/10907 [11:21<4:02:52,  1.40s/it][A
  5%|▍         | 499/10907 [11:21<4:02:58,  1.40s/it][A
  5%|▍         | 499/10907 [11:21<4:03:06,  1.40s/it][A
  5%|▍         | 499/10907 [11:21<4:03:03,  1.40s/it][A
  5%|▍         | 499/10907 [11:21<4:03:21,  1.40s/it][A
  5%|▍         | 499/10907 [11:21<4:03:12,  1.40s/it][A
  5%|▍         | 500/10907 [11:23<3:59:34,  1.38s/it][A
  5%|▍         | 500/10907 [11:23<3:59:31,  1.38s/it][A
  5%|▍         | 500/10907 [11:23<4:00:11,  1.38s/it][A
  5%|▍         | 500/10907 [11:23<4:00:31,  1.39s/it][A
  5%|▍         | 500/10907 [11:23<4:00:37,  1.39s/it][A
  5%|▍         | 500/10907 [11:23<4:00:47,  1.39s/it][A
  5%|▍         | 501/10907 [11:24<3:57:52,  1.37s/it][A
  5%|▍         | 501/10907 [11:24<3:58:15,  1.37s/it][A
  5%|▍         | 501/10907 [11:24<3:58:48,  1.38s/it][A
  5%|▍         | 501/10907 [11:24<3:58:18,  1.37s/it]
[A
  5%|▍         | 501/10907 [11:24<3:58:32,  1.38s/it][A  5%|▍         | 501/10907 [11:24<3:58:43,  1.38s/it][A
  5%|▍         | 502/10907 [11:25<3:57:21,  1.37s/it][A
  5%|▍         | 502/10907 [11:26<3:57:18,  1.37s/it]
[A
  5%|▍         | 502/10907 [11:26<3:57:09,  1.37s/it][A  5%|▍         | 502/10907 [11:26<3:57:16,  1.37s/it][A
  5%|▍         | 502/10907 [11:26<3:57:19,  1.37s/it][A
  5%|▍         | 502/10907 [11:26<3:57:27,  1.37s/it][A
  5%|▍         | 503/10907 [11:27<3:55:41,  1.36s/it][A
  5%|▍         | 503/10907 [11:27<3:55:48,  1.36s/it][A
  5%|▍         | 503/10907 [11:27<3:55:43,  1.36s/it][A
  5%|▍         | 503/10907 [11:27<3:55:50,  1.36s/it][A
  5%|▍         | 503/10907 [11:27<3:56:15,  1.36s/it][A
  5%|▍         | 503/10907 [11:27<3:56:12,  1.36s/it][A
  5%|▍         | 504/10907 [11:28<3:55:45,  1.36s/it][A
  5%|▍         | 504/10907 [11:28<3:55:54,  1.36s/it][A
  5%|▍         | 504/10907 [11:28<3:55:56,  1.36s/it][A
  5%|▍         | 504/10907 [11:28<3:56:04,  1.36s/it][A

  5%|▍         | 504/10907 [11:28<3:56:07,  1.36s/it][A  5%|▍         | 504/10907 [11:28<3:56:02,  1.36s/it][A
  5%|▍         | 505/10907 [11:30<3:54:53,  1.35s/it][A
  5%|▍         | 505/10907 [11:29<3:55:12,  1.36s/it][A

  5%|▍         | 505/10907 [11:30<3:55:12,  1.36s/it][A  5%|▍         | 505/10907 [11:30<3:55:19,  1.36s/it][A
  5%|▍         | 505/10907 [11:30<3:55:17,  1.36s/it][A
  5%|▍         | 505/10907 [11:30<3:55:21,  1.36s/it][A
  5%|▍         | 506/10907 [11:31<3:54:32,  1.35s/it][A

  5%|▍         | 506/10907 [11:31<3:54:37,  1.35s/it][A
  5%|▍         | 506/10907 [11:31<3:54:39,  1.35s/it][A  5%|▍         | 506/10907 [11:31<3:54:42,  1.35s/it][A
  5%|▍         | 506/10907 [11:31<3:54:48,  1.35s/it][A
  5%|▍         | 506/10907 [11:31<3:54:56,  1.36s/it][A
  5%|▍         | 507/10907 [11:32<3:53:00,  1.34s/it][A
  5%|▍         | 507/10907 [11:32<3:53:12,  1.35s/it][A
  5%|▍         | 507/10907 [11:32<3:53:51,  1.35s/it][A
  5%|▍         | 507/10907 [11:32<3:54:03,  1.35s/it][A
  5%|▍         | 507/10907 [11:32<3:54:00,  1.35s/it][A
  5%|▍         | 507/10907 [11:32<3:54:06,  1.35s/it][Aslurmstepd: error: *** JOB 1772858 ON jean-zay-iam21 CANCELLED AT 2023-05-20T18:18:22 ***
